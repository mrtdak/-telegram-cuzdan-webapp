from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import time
import requests
import json
import os
import re
import asyncio
import aiohttp
import hashlib
import logging
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime
from zoneinfo import ZoneInfo
from collections import defaultdict
import wikipedia  # Wikipedia API
wikipedia.set_lang("tr")  # TÃ¼rkÃ§e Wikipedia

# ğŸ”‡ DEBUG LOGLARINI KAPAT (Production iÃ§in)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("requests").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# Topic Memory - Uzun DÃ¶nem HafÄ±za Sistemi (v2.0)
# Kategori bazlÄ±, semantik benzerlikle gruplandÄ±rma
from topic_memory import TopicMemory

# Conversation Context - LLM TabanlÄ± BaÄŸlam YÃ¶netimi (v1.0)
# Konu derinleÅŸtiÄŸinde baÄŸlamÄ± koruyan akÄ±llÄ± Ã¶zet sistemi
from conversation_context import ConversationContextManager

# ============================================================
# BÃ–LÃœM 1: YARDIMCI FONKSÄ°YONLAR VE ARAÃ‡LAR
# ============================================================

def get_current_datetime() -> Dict[str, str]:
    """TÃ¼rkiye saati ile ÅŸu anki tarih ve saati getir"""
    try:
        tz = ZoneInfo("Europe/Istanbul")
        now = datetime.now(tz)

        ay_isimleri = {
            1: "Ocak", 2: "Åubat", 3: "Mart", 4: "Nisan",
            5: "MayÄ±s", 6: "Haziran", 7: "Temmuz", 8: "AÄŸustos",
            9: "EylÃ¼l", 10: "Ekim", 11: "KasÄ±m", 12: "AralÄ±k"
        }

        gun_isimleri = {
            0: "Pazartesi", 1: "SalÄ±", 2: "Ã‡arÅŸamba", 3: "PerÅŸembe",
            4: "Cuma", 5: "Cumartesi", 6: "Pazar"
        }

        ay = ay_isimleri[now.month]
        gun = gun_isimleri[now.weekday()]
        saat = now.hour

        # Zaman dilimi belirleme (basit)
        if 0 <= saat < 6:
            zaman_dilimi = "gece"
        elif 6 <= saat < 12:
            zaman_dilimi = "sabah"
        elif 12 <= saat < 18:
            zaman_dilimi = "Ã¶ÄŸleden sonra"
        else:
            zaman_dilimi = "akÅŸam"

        # Cuma kontrolÃ¼
        cuma_notu = " (Cuma)" if now.weekday() == 4 else ""

        return {
            "tarih": f"{now.day} {ay} {now.year}",
            "gun": gun,
            "saat": now.strftime("%H:%M"),
            "full": f"{now.day} {ay} {now.year} {gun}, Saat: {now.strftime('%H:%M')}",
            "zaman_dilimi": zaman_dilimi + cuma_notu,
            "saat_int": saat,
        }
    except Exception:
        return {
            "tarih": "Bilinmiyor",
            "gun": "Bilinmiyor",
            "saat": "Bilinmiyor",
            "full": "Tarih/saat bilgisi alÄ±namadÄ±",
            "zaman_dilimi": "",
            "saat_int": 12,
        }


def calculate_math(expression: str) -> str:
    """Matematiksel ifadeyi gÃ¼venli ÅŸekilde hesapla"""
    import math

    try:
        safe_expression = expression.strip()

        # TÃ¼rkÃ§e operatÃ¶rleri Ä°ngilizce'ye Ã§evir
        safe_expression = safe_expression.replace("x", "*")
        safe_expression = safe_expression.replace("X", "*")
        safe_expression = safe_expression.replace("Ã§arpÄ±", "*")
        safe_expression = safe_expression.replace("Ã§arp", "*")
        safe_expression = safe_expression.replace("bÃ¶lÃ¼", "/")
        safe_expression = safe_expression.replace("artÄ±", "+")
        safe_expression = safe_expression.replace("eksi", "-")

        # Sadece gÃ¼venli karakterlere izin ver
        allowed_chars = "0123456789+-*/(). "
        if not all(c in allowed_chars for c in safe_expression):
            return "âŒ GÃ¼venlik: Sadece sayÄ±lar ve matematiksel operatÃ¶rler kullanÄ±labilir."

        safe_dict = {
            "sqrt": math.sqrt,
            "pow": math.pow,
            "abs": abs,
            "round": round,
            "sin": math.sin,
            "cos": math.cos,
            "tan": math.tan,
            "pi": math.pi,
            "e": math.e,
        }

        result = eval(safe_expression, {"__builtins__": {}}, safe_dict)

        if isinstance(result, float):
            if result.is_integer():
                return str(int(result))
            return f"{result:.4f}".rstrip("0").rstrip(".")

        return str(result)

    except ZeroDivisionError:
        return "âŒ Hata: SÄ±fÄ±ra bÃ¶lme yapÄ±lamaz!"
    except Exception:
        return "âŒ Hesaplama hatasÄ±: GeÃ§ersiz matematiksel ifade."


async def wiki_ara(query: str) -> str:
    """
    Wikipedia'da arama yap ve Ã¶zet getir.
    Ã–nce TÃ¼rkÃ§e Wikipedia'da arar, bulamazsa Ä°ngilizce'de dener.
    """

    def _ara_dil(query: str, lang: str) -> tuple[bool, str]:
        """Belirli bir dilde Wikipedia aramasÄ± yap"""
        wikipedia.set_lang(lang)

        try:
            search_results = wikipedia.search(query, results=3)

            if not search_results:
                return False, ""

            try:
                page = wikipedia.page(search_results[0], auto_suggest=False)
                summary = wikipedia.summary(search_results[0], sentences=4, auto_suggest=False)

                lang_flag = "ğŸ‡¹ğŸ‡·" if lang == "tr" else "ğŸ‡¬ğŸ‡§"
                result = f"ğŸ“š {lang_flag} **{page.title}**\n\n{summary}"

                # Alternatif sonuÃ§larÄ± da gÃ¶ster (varsa)
                if len(search_results) > 1:
                    alternatives = ", ".join(search_results[1:3])
                    result += f"\n\nğŸ” Ä°lgili: {alternatives}"

                return True, result

            except wikipedia.DisambiguationError as e:
                # Birden fazla sonuÃ§ var - seÃ§enekleri sun
                options = e.options[:5]
                return True, f"ğŸ¤” '{query}' iÃ§in birden fazla sonuÃ§ var:\nâ€¢ " + "\nâ€¢ ".join(options) + "\n\nHangisini istediÄŸini belirtir misin?"

            except wikipedia.PageError:
                # Ä°lk sonuÃ§ bulunamadÄ±, alternatif dene
                if len(search_results) > 1:
                    try:
                        summary = wikipedia.summary(search_results[1], sentences=3, auto_suggest=False)
                        lang_flag = "ğŸ‡¹ğŸ‡·" if lang == "tr" else "ğŸ‡¬ğŸ‡§"
                        return True, f"ğŸ“š {lang_flag} {search_results[1]}:\n{summary}"
                    except (wikipedia.PageError, wikipedia.WikipediaException) as e:
                        print(f"Wikipedia alternatif arama hatasÄ±: {e}")
                return False, ""

        except Exception:
            return False, ""

    try:
        # 1. Ã–nce TÃ¼rkÃ§e Wikipedia'da ara
        found, result = _ara_dil(query, "tr")
        if found:
            wikipedia.set_lang("tr")  # Dili geri ayarla
            return result

        # 2. TÃ¼rkÃ§e'de bulunamadÄ±, Ä°ngilizce'de dene
        print(f"   ğŸ“– TÃ¼rkÃ§e Wikipedia'da bulunamadÄ±, Ä°ngilizce deneniyor...")
        found, result = _ara_dil(query, "en")
        wikipedia.set_lang("tr")  # Dili geri ayarla

        if found:
            return result

        return f"âŒ Wikipedia'da '{query}' bulunamadÄ± (TÃ¼rkÃ§e ve Ä°ngilizce denendi)."

    except Exception as e:
        wikipedia.set_lang("tr")  # Hata durumunda da dili geri ayarla
        return f"âŒ Wikipedia hatasÄ±: {str(e)}"


async def get_weather(city: str) -> str:
    """Åehir iÃ§in hava durumu bilgisi getir"""
    try:
        city = (
            city.replace("hava durumu", "")
            .replace("hava", "")
            .replace("nasÄ±l", "")
            .strip()
        )

        api_key = os.getenv("OPENWEATHER_API_KEY", "")

        if not api_key:
            return await get_weather_fallback(city)

        url = "http://api.openweathermap.org/data/2.5/weather"
        params = {
            "q": f"{city},TR",
            "appid": api_key,
            "units": "metric",
            "lang": "tr",
        }

        timeout = aiohttp.ClientTimeout(total=5)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, params=params) as response:
                if response.status != 200:
                    return await get_weather_fallback(city)
                data = await response.json()

        temp = data["main"]["temp"]
        feels_like = data["main"]["feels_like"]
        humidity = data["main"]["humidity"]
        description = data["weather"][0]["description"].capitalize()
        wind_speed = data["wind"]["speed"]

        result = "[KORUNACAK_FORMAT]\n"
        result += f"ğŸŒ¤ï¸ {city.title()} Hava Durumu\n"
        result += f"{'â”€' * 32}\n\n"
        result += f"â˜ï¸ Durum:       {description}\n"
        result += f"ğŸŒ¡ï¸ SÄ±caklÄ±k:    {temp:.1f}Â°C\n"
        result += f"ğŸ¤š Hissedilen:  {feels_like:.1f}Â°C\n"
        result += f"ğŸ’¨ RÃ¼zgar:      {wind_speed:.1f} m/s\n"
        result += f"ğŸ’§ Nem:         {humidity}%\n"
        result += "[/KORUNACAK_FORMAT]"

        return result

    except Exception:
        return await get_weather_fallback(city)


async def get_weather_fallback(city: str) -> str:
    """Fallback: hava durumu - Web search kaldÄ±rÄ±ldÄ±"""
    # Web search (DuckDuckGo) kaldÄ±rÄ±ldÄ±
    return f"âŒ {city} iÃ§in hava durumu servisi kullanÄ±lamÄ±yor. Web arama devre dÄ±ÅŸÄ±."


async def get_prayer_times(city: str, specific_prayer: str = None) -> str:
    """Åehir iÃ§in namaz vakitlerini getir (Aladhan API)"""
    try:
        city = (
            city.replace("namaz vakitleri", "")
            .replace("namaz vakti", "")
            .replace("ezan", "")
            .strip()
        )
        city = (
            city.replace("â€™da", "")
            .replace("â€™de", "")
            .replace("â€™Ä±n", "")
            .replace("â€™in", "")
            .strip()
        )

        url = "http://api.aladhan.com/v1/timingsByCity"
        params = {
            "city": city,
            "country": "Turkey",
            "method": 13,
        }

        timeout = aiohttp.ClientTimeout(total=5)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, params=params) as response:
                if response.status != 200:
                    return f"âŒ {city} ÅŸehri bulunamadÄ±."
                data = await response.json()

        if data.get("code") != 200:
            return f"âŒ {city} iÃ§in namaz vakitleri alÄ±namadÄ±."

        timings = data["data"]["timings"]
        date_info = data["data"]["date"]["gregorian"]

        prayer_names = {
            "Fajr": ("Ä°msak", "ğŸŒ™"),
            "Sunrise": ("GÃ¼neÅŸ", "â˜€ï¸"),
            "Dhuhr": ("Ã–ÄŸle", "ğŸŒ¤ï¸"),
            "Asr": ("Ä°kindi", "ğŸŒ…"),
            "Maghrib": ("AkÅŸam", "ğŸŒ†"),
            "Isha": ("YatsÄ±", "ğŸŒƒ"),
        }

        if specific_prayer:
            specific_prayer = specific_prayer.lower().strip()
            prayer_map = {
                "imsak": "Fajr",
                "gÃ¼neÅŸ": "Sunrise",
                "Ã¶ÄŸle": "Dhuhr",
                "ikindi": "Asr",
                "akÅŸam": "Maghrib",
                "yatsÄ±": "Isha",
            }

            for tr_name, eng_name in prayer_map.items():
                if tr_name in specific_prayer:
                    time_value = timings[eng_name]
                    turkish_name, emoji = prayer_names[eng_name]
                    return f"{emoji} {city.title()} {turkish_name} namazÄ±: {time_value}"

        result = "[KORUNACAK_FORMAT]\n"
        result += f"ğŸ•Œ {city.title()} Namaz Vakitleri\n"
        result += (
            f"ğŸ“… {date_info['day']} {date_info['month']['en']} {date_info['year']}\n"
        )
        result += f"{'â”€' * 32}\n\n"

        for eng_name, (turkish_name, emoji) in prayer_names.items():
            time_value = timings[eng_name]
            padded_name = f"{turkish_name:<8}"
            result += f"{emoji} {padded_name} {time_value}\n"

        result += "[/KORUNACAK_FORMAT]"
        return result.strip()

    except Exception as e:
        return f"âŒ Namaz vakitleri alÄ±namadÄ±: {str(e)}"


# ============================================================
# BÃ–LÃœM 2: TOOL SYSTEM (personal_ai.py'dan import edilecek)
# ============================================================
# NOT: ToolSystem artÄ±k personal_ai.py'da tanÄ±mlÄ± (tek kaynak)
# Lazy import ile kullanÄ±lÄ±yor (circular import Ã¶nlemi)

_ToolSystem = None

def get_tool_system_class():
    """ToolSystem'i lazy import et (circular import Ã¶nlemi)"""
    global _ToolSystem
    if _ToolSystem is None:
        try:
            from personal_ai import ToolSystem
            _ToolSystem = ToolSystem
        except ImportError:
            # Fallback: basit bir ToolSystem class
            class FallbackToolSystem:
                TOOLS = {
                    "risale_ara": {"name": "risale_ara", "description": "Dini sorulara cevap", "parameters": "soru", "when": "Dini konularda", "examples": ["Ä°man nedir?"]},
                    "zaman_getir": {"name": "zaman_getir", "description": "Tarih/saat", "parameters": "yok", "when": "Zaman sorulduÄŸunda", "examples": ["Saat kaÃ§?"]},
                    "hesapla": {"name": "hesapla", "description": "Hesaplama", "parameters": "ifade", "when": "Matematik sorulduÄŸunda", "examples": ["2+2"]},
                    "hava_durumu": {"name": "hava_durumu", "description": "Hava durumu", "parameters": "ÅŸehir", "when": "Hava sorulduÄŸunda", "examples": ["Ä°stanbul hava"]},
                    "namaz_vakti": {"name": "namaz_vakti", "description": "Namaz vakitleri", "parameters": "ÅŸehir", "when": "Namaz vakti sorulduÄŸunda", "examples": ["Ankara namaz"]},
                    "wiki_ara": {"name": "wiki_ara", "description": "Wikipedia'da ara (Ã¼nlÃ¼ kiÅŸi, yer, olay)", "parameters": "arama terimi", "when": "ÃœnlÃ¼ kiÅŸi/yer/olay sorulduÄŸunda", "examples": ["Ã–zdemir ErdoÄŸan ÅŸarkÄ±cÄ±"]},
                    "yok": {"name": "yok", "description": "Direkt cevap", "parameters": "yok", "when": "Genel sohbet", "examples": ["Merhaba"]},
                }
                @staticmethod
                def get_tools_prompt() -> str:
                    tools_text = "ARAÃ‡LAR:\n"
                    for name, info in FallbackToolSystem.TOOLS.items():
                        tools_text += f"{name}: {info['description']}\n"
                    return tools_text
                @staticmethod
                def get_tool_calling_prompt(user_input: str) -> str:
                    return f"{FallbackToolSystem.get_tools_prompt()}\nSORU: {user_input}\nARAÃ‡:"
                @staticmethod
                def parse_tool_decision(llm_response: str) -> Tuple[str, str]:
                    tool_name = "yok"
                    tool_param = ""
                    for line in llm_response.split("\n"):
                        if line.startswith("ARAÃ‡:"): tool_name = line.replace("ARAÃ‡:", "").strip().lower()
                        elif line.startswith("PARAMETRE:"): tool_param = line.replace("PARAMETRE:", "").strip()
                    if tool_name not in FallbackToolSystem.TOOLS: tool_name = "yok"
                    if tool_param.lower() == "yok": tool_param = ""
                    return tool_name, tool_param
            _ToolSystem = FallbackToolSystem
    return _ToolSystem


# ToolSystem wrapper (geriye uyumluluk iÃ§in)
class ToolSystem:
    """
    ToolSystem wrapper - personal_ai.py'daki ToolSystem'e yÃ¶nlendirir

    NOT: AsÄ±l implementasyon personal_ai.py'da (tek kaynak)
    """

    @property
    def TOOLS(self):
        return get_tool_system_class().TOOLS

    @staticmethod
    def get_tools_prompt() -> str:
        return get_tool_system_class().get_tools_prompt()

    @staticmethod
    def get_tool_calling_prompt(user_input: str) -> str:
        return get_tool_system_class().get_tool_calling_prompt(user_input)

    @staticmethod
    def parse_tool_decision(llm_response: str) -> Tuple[str, str]:
        return get_tool_system_class().parse_tool_decision(llm_response)


# ============================================================
# BÃ–LÃœM 3: WEB SEARCH - KALDIRILDI
# ============================================================
# NOT: Web aramasÄ± kaldÄ±rÄ±ldÄ± (saÃ§ma bilgiler Ã§ekiyordu)
# ArtÄ±k sadece Wikipedia (wiki_ara) kullanÄ±lÄ±yor
# Din sorularÄ± iÃ§in FAISS (risale_ara) kullanÄ±lÄ±yor


# ============================================================
# BÃ–LÃœM 4: MULTI-ROLE SYSTEM (ROLES personal_ai.py'dan alÄ±nÄ±yor)
# ============================================================

# NOT: ROLES artÄ±k personal_ai.py/SystemConfig'de tanÄ±mlÄ± (tek kaynak)
_ROLES_CACHE = None

def get_roles():
    """ROLES'u personal_ai.py'dan al (circular import Ã¶nlemi)"""
    global _ROLES_CACHE
    if _ROLES_CACHE is None:
        try:
            from personal_ai import SystemConfig
            _ROLES_CACHE = SystemConfig.ROLES
        except ImportError:
            # Fallback: varsayÄ±lan roller
            _ROLES_CACHE = {
                "friend": {"keywords": ["selam", "merhaba", "nasÄ±lsÄ±n", "naber"], "tone": "professional_warm", "max_length": 1500},
                "technical_helper": {"keywords": ["kod", "python", "hata", "bug", "error"], "tone": "professional_clear", "max_length": 2000},
                "teacher": {"keywords": ["nedir", "ne demek", "aÃ§Ä±kla", "Ã¶ÄŸret", "anlat"], "tone": "educational_clear", "max_length": 2500},
            }
    return _ROLES_CACHE


# MultiRoleSystem artÄ±k personal_ai.py'dan import ediliyor (tek kaynak)
_MultiRoleSystem = None

def get_multi_role_system_class():
    """MultiRoleSystem'i lazy import et (circular import Ã¶nlemi)"""
    global _MultiRoleSystem
    if _MultiRoleSystem is None:
        try:
            from personal_ai import MultiRoleSystem as _MRS
            _MultiRoleSystem = _MRS
        except ImportError:
            # Fallback: basit MultiRoleSystem
            class FallbackMultiRoleSystem:
                def __init__(self):
                    self.enabled = True
                @property
                def ROLES(self):
                    return get_roles()
                def detect_role(self, user_input: str) -> str:
                    user_lower = user_input.lower()
                    code_markers = ["```", "def ", "class ", "import "]
                    if any(m in user_input for m in code_markers):
                        return "technical_helper"
                    for role_name, role_config in self.ROLES.items():
                        if any(kw in user_lower for kw in role_config.get("keywords", [])):
                            return role_name
                    return "friend"
            _MultiRoleSystem = FallbackMultiRoleSystem
    return _MultiRoleSystem


class MultiRoleSystem:
    """
    MultiRoleSystem wrapper - personal_ai.py'daki class'a yÃ¶nlendirir
    NOT: AsÄ±l implementasyon personal_ai.py'da (tek kaynak)
    """

    def __init__(self):
        self._impl = get_multi_role_system_class()()

    @property
    def ROLES(self):
        return get_roles()

    def detect_role(self, user_input: str) -> str:
        return self._impl.detect_role(user_input)


# ============================================================
# BÃ–LÃœM 5: FAISS KNOWLEDGE BASE (BASIT WRAPPER)
# ============================================================

class SimpleFAISSKB:
    """
    Basit FAISS KB wrapper
    (GerÃ§ek FAISS KB PersonalAIâ€™dan alÄ±nÄ±r ve buraya inject edilir)
    """

    def __init__(self):
        self.enabled = False
        self.faiss_kb = None

    def set_faiss_kb(self, faiss_kb):
        """GerÃ§ek FAISS KB'yi inject et"""
        self.faiss_kb = faiss_kb
        self.enabled = (
            faiss_kb is not None and hasattr(faiss_kb, "enabled") and faiss_kb.enabled
        )

    def get_relevant_context(self, query: str, max_chunks: int = 6) -> str:
        """FAISS'ten ilgili baÄŸlamÄ± getir"""
        if not self.enabled or not self.faiss_kb:
            return ""
        try:
            return self.faiss_kb.get_relevant_context(query, max_chunks)
        except Exception as e:
            print(f"âŒ FAISS hatasÄ±: {e}")
            return ""


# ============================================================
# BÃ–LÃœM 6: DECISION LLM (Together.ai - Llama 70B)
# ============================================================

class DecisionLLM:
    """Together.ai API ile akÄ±llÄ± karar verme (Llama 70B)"""

    def __init__(self, api_key: str = None, model: str = "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"):
        self.api_key = api_key or os.getenv("TOGETHER_API_KEY")
        self.model = model
        self.base_url = "https://api.together.xyz/v1/completions"

        if not self.api_key:
            raise ValueError("âŒ TOGETHER_API_KEY bulunamadÄ±! .env dosyasÄ±nÄ± kontrol edin.")

        # API baÄŸlantÄ±sÄ±nÄ± test et
        if not self._try_connect():
            raise ConnectionError("âŒ Together.ai API'sine baÄŸlanÄ±lamadÄ±!")

        print(f"ğŸ§  DecisionLLM baÅŸlatÄ±ldÄ± (Model: {model}, Together.ai)")

    def _try_connect(self) -> bool:
        """Together.ai API baÄŸlantÄ±sÄ±nÄ± test et"""
        try:
            response = requests.get(
                "https://api.together.xyz/v1/models",
                headers={"Authorization": f"Bearer {self.api_key}"},
                timeout=5
            )
            return response.status_code == 200
        except (requests.RequestException, requests.Timeout) as e:
            print(f"Together API baÄŸlantÄ± hatasÄ±: {e}")
            return False

    def _call_llm(self, prompt: str, max_tokens: int = 100) -> str:
        """Together.ai API'sine prompt gÃ¶nder"""
        try:
            response = requests.post(
                self.base_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "max_tokens": max_tokens,
                    "temperature": 0.3,
                    "stop": ["<|eot_id|>", "<|end_of_text|>"]
                },
                timeout=15,
            )

            if response.status_code == 200:
                return response.json()["choices"][0]["text"].strip()
            return ""
        except Exception as e:
            print(f"âŒ DecisionLLM hatasÄ±: {e}")
            return ""

    def extract_topics(self, query: str, max_topics: int = 3) -> List[str]:
        """KonularÄ± akÄ±llÄ±ca Ã§Ä±kar"""
        prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

KullanÄ±cÄ± mesajÄ±: "{query}"

GÃ–REV: ANA KONULARI bul (maksimum {max_topics} adet)

KURALLAR:

- Uzun kelimeler deÄŸil, ANLAMLI konular
- Her satÄ±ra 1 konu
- AlakasÄ±z kelime ekleme

KONULAR:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

        response = self._call_llm(prompt, max_tokens=50)

        topics = [
            line.strip().strip("-â€¢*").strip()
            for line in response.split("\n")
            if line.strip() and len(line.strip()) > 3
        ]

        return topics[:max_topics]


# ============================================================
# BÃ–LÃœM 7: ANA HAFIZA ASÄ°STANI (GENÄ°ÅLETÄ°LMÄ°Å SEKRETER)
# ============================================================

class HafizaAsistani:
    """
    ğŸ§  GeliÅŸmiÅŸ HafÄ±za AsistanÄ± v3.0 - GERÃ‡EK SEKRETER

    Ã–ZELLÄ°KLER:
    - Benzerlik tabanlÄ± arama (BGE-M3)
    - Tool System entegrasyonu
    - Web Search
    - FAISS KB eriÅŸimi
    - Multi-Role System
    - AkÄ±llÄ± prompt hazÄ±rlama
    - DecisionLLM ile karar verme
    """

    def __init__(
        self,
        saat_limiti: int = 48,
        esik: float = 0.50,
        max_mesaj: int = 20,
        model_adi: str = "BAAI/bge-m3",
        use_decision_llm: bool = True,
        together_api_key: str = None,
        decision_model: str = "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
    ):
        print("=" * 60)
        print("ğŸ§  HafizaAsistani v3.0 - GeniÅŸletilmiÅŸ Sekreter")
        print("=" * 60)

        # Together.ai API key
        self.together_api_key = together_api_key or os.getenv("TOGETHER_API_KEY")
        self.decision_model = decision_model

        # Embedding modeli
        print("ğŸ“¦ Embedding modeli yÃ¼kleniyor...")
        self.embedder = SentenceTransformer(model_adi)
        print(f"âœ… Model '{model_adi}' yÃ¼klendi!")

        # Temel ayarlar
        self.hafiza: List[Dict[str, Any]] = []
        self.saat_limiti = saat_limiti * 3600
        self.esik = esik
        self.max_mesaj = max_mesaj

        # DecisionLLM (zorunlu)
        if not use_decision_llm:
            raise ValueError("âŒ DecisionLLM zorunludur!")

        try:
            self.decision_llm = DecisionLLM(api_key=self.together_api_key, model=decision_model)
            self.use_decision_llm = True
            print("âœ… DecisionLLM aktif!")
        except Exception as e:
            raise RuntimeError(f"DecisionLLM baÅŸlatÄ±lamadÄ±: {e}")

        # Tool System
        self.tool_system = ToolSystem()
        print("âœ… Tool System aktif!")

        # Multi-Role System
        self.multi_role = MultiRoleSystem()
        print("âœ… Multi-Role System aktif!")

        # FAISS KB (placeholder, inject edilecek)
        self.faiss_kb = SimpleFAISSKB()
        print("âœ… FAISS KB wrapper hazÄ±r (inject edilecek)!")

        # ğŸ†• KAPANAN KONULAR LÄ°STESÄ° (sohbet iÃ§inde kapanan konularÄ± takip et)
        # AynÄ± konuya geri dÃ¶nmemek iÃ§in kullanÄ±lÄ±r
        self.closed_topics: List[Dict[str, Any]] = []
        self.max_closed_topics = 20  # En fazla 20 kapanan konu tut
        print("âœ… Closed Topics Tracker aktif!")

        # ğŸ“š TOPIC MEMORY - Uzun dÃ¶nem hafÄ±za sistemi (v2.0)
        # Kategori bazlÄ±, semantik benzerlikle gruplandÄ±rma
        self.topic_memory = TopicMemory(
            user_id="murat",  # Sabit kullanÄ±cÄ±
            base_dir="user_data",
            together_api_key=self.together_api_key,
            together_model=decision_model,
            embedding_model=model_adi  # AynÄ± embedding modelini kullan
        )
        print("âœ… Topic Memory aktif!")

        # ğŸ§  CONVERSATION CONTEXT - LLM tabanlÄ± akÄ±llÄ± baÄŸlam yÃ¶netimi (v1.0)
        # Konu derinleÅŸtiÄŸinde baÄŸlamÄ± koruyan Ã¶zet sistemi
        self.conversation_context = ConversationContextManager(
            user_id="murat",  # Sabit kullanÄ±cÄ±
            base_dir="user_data",
            together_api_key=self.together_api_key,
            together_model=decision_model,
            archive_to_faiss=False  # Åimdilik dosya bazlÄ± arÅŸivleme
        )
        print("âœ… Conversation Context aktif!")

        print("\nâš™ï¸ Sekreter AyarlarÄ±:")
        print(f"   â€¢ Zaman limiti: {saat_limiti} saat")
        print(f"   â€¢ Benzerlik eÅŸiÄŸi: {esik}")
        print(f"   â€¢ Max mesaj: {max_mesaj}")
        print("   â€¢ Tool System: âœ…")
        print("   â€¢ Wikipedia (wiki_ara): âœ…")
        print("   â€¢ Multi-Role: âœ…")
        print("   â€¢ DecisionLLM: âœ…")
        print("   â€¢ Topic Memory (v2.0): âœ…")
        print("=" * 60 + "\n")

    # ---------- TEMEL HAFIZA FONKSÄ°YONLARI ----------

    def mesaj_ekle(self, mesaj: str, rol: str = "user"):
        """Yeni mesajÄ± vektÃ¶rleÅŸtirip hafÄ±zaya ekler"""
        vektor = self.embedder.encode(mesaj)
        self.hafiza.append(
            {"rol": rol, "mesaj": mesaj, "zaman": time.time(), "vektor": vektor}
        )
        self._eski_mesajlari_sil()

    def add(self, user_message: str, ai_response: str, chat_history: List[Dict] = None):
        """
        KullanÄ±cÄ± ve AI mesajlarÄ±nÄ± hafÄ±zaya ekler.
        AyrÄ±ca ConversationContext'i de gÃ¼nceller.

        Args:
            user_message: KullanÄ±cÄ± mesajÄ±
            ai_response: AI yanÄ±tÄ±
            chat_history: Opsiyonel sohbet geÃ§miÅŸi (context iÃ§in)
        """
        self.mesaj_ekle(user_message, rol="user")
        self.mesaj_ekle(ai_response, rol="assistant")

        # ConversationContext gÃ¼ncelle (LLM tabanlÄ± Ã¶zet sistemi)
        if self.conversation_context and chat_history:
            try:
                result = self.conversation_context.process_message(
                    user_message, ai_response, chat_history
                )
                if result.get("new_session_started"):
                    print("ğŸ”„ Yeni konu tespit edildi, session deÄŸiÅŸtirildi")

                    # ğŸ†• TAMPON BÃ–LGE: 12'den eski mesajlarÄ± Ã¶zetle ve TopicMemory'ye kaydet
                    if len(self.hafiza) > 12:
                        tampon_bolge = self.hafiza[:-12]  # 12'den eski mesajlar
                        if tampon_bolge and self.topic_memory:
                            # Tampon bÃ¶lgeyi metin olarak hazÄ±rla
                            tampon_text = "\n".join([
                                f"[{m['rol'].upper()}]: {m['mesaj']}"
                                for m in tampon_bolge if m.get('mesaj')
                            ])
                            # Ã–zet olarak kaydet (conversation_context'ten al)
                            topic_summary = result.get('current_summary', '') or tampon_text[:200]
                            if topic_summary:
                                print(f"ğŸ’¾ Tampon bÃ¶lge TopicMemory'ye kaydediliyor ({len(tampon_bolge)} mesaj)")
                                self.add_closed_topic(topic_summary, chat_history)

                    # Son 4 mesaj kalsÄ±n (baÄŸlam kopmasÄ±n) - geri kalanÄ± sil
                    if len(self.hafiza) > 4:
                        self.hafiza = self.hafiza[-4:]
                        print("ğŸ§¹ HafÄ±za temizlendi (son 4 mesaj kaldÄ± - baÄŸlam korundu)")
                elif result.get("summary_updated"):
                    print(f"ğŸ“ Konu Ã¶zeti gÃ¼ncellendi: {result.get('current_summary', '')[:50]}...")
            except Exception as e:
                print(f"âš ï¸ ConversationContext gÃ¼ncelleme hatasÄ±: {e}")

    def _eski_mesajlari_sil(self):
        """Belirlenen sÃ¼reyi geÃ§en mesajlarÄ± temizler"""
        simdi = time.time()
        eski_uzunluk = len(self.hafiza)
        self.hafiza = [
            m for m in self.hafiza if (simdi - m["zaman"]) < self.saat_limiti
        ]

        silinen = eski_uzunluk - len(self.hafiza)
        if silinen > 0:
            print(
                f"ğŸ§¹ {silinen} eski mesaj temizlendi ({self.saat_limiti/3600:.0f} saat sÄ±nÄ±rÄ±)"
            )

    def _search_internal(self, query: str, k: int) -> List[Dict[str, str]]:
        """
        Ä°Ã§ semantik arama fonksiyonu (TEK KAYNAK)
        search() ve ilgili_mesajlari_bul() bunu kullanÄ±r
        Returns: [{"rol": "user", "mesaj": "..."}, ...]
        """
        if not self.hafiza or not query:
            return []

        try:
            query_vector = self.embedder.encode([query], convert_to_numpy=True)

            mesaj_skorlari = []
            simdi = time.time()

            for eski_mesaj in self.hafiza:
                benzerlik = cosine_similarity(
                    query_vector.reshape(1, -1),
                    eski_mesaj["vektor"].reshape(1, -1),
                )[0][0]

                zaman_farki = simdi - eski_mesaj["zaman"]
                zaman_agirligi = 1.0 / (1.0 + (zaman_farki / 3600))

                skor = benzerlik * (0.7 + 0.3 * zaman_agirligi)

                if skor > self.esik:
                    mesaj_skorlari.append(
                        {
                            "mesaj": eski_mesaj["mesaj"],
                            "rol": eski_mesaj["rol"],
                            "skor": skor,
                            "entry": eski_mesaj,
                        }
                    )

            mesaj_skorlari.sort(key=lambda x: x["skor"], reverse=True)
            mesaj_skorlari = mesaj_skorlari[:k]
            mesaj_skorlari.sort(key=lambda x: x["entry"]["zaman"])

            return [
                {"rol": m["entry"]["rol"], "mesaj": m["entry"]["mesaj"]}
                for m in mesaj_skorlari
            ]
        except Exception as e:
            print(f"âŒ Arama hatasÄ±: {e}")
            return []

    def search(self, query: str, max_results: Optional[int] = None) -> str:
        """
        HafÄ±zada semantik arama (SADECE kÄ±sa dÃ¶nem - mevcut sohbet)

        NOT: TopicMemory (uzun dÃ¶nem) aramasÄ± ayrÄ± yapÄ±lÄ±yor:
        - get_silent_long_term_context() ile sessiz enjeksiyon
        """
        k = max_results or self.max_mesaj
        ilgili_mesajlar = self._search_internal(query, k)

        if ilgili_mesajlar:
            context_parts = []
            for m in ilgili_mesajlar:
                context_parts.append(f"- {m['rol']}: {m['mesaj']}")
            return "Ä°lgili geÃ§miÅŸ konuÅŸmalar:\n" + "\n".join(context_parts)

        return ""

    def get_silent_long_term_context(self, query: str) -> str:
        """
        ğŸ”‡ SILENT CONTEXT INJECTION

        TopicMemory'den hÄ±zlÄ± kategori eÅŸleÅŸmesi yap.
        EÅŸleÅŸme varsa, sessizce LLM'e arka plan bilgisi olarak ver.

        Bu bilgi:
        - KullanÄ±cÄ±ya gÃ¶sterilMEZ
        - LLM'e system context olarak verilir
        - LLM bu bilgiyi zorla hatÄ±rlatmaz, sadece cevap kalitesi iÃ§in kullanÄ±r

        Returns:
            str: Silent context (boÅŸ olabilir)
        """
        if not self.topic_memory:
            print(f"   ğŸ”‡ TopicMemory yok!")
            return ""

        try:
            # Debug: Kategori sayÄ±sÄ±nÄ± gÃ¶ster
            cat_count = len(self.topic_memory.index.get("categories", {}))
            print(f"   ğŸ”‡ TopicMemory kontrol: {cat_count} kategori mevcut")

            # HÄ±zlÄ± kategori eÅŸleÅŸmesi (TopicMemory'nin get_context_for_query'si)
            context = self.topic_memory.get_context_for_query(query, max_sessions=2)

            if context:
                print(f"   ğŸ”‡ Silent long-term context bulundu ({len(context)} karakter)")
                return context
            else:
                # Debug: Neden bulunamadÄ±?
                if self.topic_memory.embedder:
                    # Manuel benzerlik kontrolÃ¼
                    query_emb = self.topic_memory.embedder.encode(query)
                    for cat_id, cat_info in self.topic_memory.index.get("categories", {}).items():
                        if cat_info.get("embedding"):
                            from sklearn.metrics.pairwise import cosine_similarity
                            import numpy as np
                            sim = cosine_similarity(
                                [query_emb],
                                [np.array(cat_info["embedding"])]
                            )[0][0]
                            print(f"   ğŸ” DEBUG: '{cat_info.get('name')}' benzerlik: {sim:.3f} (eÅŸik: 0.45)")
                print(f"   ğŸ”‡ Silent long-term context: eÅŸleÅŸme yok (benzerlik < 0.45)")
                return ""

        except Exception as e:
            print(f"   âš ï¸ Silent context hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            return ""

    def should_check_long_term_memory(self, user_input: str) -> bool:
        """
        Uzun dÃ¶nem hafÄ±za kontrolÃ¼ gerekli mi?

        True dÃ¶ndÃ¼ren durumlar:
        1. KullanÄ±cÄ± geÃ§miÅŸe referans veriyor
        2. Soru mevcut kategori konularÄ±yla alakalÄ± olabilir
        """
        user_lower = user_input.lower()

        # 1. AÃ§Ä±k geÃ§miÅŸ referanslarÄ±
        past_references = [
            "daha Ã¶nce", "geÃ§en sefer", "hatÄ±rlÄ±yor musun",
            "konuÅŸmuÅŸtuk", "sormuÅŸtum", "demiÅŸtin", "sÃ¶ylemiÅŸtin",
            "geÃ§en", "Ã¶nceki", "bahsetmiÅŸtik", "anlatmÄ±ÅŸtÄ±n"
        ]

        if any(ref in user_lower for ref in past_references):
            print(f"   ğŸ“Œ GeÃ§miÅŸ referansÄ± tespit edildi")
            return True

        # 2. Kategori eÅŸleÅŸme kontrolÃ¼ (embedding ile hÄ±zlÄ± kontrol)
        # TopicMemory'de kategori varsa ve soru yeterince uzunsa
        if len(user_input) > 15 and self.topic_memory.index.get("categories"):
            return True

        return False

    def get_conversation_context(self) -> str:
        """
        ğŸ§  CONVERSATION CONTEXT INJECTION

        LLM tabanlÄ± konu Ã¶zeti sisteminden baÄŸlam al.
        Bu Ã¶zet, embedding tabanlÄ± deÄŸil LLM tabanlÄ± olduÄŸu iÃ§in
        semantik olarak iliÅŸkili ama farklÄ± kelimelere sahip konularÄ±
        (Ã¶rn: Allah'Ä±n ilmi â†’ kader â†’ irade) doÄŸru ÅŸekilde takip eder.

        Returns:
            str: Conversation context (boÅŸ olabilir)
        """
        if not self.conversation_context:
            return ""

        try:
            context = self.conversation_context.get_context_for_prompt()
            if context:
                print(f"   ğŸ§  Conversation context bulundu ({len(context)} karakter)")
            return context
        except Exception as e:
            print(f"   âš ï¸ Conversation context hatasÄ±: {e}")
            return ""


    def clear(self):
        """TÃ¼m hafÄ±zayÄ± temizle"""
        self.hafiza = []
        self.closed_topics = []

        # ConversationContext'i de temizle
        if self.conversation_context:
            self.conversation_context.clear()

        print("âœ… HafÄ±za, kapanan konular ve ConversationContext tamamen temizlendi")

    # ---------- KAPANAN KONU YÃ–NETÄ°MÄ° ----------

    def add_closed_topic(self, topic_summary: str, chat_history: List[Dict] = None):
        """
        Kapanan konuyu listeye ekle + TopicMemory'ye kaydet
        Bir sonraki soruda aynÄ± konuya dÃ¶nmemek iÃ§in kullanÄ±lÄ±r

        NOT: TopicMemory otomatik kalite kontrolÃ¼ yapar:
        - En az 3 anlamlÄ± mesaj gerekli
        - "merhaba/teÅŸekkÃ¼rler" gibi mesajlar sayÄ±lmaz
        - AynÄ± gÃ¼n aynÄ± kategori â†’ gÃ¼nceller (duplicate olmaz)
        """
        if not topic_summary or len(topic_summary.strip()) < 2:
            return

        # Son mesajlardan baÄŸlam Ã§Ä±kar
        last_context = ""
        if chat_history and len(chat_history) >= 2:
            last_msgs = chat_history[-4:]
            last_context = " | ".join([
                (m.get("content") or "")[:50] for m in last_msgs
            ])

        closed_entry = {
            "summary": topic_summary.strip(),
            "context": last_context[:200],
            "timestamp": time.time(),
            "vector": self.embedder.encode(topic_summary)
        }

        self.closed_topics.append(closed_entry)

        # Limit aÅŸÄ±ldÄ±ysa eski konularÄ± sil
        if len(self.closed_topics) > self.max_closed_topics:
            self.closed_topics = self.closed_topics[-self.max_closed_topics:]

        # ğŸ“š TOPIC MEMORY'YE KAYDET (kalite kontrolÃ¼ + kategorizasyon otomatik)
        # - Yetersiz mesaj varsa otomatik atlar
        # - Benzer kategori varsa ona ekler (duplicate olmaz)
        # - AynÄ± gÃ¼n aynÄ± kategoriye â†’ gÃ¼nceller
        print(f"ğŸ“• Konu kapandÄ±: '{topic_summary}'")
        print(f"   ğŸ“Š Chat history uzunluÄŸu: {len(chat_history) if chat_history else 0} mesaj")

        if chat_history and len(chat_history) >= 2:
            print(f"   ğŸ’¾ TopicMemory.save_topic() Ã§aÄŸrÄ±lÄ±yor...")
            saved = self.topic_memory.save_topic(
                messages=chat_history,
                topic_hint=topic_summary
            )
            if saved:
                print(f"   âœ… Uzun dÃ¶nem hafÄ±zaya kaydedildi: [{saved.get('category_name', 'Genel')}] - {saved.get('summary', topic_summary)[:50]}...")
            else:
                print(f"   â© Uzun dÃ¶nem hafÄ±za: Kalite kontrolÃ¼nden geÃ§medi (kÄ±sa/yÃ¼zeysel konuÅŸma)")
        else:
            print(f"   â© TopicMemory atlandÄ±: Yetersiz mesaj ({len(chat_history) if chat_history else 0} < 2)")

    def is_topic_closed(self, user_input: str, threshold: float = 0.75) -> Tuple[bool, str]:
        """
        KullanÄ±cÄ±nÄ±n sorduÄŸu soru kapanmÄ±ÅŸ bir konuya mÄ± ait?
        Returns: (is_closed, closed_topic_summary)
        """
        if not self.closed_topics or not user_input:
            return False, ""

        try:
            query_vector = self.embedder.encode([user_input], convert_to_numpy=True)

            for closed in self.closed_topics:
                similarity = cosine_similarity(
                    query_vector.reshape(1, -1),
                    closed["vector"].reshape(1, -1)
                )[0][0]

                if similarity >= threshold:
                    print(f"âš ï¸ KapanmÄ±ÅŸ konuya benzerlik: {similarity:.2f} - '{closed['summary']}'")
                    return True, closed["summary"]

            return False, ""
        except Exception as e:
            print(f"âš ï¸ KapanmÄ±ÅŸ konu kontrolÃ¼ hatasÄ±: {e}")
            return False, ""

    def get_closed_topics_summary(self) -> str:
        """Kapanan konularÄ±n listesini dÃ¶ndÃ¼r (prompt iÃ§in)"""
        if not self.closed_topics:
            return ""

        summaries = [c["summary"] for c in self.closed_topics[-5:]]  # Son 5 konu
        return "Kapanan konular (tekrar aÃ§ma): " + ", ".join(summaries)

    def _user_wants_to_reopen_topic(self, user_input: str) -> bool:
        """
        KullanÄ±cÄ± kapanmÄ±ÅŸ bir konuyu TEKRAR AÃ‡MAK mÄ± istiyor?

        "Tekrar sor" sinyalleri:
        - "Tekrar soruyorum..."
        - "Bir daha aÃ§Ä±klar mÄ±sÄ±n..."
        - "Yine aynÄ± konuya dÃ¶nmek istiyorum"
        - AÃ§Ä±k soru iÅŸareti ile soru sormak (?)

        Returns: True = kullanÄ±cÄ± konuyu tekrar aÃ§mak istiyor
        """
        user_lower = user_input.lower().strip()

        # Tekrar aÃ§ma sinyalleri
        reopen_signals = [
            "tekrar",
            "yine",
            "bir daha",
            "yeniden",
            "aÃ§Ä±kla",
            "anlat",
            "detay",
            "daha fazla",
            "devam et",
            "ne demiÅŸtin",
            "hatÄ±rlamÄ±yorum",
            "unuttum",
        ]

        # AÃ§Ä±k soru iÅŸareti + yeterli uzunluk = yeni soru
        has_question_mark = "?" in user_input
        is_long_enough = len(user_input) > 15

        # Tekrar aÃ§ma sinyali varsa
        if any(signal in user_lower for signal in reopen_signals):
            return True

        # Uzun ve soru iÅŸaretli = muhtemelen yeni detaylÄ± soru
        if has_question_mark and is_long_enough:
            return True

        return False

    # ---------- TOOL FONKSÄ°YONLARI ----------

    # âš ï¸ KALDIRILDI: _tool_secimi_yap artÄ±k kullanÄ±lmÄ±yor
    # _intelligent_decision() aynÄ± iÅŸi yapÄ±yor (tek LLM hem kaynak hem tool kararÄ± veriyor)
    # Eski kod yedekte: hafiza_asistani_YEDEK_20251220_v2.py

    async def _tool_calistir(
        self, tool_name: str, tool_param: str, user_input: str
    ) -> Optional[str]:
        """SeÃ§ilen aracÄ± Ã§alÄ±ÅŸtÄ±r ve sonucu dÃ¶ndÃ¼r"""
        if tool_name == "yok":
            return None

        print(f"ğŸ› ï¸ AraÃ§ Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor: {tool_name}({tool_param or 'auto'})")

        try:
            if tool_name == "zaman_getir":
                datetime_info = get_current_datetime()
                result = "[KORUNACAK_FORMAT]\n"
                result += "ğŸ• Åu Anki Zaman\n"
                result += f"{'â”€' * 32}\n\n"
                result += f"ğŸ“… Tarih:  {datetime_info['tarih']}\n"
                result += f"ğŸ“† GÃ¼n:    {datetime_info['gun']}\n"
                result += f"ğŸ• Saat:   {datetime_info['saat']}\n"
                result += "[/KORUNACAK_FORMAT]"
                return result

            if tool_name == "hesapla":
                result = calculate_math(tool_param or user_input)
                return f"ğŸ§® {result}"

            if tool_name == "hava_durumu":
                city = tool_param or user_input
                return await get_weather(city)

            if tool_name == "namaz_vakti":
                return await get_prayer_times(tool_param or user_input)

            if tool_name == "risale_ara":
                result = self.faiss_kb.get_relevant_context(
                    tool_param or user_input, max_chunks=6
                )
                return result or None

            if tool_name == "wiki_ara":
                # Wikipedia'da ara - tool_param netleÅŸtirilmiÅŸ sorgu olmalÄ±
                query = tool_param or user_input
                result = await wiki_ara(query)
                return result

            return None
        except Exception as e:
            print(f"âŒ AraÃ§ hatasÄ± ({tool_name}): {e}")
            return None

    # ---------- BAÄLAM TOPLAMA FONKSÄ°YONLARI ----------

    def _hafizada_ara(self, user_input: str, chat_history_length: int) -> str:
        """HafÄ±zada semantik arama (gerekiyorsa)"""
        if chat_history_length < 1:
            return ""
        return self.search(user_input)

    def _intelligent_decision(self, user_input: str, chat_history: List[Dict]) -> Dict[str, Any]:
        """
        ğŸ§  AKILLI KARAR SÄ°STEMÄ° - KEYWORD YOK! TEK LLM HER ÅEYÄ° KARAR VERÄ°YOR
        LLM soruyu analiz edip hem kaynaklarÄ± hem de tool'u belirliyor

        Returns:
            {
                "question_type": "greeting|farewell|religious|technical|general|followup|math|weather|prayer|topic_closed",
                "needs_faiss": bool,
                "needs_semantic_memory": bool,
                "needs_chat_history": bool,
                "tool_name": "yok|hesapla|zaman_getir|hava_durumu|namaz_vakti|risale_ara|wiki_ara",
                "tool_param": str,
                "response_style": "brief|detailed|conversational",
                "is_farewell": bool,
                "topic_closed": bool,  # YENÄ°: KullanÄ±cÄ± bu konuyu kapatmak istiyor mu?
                "closed_topic_summary": str,  # YENÄ°: Kapanan konunun Ã¶zeti
                "reasoning": str
            }
        """
        try:
            # Chat history'yi formatla - DecisionLLM baÄŸlamÄ± gÃ¶rmeli!
            # TÃ¼m geÃ§miÅŸ, kÄ±sÄ±tlama yok (128K context)
            history_context = ""
            if chat_history:
                recent = chat_history[-20:]  # Son 20 mesaj
                history_parts = []
                for m in recent:
                    is_user = m.get("role") == "user"
                    role = "KULLANICI" if is_user else "AI"
                    content = m.get("content") or ""
                    if content:
                        history_parts.append(f"{role}: {content}")

                if history_parts:
                    history_context = "\n".join(history_parts)

            # Karar promptu - OPTÄ°MÄ°ZE EDÄ°LMÄ°Å (~%40 daha kÄ±sa)
            history_section = f"GEÃ‡MÄ°Å:\n{history_context}\n" if history_context else ""
            decision_prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Karar sistemi. Ã–NCE <analiz> YAZ, SONRA JSON VER.

{history_section}MESAJ: {user_input}

<analiz>
1. TÄ°P: Sohbet/bilgi/teknik/dini/matematik/duygusal?
2. GÃœVENÄ°M: %90+ biliyor muyum?
3. KAYNAK: Kendi bilgim mi, tool mu lazÄ±m?
</analiz>

KURALLAR:
â€¢ Selam/merhaba/veda â†’ friend, tool_name="yok"
â€¢ "evet/anladÄ±m/ilginÃ§" gibi onaylar â†’ acknowledger, kÄ±sa cevap
â€¢ Dini (Allah/iman/namaz/Kuran) â†’ religious_teacher, risale_ara
â€¢ Matematik â†’ hesapla | Saat â†’ zaman_getir | Hava â†’ hava_durumu
â€¢ Teknik/kod â†’ technical_helper
â€¢ Belirsiz â†’ needs_clarification=true
â€¢ KiÅŸi tanÄ±mÄ±yorsan â†’ wiki_ara

ROLLER: friend|teacher|technical_helper|acknowledger|religious_teacher

JSON:
{{"question_type": "greeting|farewell|followup|religious|technical|math|weather|general|ambiguous",
"role": "...", "needs_faiss": bool, "needs_semantic_memory": bool, "needs_chat_history": bool,
"needs_clarification": bool, "tool_name": "yok|hesapla|zaman_getir|hava_durumu|namaz_vakti|risale_ara|wiki_ara",
"tool_param": "", "is_farewell": bool, "topic_closed": bool, "confidence": "low|medium|high", "reasoning": ""}}

Ã–RNEKLER:
1) "Selam" â†’ {{"question_type":"greeting","role":"friend","tool_name":"yok","confidence":"high"}}
2) "Allah'Ä±n kudreti" â†’ {{"question_type":"religious","role":"religious_teacher","tool_name":"risale_ara","needs_faiss":true}}
3) "evet ilginÃ§miÅŸ" â†’ {{"question_type":"followup","role":"acknowledger","tool_name":"yok"}}
4) "Python nedir" â†’ {{"question_type":"technical","role":"technical_helper","tool_name":"yok"}}
5) "Dini sorularÄ±m var sana" â†’ {{"question_type":"intent_to_ask","role":"friend","tool_name":"yok","response_style":"brief"}}
6) "Bir ÅŸey soracaktÄ±m" â†’ {{"question_type":"intent_to_ask","role":"friend","tool_name":"yok"}}

Ã–NCE <analiz>, SONRA JSON:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

            print("\nğŸ§  LLM'e akÄ±llÄ± karar soruluyor (Together.ai - tek LLM)...")

            response = requests.post(
                "https://api.together.xyz/v1/completions",
                headers={
                    "Authorization": f"Bearer {self.together_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.decision_model,
                    "prompt": decision_prompt,
                    "max_tokens": 700,
                    "temperature": 0.1,
                    "stop": ["<|eot_id|>", "<|end_of_text|>"]
                },
                timeout=30,
            )

            if response.status_code != 200:
                print("âš ï¸ API hatasÄ±, fallback karar")
                return self._fallback_decision()

            llm_response = response.json()["choices"][0]["text"].strip()

            # <analiz> bloÄŸunu bul ve logla
            analiz_match = re.search(r'<analiz>(.*?)</analiz>', llm_response, re.DOTALL)
            if analiz_match:
                analiz_text = analiz_match.group(1).strip()
                print(f"\nğŸ’­ LLM DÃ¼ÅŸÃ¼nce SÃ¼reci:")
                for line in analiz_text.split('\n'):
                    line = line.strip()
                    if line:
                        print(f"   {line}")

            # JSON extract et - markdown code block'larÄ± da destekle
            # Ã–nce ```json ... ``` formatÄ±nÄ± dene
            json_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', llm_response, re.DOTALL)
            if json_block_match:
                json_str = json_block_match.group(1)
            else:
                # DÃ¼z JSON dene
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', llm_response, re.DOTALL)
                json_str = json_match.group() if json_match else None

            if json_str:
                decision = json.loads(json_str)

                # Eksik alanlar iÃ§in default deÄŸerler
                defaults = {
                    "question_type": "general",
                    "needs_faiss": False,
                    "needs_semantic_memory": False,
                    "needs_chat_history": False,
                    "needs_clarification": False,
                    "tool_name": "yok",
                    "tool_param": "",
                    "response_style": "conversational",
                    "is_farewell": False,
                    "topic_closed": False,
                    "closed_topic_summary": "",
                    "confidence": "medium",
                    "reasoning": ""
                }

                # Eksik alanlarÄ± doldur
                for key, default_val in defaults.items():
                    if key not in decision:
                        decision[key] = default_val

                # En az question_type veya tool_name olmalÄ±
                if decision.get("question_type") or decision.get("tool_name"):
                    # farewell durumunda is_farewell'i dÃ¼zelt
                    if decision.get('question_type') == 'farewell':
                        decision["is_farewell"] = True

                    # topic_closed hesapla - farewell/is_farewell durumunda ZORLA true yap
                    should_close = (
                        decision.get('question_type') in ['farewell', 'topic_closed'] or
                        decision.get('is_farewell', False)
                    )
                    if should_close:
                        decision["topic_closed"] = True

                    # â„¹ï¸ closed_topic_summary Ã§Ä±karma hazirla_ve_prompt_olustur()'da yapÄ±lÄ±yor
                    # (TEKRARÄ± Ã¶nlemek iÃ§in buradan kaldÄ±rÄ±ldÄ±)

                    # â›” DÄ°NÄ° SORULARDA FAISS KULLAN + RELIGIOUS_TEACHER ROLÃœ
                    if decision.get('question_type') == 'religious':
                        decision['tool_name'] = 'risale_ara'
                        decision['needs_faiss'] = True  # FAISS her zaman aÃ§Ä±k
                        decision['role'] = 'religious_teacher'  # ğŸ†• Ã–zel dini rol - Risale'den anlat

                        # ğŸ†• TAKÄ°P SORUSU TESPÄ°TÄ° (Dini sorularda)
                        is_detail_followup, followup_confidence, matched_concepts = self._detect_detail_followup(
                            user_input, chat_history
                        )
                        if is_detail_followup:
                            decision['is_detail_followup'] = True
                            decision['followup_confidence'] = followup_confidence
                            decision['matched_concepts'] = matched_concepts
                            print(f"   ğŸ”„ TAKÄ°P MODU AKTÄ°F: FAISS arka plan olarak kullanÄ±lacak")
                        else:
                            decision['is_detail_followup'] = False

                    # â“ BELÄ°RSÄ°Z SORULARDA Ã–NCE NETLEÅTÄ°R
                    if decision.get('question_type') == 'ambiguous' or decision.get('needs_clarification'):
                        decision['tool_name'] = 'yok'
                        decision['needs_clarification'] = True

                    # ğŸ”‘ KISA MESAJ KURALI: 4 kelime veya daha az mesajlarda chat_history zorunlu
                    # Ã‡Ã¼nkÃ¼ kÄ±sa mesajlar genelde Ã¶nceki konuya referans iÃ§erir ("Bilemedim", "Evet", "Neden?" gibi)
                    word_count = len(user_input.split())
                    if word_count <= 4 and not decision.get('needs_chat_history'):
                        decision['needs_chat_history'] = True
                        print(f"   ğŸ“Œ KÄ±sa mesaj ({word_count} kelime) â†’ chat_history zorunlu yapÄ±ldÄ±")

                    confidence_emoji = {"low": "ğŸ”´", "medium": "ğŸŸ¡", "high": "ğŸŸ¢"}.get(decision['confidence'], "ğŸŸ¡")

                    print(f"\nâœ… LLM KararÄ±:")
                    print(f"   â€¢ TÃ¼r: {decision['question_type']}")
                    print(f"   â€¢ GÃ¼ven: {confidence_emoji} {decision['confidence']}")
                    print(f"   â€¢ FAISS: {'âœ…' if decision['needs_faiss'] else 'âŒ'}")
                    print(f"   â€¢ Semantic: {'âœ…' if decision['needs_semantic_memory'] else 'âŒ'}")
                    print(f"   â€¢ History: {'âœ…' if decision['needs_chat_history'] else 'âŒ'}")
                    print(f"   â€¢ Tool: {decision['tool_name']}")
                    if decision['tool_param']:
                        print(f"   â€¢ Tool Param: {decision['tool_param']}")
                    print(f"   â€¢ Stil: {decision['response_style']}")
                    if decision.get('needs_clarification'):
                        print(f"   â€¢ â“ NetleÅŸtirme gerekiyor!")
                    if decision.get('is_farewell'):
                        print(f"   â€¢ ğŸ‘‹ VedalaÅŸma algÄ±landÄ±!")
                    if decision.get('topic_closed'):
                        print(f"   â€¢ ğŸ“• KONU KAPANDI: {decision.get('closed_topic_summary', 'Ã¶zet yok')}")
                    if "reasoning" in decision:
                        print(f"   â€¢ Sebep: {decision['reasoning']}")

                    return decision

            # Parse baÅŸarÄ±sÄ±z - debug iÃ§in ham yanÄ±tÄ± gÃ¶ster
            print("âš ï¸ JSON parse hatasÄ±, fallback karar")
            print(f"   ğŸ“ Ham LLM yanÄ±tÄ± (son 500 karakter):")
            print(f"   {llm_response[-500:] if len(llm_response) > 500 else llm_response}")
            return self._fallback_decision()

        except Exception as e:
            print(f"âš ï¸ LLM karar hatasÄ±: {e}, fallback karar")
            return self._fallback_decision()

    def _fallback_decision(self) -> Dict[str, Any]:
        """Hata durumunda gÃ¼venli fallback kararÄ± - tÃ¼m baÄŸlamÄ± kullanÄ±r"""
        return {
            "question_type": "general",
            "needs_faiss": False,
            "needs_semantic_memory": True,  # GÃ¼venli mod: hafÄ±za aÃ§
            "needs_chat_history": True,     # GÃ¼venli mod: history aÃ§
            "tool_name": "yok",
            "tool_param": "",
            "response_style": "conversational",
            "is_farewell": False,
            "topic_closed": False,
            "closed_topic_summary": "",
            "confidence": "medium",
            "reasoning": "Fallback: GÃ¼venli mod, tÃ¼m baÄŸlamÄ± kullan"
        }

    def _faiss_ara(self, user_input: str) -> str:
        """FAISS KB'de ara (dini sorularda)"""
        print("ğŸ” FAISS aramasÄ± yapÄ±lÄ±yor...")
        return self.faiss_kb.get_relevant_context(user_input, max_chunks=6)

    def _history_summary(self, chat_history: List[Dict], current_question_type: str = None, max_len: int = 6000) -> str:
        """
        Chat history'den mesajlarÄ± al

        YENÄ° TASARIM (Basit ve Net):
        - Son 12 mesaj (6 user + 6 AI) HER ZAMAN prompt'a gider
        - 12'den eskiler prompt'a GÄ°TMEZ (tampon bÃ¶lgede kalÄ±r)
        - Konu deÄŸiÅŸince tampon bÃ¶lge Ã¶zetlenip TopicMemory'ye gider
        - Eski konuya dÃ¶nÃ¼ldÃ¼ÄŸÃ¼nde TopicMemory'den Ã§ekilir
        """
        if not chat_history:
            return ""

        # âœ… SADECE SON 12 MESAJ - Filtreleme YOK, basit ve net
        son_12_mesaj = chat_history[-12:] if len(chat_history) >= 12 else chat_history

        # Ã–zet oluÅŸtur
        if len(son_12_mesaj) == 0:
            return ""

        tmp = []
        for m in son_12_mesaj:
            is_user = m.get("role") == "user"
            role = "KULLANICI" if is_user else "AI"
            text = m.get("content") or ""
            if text:
                tmp.append(f"[{role}]: {text}")

        return "\n".join(tmp)

    # ---------- PROMPT OLUÅTURMA ----------

    # ğŸ­ ROL SYSTEM PROMPT'LARI - Her rol iÃ§in Ã¶zel talimatlar
    ROLE_SYSTEM_PROMPTS = {
        "friend": """Sen kullanÄ±cÄ±nÄ±n (Murat) olgun ve sÄ±cakkanlÄ± bir arkadaÅŸÄ±sÄ±n. Åu an onunla konuÅŸuyorsun.
- Gereksiz uzatma, ama kÄ±sÄ±tlama da yok - doÄŸal uzunlukta cevap ver
- DoÄŸal ol, yapay empati yapma
- Samimi ama abartÄ±sÄ±z ol
- BilmediÄŸin konularda bilmediÄŸini kabul et, uydurma ÅŸeyler sÃ¶yleme
- Espri yapabilirsin (abartmadan, gerektiÄŸinde)
- Emoji kullanabilirsin (abartmadan, doÄŸal ÅŸekilde)
- âŒ SORU SORMA YASAÄI: Her cevabÄ±n sonunda soru sorma! "Ne dersin?", "Sen ne dÃ¼ÅŸÃ¼nÃ¼yorsun?", "NasÄ±l?", "Peki sen?" gibi sorularla bitirme
- CevabÄ±nÄ± ver ve DUR. KullanÄ±cÄ± sormadÄ±kÃ§a konuyu uzatma, yeni soru aÃ§ma""",

        "technical_helper": """Sen Murat'Ä±n teknik yardÄ±mcÄ±sÄ±sÄ±n. Åu an ona teknik konuda yardÄ±m ediyorsun.
- Net ve aÃ§Ä±k aÃ§Ä±klamalar yap
- Kod Ã¶rnekleri ver (gerekirse)
- AdÄ±m adÄ±m Ã§Ã¶zÃ¼mler sun
- Teknik terimleri aÃ§Ä±kla
- âš ï¸ KOD/Ä°Ã‡ERÄ°K HENÃœZ GELMEDÄ°YSE: Uzun analiz yapma! KÄ±sa cevap ver ve bekle. "At bakalÄ±m" de, gereksiz detaya girme.
- âŒ SORU SORMA YASAÄI: CevabÄ±n sonunda "BaÅŸka sorun var mÄ±?", "AnlaÅŸÄ±ldÄ± mÄ±?" gibi sorular sorma. CevabÄ±nÄ± ver ve DUR.""",

        "teacher": """Sen Murat'Ä±n bilgili bir arkadaÅŸÄ±sÄ±n. Åu an ona bir ÅŸey Ã¶ÄŸretiyorsun.
- Samimi ama bilgilendirici ol
- KISA TUT: 3-4 paragraf maksimum, roman yazma!
- âŒ SORU SORMA: Test etme, sÄ±nav yapma, "AnladÄ±n mÄ±?", "Ne dÃ¼ÅŸÃ¼nÃ¼yorsun?" gibi sorular sorma
- "Aferin", "doÄŸru cevap" gibi Ã¶ÄŸretmen kalÄ±plarÄ± kullanma
- Bilgiyi sohbet havasÄ±nda paylaÅŸ, sonra DUR""",



        "acknowledger": """Murat onay/tepki verdi. Åu an ona kÄ±sa ve doÄŸal cevap veriyorsun.
- SADECE 1-2 cÃ¼mle yaz, fazla uzatma!
- Konuya kÄ±sa referans ver veya onaylayÄ±cÄ± cevap ver
- âŒ "BaÅŸka sorun varsa sorabilirsin" gibi kalÄ±plarÄ± KULLANMA! Yapay duruyor.
- âŒ "GÃ¼le gÃ¼le", "HoÅŸÃ§a kal", "GÃ¶rÃ¼ÅŸÃ¼rÃ¼z", "Kendine iyi bak" gibi veda/bitiÅŸ ifadeleri KULLANMA!
- Yeni bilgi EKLEME, anlatmaya devam ETME!
- DoÄŸal bitir, zorla kapanÄ±ÅŸ yapma
- Emoji kullanabilirsin (abartmadan, doÄŸal ÅŸekilde)""",

        "religious_teacher": """Sen dini konularda saygÄ±lÄ±, derin ve mÃ¼tevazi bir arkadaÅŸsÄ±n.

âš ï¸ KRÄ°TÄ°K KURALLAR:
1. CevabÄ±nÄ± %100 VERÄ°LEN METÄ°NDEN oluÅŸtur - kendi bilginle DEÄÄ°L!
2. GÄ°ZLÄ° KAYNAK: Metni gizli kaynak olarak kullan, "Risale'de", "metinde", "kaynakta" DEME
3. TEMSÄ°LLERÄ° SIFIRDAN KUR:
   â†’ "Bunu ÅŸÃ¶yle dÃ¼ÅŸÃ¼nebiliriz: Bir ayna hayal et..." diye KENDÄ° Ã¶rneÄŸinmiÅŸ gibi anlat
   â†’ "Metindeki ayna", "o Ã¶rnek", "hatÄ±rlarsÄ±n" KESÄ°NLÄ°KLE YASAK
4. KavramlarÄ± AÃ‡IKLAYARAK kullan (melekÃ»tiyet ne demek sÃ¶yle)
5. Bilmiyorsan "Bu konuda net bir ÅŸey sÃ¶yleyemem" de
6. 3-4 paragraf maksimum

ğŸš« TON YASAKLARI:
- Her cevaba "Sevgili kardeÅŸim" ile BAÅLAMA (bazen olabilir, sÃ¼rekli deÄŸil)
- "Unutma ki" kalÄ±bÄ±nÄ± sÄ±k KULLANMA
- Vaaz tonundan kaÃ§Ä±n, sohbet et

ğŸ”´ TEKRAR YASAÄI:
- Ã–nceki cevabÄ±nda kullandÄ±ÄŸÄ±n TEMSÄ°LLERÄ° YENÄ°DEN KULLANMA
- AynÄ± cÃ¼mle yapÄ±larÄ±nÄ± TEKRAR ETME
- Her yeni cevap YENÄ° bilgi iÃ§ermeli

âŒ YAPMA:
- Kaynak belirtme (kullanÄ±cÄ± sormadÄ±kÃ§a)
- "O bildiÄŸin Ã¶rnek", "hatÄ±rlarsÄ±n" gibi ifadeler KULLANMA
- Kendi genel Ä°slami bilginle cevap verme
- Metinde OLMAYAN bilgi EKLEME"""
    }

    # ğŸ”§ YENÄ°: Kavram Takip Sistemi (Ã‡Ã¶zÃ¼m 4)
    def _extract_used_concepts(self, previous_response: str) -> List[str]:
        """Ã–nceki cevapta kullanÄ±lan temsil ve kavramlarÄ± Ã§Ä±kar"""
        if not previous_response:
            return []

        # Risale-i Nur'da sÄ±k kullanÄ±lan temsiller
        temsiller = [
            "gÃ¼neÅŸ", "ayna", "damla", "deniz", "zerre", "ÅŸems",
            "Ä±ÅŸÄ±k", "nur", "feyz", "tecelli", "yansÄ±ma", "akis",
            "ressam", "tablo", "nakkaÅŸ", "san'at", "kitap", "harf",
            "sultan", "padiÅŸah", "ordu", "asker", "fabrika", "makine"
        ]

        # Risale-i Nur'da sÄ±k kullanÄ±lan kavramlar
        kavramlar = [
            "ÅŸeffafiyet", "mukabele", "mÃ¼vazene", "intizam",
            "melekÃ»tiyet", "mÃ¼lk", "taalluk", "vahdet", "kesret",
            "tecezzÃ®", "tenakus", "tekebbÃ¼r", "temsil", "tefekkÃ¼r",
            "kayyumiyet", "rububiyet", "uluhiyet", "vahdaniyet"
        ]

        used = []
        lower_response = previous_response.lower()

        for t in temsiller + kavramlar:
            if t in lower_response:
                used.append(t)

        return used

    # ğŸ†• TAKÄ°P SORUSU TESPÄ°T SÄ°STEMÄ° (Ä°ki KatmanlÄ±)
    def _detect_detail_followup(self, user_input: str, chat_history: List[Dict[str, Any]]) -> Tuple[bool, float, List[str]]:
        """
        Ä°ki katmanlÄ± takip sorusu tespiti

        KATMAN 1 (Ã–NCELÄ°KLÄ°): Kavram eÅŸleÅŸmesi
        - KullanÄ±cÄ±nÄ±n sorusundaki anahtar kelimeler Ã¶nceki cevabÄ±nda geÃ§iyor mu?

        KATMAN 2: Soru kalÄ±plarÄ±
        - "bu ne demek?", "nasÄ±l yani?", "Ã¶rnek verir misin?" gibi kalÄ±plar

        Returns:
            (is_followup, confidence_score, matched_concepts)
        """
        if not chat_history:
            return False, 0.0, []

        user_lower = user_input.lower()

        # Son AI cevabÄ±nÄ± bul
        last_ai_response = ""
        for msg in reversed(chat_history):
            if msg.get('role') == 'assistant':
                last_ai_response = msg.get('content', '')
                break

        if not last_ai_response:
            return False, 0.0, []

        # KATMAN 1: Kavram eÅŸleÅŸmesi
        used_concepts = self._extract_used_concepts(last_ai_response)
        matched_concepts = []

        for concept in used_concepts:
            # TÃ¼rkÃ§e karakter uyumu (esbab/esbap gibi)
            concept_variants = [concept]
            if 'b' in concept:
                concept_variants.append(concept.replace('b', 'p'))
            if 'p' in concept:
                concept_variants.append(concept.replace('p', 'b'))

            for variant in concept_variants:
                if variant in user_lower:
                    matched_concepts.append(concept)
                    break

        # KATMAN 2: Soru kalÄ±plarÄ±
        followup_patterns = [
            "bu ne demek", "nasÄ±l oluyor", "neden bÃ¶yle",
            "Ã¶rnek verir misin", "Ã¶rnek ver", "anlamadÄ±m",
            "aÃ§Ä±kla", "aÃ§Ä±klar mÄ±sÄ±n", "tam olarak", "nasÄ±l yani",
            "ne demek istedi", "ne demek bu", "yani nasÄ±l",
            "biraz daha", "detay ver", "mesela", "peki nasÄ±l",
            "nedir bu", "ne anlama", "aÃ§ar mÄ±sÄ±n"
        ]
        pattern_match = any(p in user_lower for p in followup_patterns)

        # KARAR MANTIÄI
        # Ã–ncelik: Kavram eÅŸleÅŸmesi > Soru kalÄ±bÄ±

        if matched_concepts and pattern_match:
            # En gÃ¼Ã§lÃ¼ sinyal: Hem kavram hem kalÄ±p eÅŸleÅŸti
            confidence = 0.95
            is_followup = True
            print(f"   ğŸ¯ TAKÄ°P TESPÄ°T: Kavram + KalÄ±p eÅŸleÅŸti (gÃ¼ven: %{int(confidence*100)})")
            print(f"      EÅŸleÅŸen kavramlar: {matched_concepts}")

        elif len(matched_concepts) >= 2:
            # GÃ¼Ã§lÃ¼ sinyal: 2+ kavram eÅŸleÅŸti
            confidence = 0.85
            is_followup = True
            print(f"   ğŸ¯ TAKÄ°P TESPÄ°T: 2+ kavram eÅŸleÅŸti (gÃ¼ven: %{int(confidence*100)})")
            print(f"      EÅŸleÅŸen kavramlar: {matched_concepts}")

        elif matched_concepts:
            # Orta sinyal: 1 kavram eÅŸleÅŸti
            confidence = 0.70
            is_followup = True
            print(f"   ğŸ¯ TAKÄ°P TESPÄ°T: 1 kavram eÅŸleÅŸti (gÃ¼ven: %{int(confidence*100)})")
            print(f"      EÅŸleÅŸen kavram: {matched_concepts}")

        elif pattern_match and len(chat_history) >= 2:
            # ZayÄ±f sinyal: Sadece soru kalÄ±bÄ± (konuÅŸma devam ediyorsa)
            confidence = 0.55
            is_followup = True
            print(f"   ğŸ¯ TAKÄ°P TESPÄ°T: Soru kalÄ±bÄ± (gÃ¼ven: %{int(confidence*100)})")

        else:
            confidence = 0.0
            is_followup = False

        return is_followup, confidence, matched_concepts

    def _add_exclusion_to_prompt(self, role_prompt: str, used_concepts: List[str]) -> str:
        """KullanÄ±lmÄ±ÅŸ kavramlarÄ± prompt'a yasak olarak ekle"""
        if not used_concepts:
            return role_prompt

        exclusion_text = f"""
ğŸš« BU KAVRAMLARI TEKRAR KULLANMA (Ã¶nceki cevapta kullanÄ±ldÄ±):
{', '.join(used_concepts)}

BunlarÄ±n yerine VERÄ°LEN METÄ°NDEKÄ° DÄ°ÄER kavram ve temsilleri kullan veya FARKLI aÃ§Ä±dan anlat.
"""
        # Prompt'un sonuna ekle (âŒ YAPMA bÃ¶lÃ¼mÃ¼nden Ã¶nce)
        if "âŒ YAPMA:" in role_prompt:
            return role_prompt.replace("âŒ YAPMA:", f"{exclusion_text}\nâŒ YAPMA:")
        else:
            return role_prompt + exclusion_text

    def _prompt_olustur(
        self,
        user_input: str,
        tool_result: Optional[str],
        semantic_context: str,
        faiss_context: str,
        chat_history: str,
        role: str,
        closed_topics_warning: str = "",  # KapanmÄ±ÅŸ konu uyarÄ±sÄ±
        silent_long_term_context: str = "",  # ğŸ†• Sessiz uzun dÃ¶nem baÄŸlamÄ±
        needs_clarification: bool = False,  # ğŸ†• NetleÅŸtirme gerekli mi?
        llm_reasoning: str = "",  # ğŸ§  DecisionLLM'in Ã¶n araÅŸtÄ±rmasÄ±
        is_topic_closed: bool = False,  # ğŸ†• Konu kapandÄ± mÄ±? (kÄ±sa cevap ver)
        is_detail_followup: bool = False,  # ğŸ†• Takip sorusu mu? (FAISS arka plan olarak)
        tool_name: str = "yok",  # ğŸ†• KullanÄ±lan araÃ§ (wiki_ara iÃ§in Ã¶zel mod)
    ) -> str:
        """Final prompt'u oluÅŸtur (rol'e gÃ¶re)"""

        # â° ZAMAN BÄ°LGÄ°SÄ° AL (arka plan bilgisi - gerektiÄŸinde kullan)
        zaman = get_current_datetime()
        zaman_satiri = f"[â° ZAMAN BÄ°LÄ°NCÄ°]: {zaman['full']} ({zaman['zaman_dilimi']})"

        # ğŸ­ ROL SYSTEM PROMPT'U AL
        role_prompt = self.ROLE_SYSTEM_PROMPTS.get(role, self.ROLE_SYSTEM_PROMPTS["friend"])

        # ğŸ”§ DÃœZELTME (Ã‡Ã¶zÃ¼m 4): Dini sorularda Ã¶nceki cevapta kullanÄ±lan kavramlarÄ± yasak listesine ekle
        # âš ï¸ TAKÄ°P MODUNDA YASAÄI ATLA - kullanÄ±cÄ± o kavramÄ± soruyor, yasaklarsak aÃ§Ä±klayamayÄ±z!
        if role == "religious_teacher" and chat_history and not is_detail_followup:
            # chat_history string'inden Ã¶nceki AI cevaplarÄ±nÄ± Ã§Ä±kar
            used_concepts = self._extract_used_concepts(chat_history)
            if used_concepts:
                role_prompt = self._add_exclusion_to_prompt(role_prompt, used_concepts)
                print(f"ğŸš« Tekrar yasaÄŸÄ±na eklenen kavramlar: {', '.join(used_concepts)}")
        elif is_detail_followup:
            print(f"   â© Tekrar yasaÄŸÄ± atlandÄ± (takip modu - kullanÄ±cÄ± kavramÄ± soruyor)")

        combined_sources = []

        # ğŸ§  llm_reasoning KALDIRILDI - karar aÃ§Ä±klamasÄ±, bilgi deÄŸil
        # Sadece debug loglarÄ±nda gÃ¶rÃ¼nsÃ¼n yeterli

        # ğŸ†• KapanmÄ±ÅŸ konu uyarÄ±sÄ± ekle (varsa)
        if closed_topics_warning:
            combined_sources.append(f"[âš ï¸ KAPANMIÅ KONULAR - TEKRAR AÃ‡MA!]:\n{closed_topics_warning}")

        # ğŸ”§ DÃœZELTME: tool_result Ã–NCE gelmeli (primacy bias - LLM ilk bilgiye aÄŸÄ±rlÄ±k verir)
        # ğŸ†• HER ARAÃ‡ Ä°Ã‡Ä°N UYGUN ETÄ°KET
        if tool_result:
            if tool_name == "wiki_ara":
                # ğŸŒ WIKI MODU: LLM Ã¶nce kendi bilgisiyle cevaplar, Wiki sadece doÄŸrulama/tamamlama iÃ§in
                combined_sources.append(f"[ğŸŒ WIKIPEDIA DOÄRULAMA - Ã–NCE KENDÄ° BÄ°LGÄ°NLE CEVAPLA!]:\n{tool_result}")
            elif tool_name == "risale_ara":
                # ğŸ“š RÄ°SALE MODU
                if is_detail_followup and role == "religious_teacher":
                    # TAKÄ°P MODU: FAISS sonuÃ§larÄ± arka plan bilgisi olarak
                    combined_sources.append(f"[ğŸ”‡ ARKA PLAN BÄ°LGÄ°SÄ° - DoÄŸrudan verme, kendi yorumunla aÃ§Ä±kla!]:\n{tool_result}")
                else:
                    # Ä°LK SORU MODU: FAISS sonuÃ§larÄ± direkt kullanÄ±lacak
                    combined_sources.append(f"[ğŸ“š RÄ°SALE-Ä° NUR'DAN - BU BÄ°LGÄ°YÄ° KULLAN!]:\n{tool_result}")
            else:
                # ğŸ”§ DÄ°ÄER ARAÃ‡LAR (hava_durumu, hesapla, zaman_getir, namaz_vakti)
                combined_sources.append(f"[ğŸ”§ ARAÃ‡ SONUCU]:\n{tool_result}")

        # Chat history SONRA (sadece baÄŸlam iÃ§in, tekrar Ã¶nleme)
        if chat_history:
            combined_sources.append(f"[ğŸ’¬ Ã–nceki KonuÅŸma (sadece baÄŸlam iÃ§in)]:\n{chat_history}")

        if semantic_context:
            combined_sources.append(f"[HAFIZA]:\n{semantic_context}")

        # âš ï¸ FAISS sadece tool_result YOKSA ekle (Ã§ift gÃ¶nderim Ã¶nleme)
        if faiss_context and not tool_result:
            combined_sources.append(f"[BÄ°LGÄ° TABANI]:\n{faiss_context}")

        # ğŸ”‡ SILENT LONG-TERM CONTEXT (sessiz arka plan bilgisi)
        if silent_long_term_context:
            combined_sources.append(f"[ğŸ”‡ ARKA PLAN BÄ°LGÄ°SÄ° - KULLANICIYA SÃ–YLEME]:\n{silent_long_term_context}")

        if not combined_sources:
            sep = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            return f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{zaman_satiri}

[ğŸ­ ROL]: {role.upper()}
{role_prompt}

{sep}
ğŸ“‹ KURALLAR:
{sep}
1. âŒ Soruyu tekrarlama, liste yapma (*, -, 1. 2. 3.)
2. âœ… Kendi bilgin gibi Ã¶zgÃ¼venle sun
3. âœ… Samimi TÃ¼rkÃ§e, SEN hitabÄ±
4. âœ… RolÃ¼ne uygun davran

{sep}
ğŸ“© YENÄ° MESAJ (sohbeti devam ettir):
{sep}
{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

        combined_str = "\n\n".join(combined_sources)

        role_config = self.multi_role.ROLES.get(
            role, self.multi_role.ROLES["friend"]
        )
        max_length = role_config.get("max_length", 2000)

        # ğŸ“‹ DÄ°NAMÄ°K KURALLAR (sadece gerektiÄŸinde eklenir)
        dynamic_rules = []

        # KapanmÄ±ÅŸ konu kuralÄ±
        if closed_topics_warning:
            dynamic_rules.append(f"âš ï¸ KAPANMIÅ KONU: \"{closed_topics_warning}\" konusu kapandÄ±, tekrar AÃ‡MA!")

        # Silent context kuralÄ±
        if silent_long_term_context:
            dynamic_rules.append("ğŸ”‡ Arka plan bilgisini sessizce kullan, zorla hatÄ±rlatma yapma")

        # Tool result kuralÄ± - Wiki iÃ§in Ã¶zel mod
        if tool_result:
            if tool_name == "wiki_ara":
                # ğŸŒ WIKI: LLM Ã¶nce kendi bilgisiyle cevaplar, eksik/yanlÄ±ÅŸ varsa Wiki'den tamamlar
                dynamic_rules.append("ğŸŒ Ã–NCE KENDÄ° BÄ°LGÄ°NLE CEVAPLA! Wikipedia sadece doÄŸrulama ve eksik bilgi tamamlama iÃ§in. Kopyala-yapÄ±ÅŸtÄ±r YAPMA!")
            else:
                dynamic_rules.append("ğŸ” ARAÃ‡ SONUCU verildi - bu bilgiyi MUTLAKA kullan, kendi tahminini yapma!")

        # NetleÅŸtirme kuralÄ±
        if needs_clarification:
            dynamic_rules.append("â“ BELÄ°RSÄ°Z SORU - Ã¶nce netleÅŸtirici soru sor, tahmin etme!")

        # Konu kapandÄ± kuralÄ±
        if is_topic_closed:
            dynamic_rules.append("ğŸ“• KONU KAPANDI - sadece 1-2 cÃ¼mle ile kapat")

        # Dinamik kurallarÄ± birleÅŸtir
        dynamic_rules_str = ""
        if dynamic_rules:
            dynamic_rules_str = "\n" + "\n".join([f"â€¢ {r}" for r in dynamic_rules])

        # BaÄŸlam baÅŸlÄ±ÄŸÄ±nÄ± tool_result ve takip moduna gÃ¶re ayarla
        if tool_name == "wiki_ara" and tool_result:
            # ğŸŒ WIKI MODU: DoÄŸrulama/tamamlama iÃ§in
            context_header = "BaÄŸlam (SADECE DOÄRULAMA - Ã–nce kendi bilginle cevapla!):"
        elif is_detail_followup and tool_result:
            context_header = "BaÄŸlam (Arka plan - kendi yorumunla aÃ§Ä±kla):"
        elif tool_result:
            context_header = "BaÄŸlam (ARAÃ‡ SONUCUNU MUTLAKA KULLAN!):"
        else:
            context_header = "BaÄŸlam (Kullan, ama sadece GERÃ‡EKTEN alakalÄ±ysa):"

        # ğŸ”‘ ROL BAZLI KURALLAR
        if role == "religious_teacher":
            if is_detail_followup:
                # ğŸ†• TAKÄ°P SORUSU MODU: Kendi yorumla aÃ§Ä±kla
                rules_text = """KURALLAR (TAKÄ°P SORUSU - AÃ‡IKLAMA MODU):
1. ğŸ”‡ ARKA PLAN bilgisini DOÄRUDAN VERME, referans olarak kullan
2. âœ… KENDÄ° YORUMUNLA ve Ã–RNEKLERLE aÃ§Ä±kla
3. âœ… Ã–nceki cevabÄ±ndan devam et, baÄŸlamÄ± koru
4. âœ… GÃ¼nlÃ¼k hayattan somut Ã¶rnekler ver
5. âœ… Samimi TÃ¼rkÃ§e, SEN hitabÄ±
6. âŒ Metni kopyala-yapÄ±ÅŸtÄ±r YAPMA, sindirerek anlat
7. ğŸ­ Bir arkadaÅŸÄ±na anlatÄ±r gibi aÃ§Ä±kla"""
            else:
                # Ä°LK SORU MODU: Metne sadÄ±k kal
                rules_text = """KURALLAR:
1. âš ï¸ YanlÄ±ÅŸ bilgiyi onaylama, nazikÃ§e dÃ¼zelt
2. âŒ Soruyu tekrarlama, liste yapma (*, -, 1. 2. 3.)
3. âœ… VERÄ°LEN METÄ°NDEN anlat - metindeki kavramlarÄ± MUTLAKA kullan
4. âœ… Samimi TÃ¼rkÃ§e, SEN hitabÄ±
5. ğŸ” TEKRAR YASAK: Ã–nceki cevaplarÄ± tekrarlama
6. ğŸ­ ROLÃœNE UYGUN DAVRAN: YukarÄ±daki rol talimatlarÄ±na uy"""
        else:
            rules_text = """KURALLAR:
1. âš ï¸ YanlÄ±ÅŸ bilgiyi onaylama, nazikÃ§e dÃ¼zelt
2. âŒ Soruyu tekrarlama, liste yapma (*, -, 1. 2. 3.)
3. âŒ KAYNAK BELÄ°RTME YASAK: "Kaynaklara gÃ¶re" gibi ifadeler KULLANMA
4. âœ… Kendi bilgin gibi Ã¶zgÃ¼venle sun
5. âœ… Samimi TÃ¼rkÃ§e, SEN hitabÄ±
6. ğŸ”„ CevabÄ±nÄ±n arkasÄ±nda dur, somutlaÅŸtÄ±r
7. ğŸ” TEKRAR YASAK: Ã–nceki cevaplarÄ± tekrarlama
8. ğŸ­ ROLÃœNE UYGUN DAVRAN: YukarÄ±daki rol talimatlarÄ±na uy"""

        # ğŸ”‘ SEPARATOR
        sep = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

        prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{zaman_satiri}

[ğŸ­ ROL]: {role.upper()}
{role_prompt}

{sep}
ğŸ“š BAÄLAM (gerekirse kullan):
{sep}
{combined_str}

{sep}
ğŸ“‹ KURALLAR:
{sep}
{rules_text}{dynamic_rules_str}

{sep}
ğŸ“© YENÄ° MESAJ (sohbeti devam ettir):
{sep}
{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

        return prompt

    # ---------- ANA SEKRETER FONKSÄ°YONU ----------

    async def hazirla_ve_prompt_olustur(
        self, user_input: str, chat_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        SEKRETER'Ä°N ANA GÃ–REVÄ° - TEK LLM KARAR SÄ°STEMÄ°!

        1. LLM'e karar verdirme (keyword yok! hem kaynak hem tool)
        2. LLM'in seÃ§tiÄŸi tool'u Ã§alÄ±ÅŸtÄ±r
        3. BaÄŸlamÄ± topla (LLM'in kararÄ±na gÃ¶re!)
        4. Prompt'u hazÄ±rla
        5. Gemma3 iÃ§in hazÄ±r paketi dÃ¶ndÃ¼r
        """
        print("\n" + "=" * 60)
        print("ğŸ“‹ SEKRETER Ã‡ALIÅIYOR (TEK LLM SÄ°STEMÄ°)")
        print("=" * 60)
        print(f"ğŸ“ KullanÄ±cÄ±: {user_input}")

        # 0. ğŸ” KAPANMIÅ KONU KONTROLÃœ (yeni soruda aynÄ± konuya dÃ¶nmemek iÃ§in)
        is_closed, closed_summary = self.is_topic_closed(user_input)
        if is_closed:
            print(f"âš ï¸ UYARI: Bu soru kapanmÄ±ÅŸ bir konuya benziyor: '{closed_summary}'")
            print("   AI'a bu konuyu tekrar aÃ§mamasÄ± sÃ¶ylenecek.")

        # 1. ğŸ§  LLM'E KARAR VERDÄ°R (HEM KAYNAK HEM TOOL!)
        print("\nğŸ§  1. LLM tek karar veriyor (hem kaynak hem tool)...")
        decision = self._intelligent_decision(user_input, chat_history)

        # 1.5 ğŸ†• KONU KAPANDI MI? KAYDET!
        if decision.get('topic_closed', False):
            topic_summary = decision.get('closed_topic_summary', '')

            # Ã–zet yoksa farklÄ± kaynaklardan Ã§Ä±karmayÄ± dene
            if not topic_summary:
                # 1. Son AI mesajÄ±ndan konu Ã§Ä±kar
                if chat_history:
                    for msg in reversed(chat_history):
                        if msg.get('role') == 'assistant':
                            content = (msg.get('content') or '')[:100]
                            if content and len(content) > 5:
                                topic_summary = content
                                break

                # 2. Hala yoksa - son user sorusundan Ã§Ä±kar (teÅŸekkÃ¼r hariÃ§)
                if not topic_summary and chat_history:
                    for msg in reversed(chat_history):
                        if msg.get('role') == 'user':
                            content = (msg.get('content') or '').strip()
                            # "teÅŸekkÃ¼rler", "saÄŸol" gibi kapanÄ±ÅŸ mesajlarÄ±nÄ± atla
                            if content and len(content) > 10 and not any(
                                w in content.lower() for w in ['teÅŸekkÃ¼r', 'saÄŸol', 'eyvallah', 'gÃ¶rÃ¼ÅŸÃ¼rÃ¼z', 'bye', 'hoÅŸÃ§a']
                            ):
                                topic_summary = content[:100]
                                break

                # 3. Hala yoksa LLM reasoning'den Ã§Ä±kar
                if not topic_summary and decision.get('reasoning'):
                    topic_summary = decision['reasoning'][:100]

            # Ã–zet varsa kaydet
            if topic_summary:
                print(f"ğŸ’¾ Konu kaydediliyor: '{topic_summary[:50]}...'")
                self.add_closed_topic(topic_summary, chat_history)
            else:
                print("âš ï¸ topic_closed=true ama Ã¶zet Ã§Ä±karÄ±lamadÄ±, kayÄ±t atlandÄ±")

        # 2. LLM'in seÃ§tiÄŸi tool'u al
        tool_name = decision.get('tool_name', 'yok')
        tool_param = decision.get('tool_param', '')

        # 3. AraÃ§ Ã§alÄ±ÅŸtÄ±r (LLM'in kararÄ±na gÃ¶re)
        print(f"\nğŸ› ï¸ 2. AraÃ§ Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor (LLM kararÄ±: {tool_name})...")
        tool_result = await self._tool_calistir(tool_name, tool_param, user_input)

        # 4. BaÄŸlam toplama (LLM'in KARARLARINI KULLAN!)
        print("\nğŸ“š 3. BaÄŸlam toplanÄ±yor (LLM kararÄ±na gÃ¶re)...")

        # Semantic Memory (LLM karar verdi)
        if decision['needs_semantic_memory']:
            semantic_context = self._hafizada_ara(user_input, len(chat_history))
            print(f"   â€¢ Semantic HafÄ±za: {'âœ… bulundu' if semantic_context else 'âŒ bulunamadÄ±'} (LLM kararÄ±)")
        else:
            semantic_context = ""
            print("   â€¢ Semantic HafÄ±za: â© atlandÄ± (LLM: gereksiz)")

        # FAISS KB (LLM karar verdi)
        # âš ï¸ EÄŸer risale_ara tool'u zaten Ã§alÄ±ÅŸtÄ±ysa, FAISS'i tekrar Ã§aÄŸÄ±rma!
        if decision['needs_faiss'] and tool_name != "risale_ara":
            faiss_context = self._faiss_ara(user_input)
            print(f"   â€¢ FAISS KB: {'âœ… bulundu' if faiss_context else 'âŒ bulunamadÄ±'} (LLM kararÄ±)")
        elif tool_name == "risale_ara":
            faiss_context = ""  # Tool zaten FAISS kullandÄ±, duplicate arama yapma
            print("   â€¢ FAISS KB: â© atlandÄ± (risale_ara tool'u zaten FAISS kullandÄ±)")
        else:
            faiss_context = ""
            print("   â€¢ FAISS KB: â© atlandÄ± (LLM: gereksiz)")

        # ğŸ§  CONVERSATION CONTEXT (LLM tabanlÄ± Ã¶zet) - Ã–NCELÄ°KLÄ°
        # Konu derinleÅŸtiÄŸinde baÄŸlamÄ± koruyan akÄ±llÄ± Ã¶zet sistemi

        # ğŸ”‘ Ã–NCELÄ°KLE: Konu deÄŸiÅŸimi kontrolÃ¼ (context almadan Ã–NCE!)
        if self.conversation_context:
            topic_changed = self.conversation_context.check_topic_before_response(
                user_input, chat_history
            )
            if topic_changed:
                print(f"   â€¢ ğŸ”„ Yeni konu algÄ±landÄ± - eski baÄŸlam temizlendi")

        conversation_context = self.get_conversation_context()
        if conversation_context:
            print(f"   â€¢ ğŸ§  ConversationContext: âœ… LLM Ã¶zeti enjekte edildi")
        else:
            print(f"   â€¢ ğŸ§  ConversationContext: â© henÃ¼z Ã¶zet yok")

        # ğŸ”‡ UZUN DÃ–NEM HAFIZA (TopicMemory) - Silent Context Injection
        # Sadece gerektiÄŸinde kontrol et, sessizce enjekte et
        silent_long_term_context = ""
        if self.should_check_long_term_memory(user_input):
            silent_long_term_context = self.get_silent_long_term_context(user_input)
            if silent_long_term_context:
                print(f"   â€¢ ğŸ”‡ TopicMemory: âœ… sessiz baÄŸlam enjekte edildi")
            else:
                print(f"   â€¢ ğŸ”‡ TopicMemory: âŒ eÅŸleÅŸme yok")
        else:
            print("   â€¢ ğŸ”‡ TopicMemory: â© atlandÄ± (geÃ§miÅŸ referansÄ± yok)")

        # Context'leri birleÅŸtir (ConversationContext Ã¶ncelikli)
        combined_silent_context = ""
        if conversation_context:
            combined_silent_context = conversation_context
        if silent_long_term_context:
            if combined_silent_context:
                combined_silent_context += "\n\n" + silent_long_term_context
            else:
                combined_silent_context = silent_long_term_context

        # Chat History - SORU TÄ°PÄ°NE GÃ–RE DÄ°NAMÄ°K BOYUT
        # Basit sorularda az context, derin konularda Ã§ok context
        if chat_history and len(chat_history) > 0:
            question_type = decision['question_type']

            # Soru tipine gÃ¶re max mesaj sayÄ±sÄ± belirle
            if question_type in ['greeting', 'farewell', 'topic_closed']:
                max_history_msgs = 3  # Basit: son 3 mesaj yeter
            elif question_type in ['math', 'weather', 'prayer']:
                max_history_msgs = 4  # Tool-based: son 4 mesaj
            elif question_type in ['followup', 'general', 'ambiguous']:
                max_history_msgs = 6  # Orta: son 6 mesaj
            else:
                max_history_msgs = 10  # Derin konu (religious, technical): tam context

            # History'yi kÄ±sÄ±tla
            limited_history = chat_history[-max_history_msgs:] if len(chat_history) > max_history_msgs else chat_history

            # Chat history'yi Ã¶zetle
            chat_history_summary = self._history_summary(
                limited_history,
                current_question_type=question_type
            )
            print(f"   â€¢ Chat History: âœ… son 12 mesaj dahil edildi ({min(12, len(limited_history))}/{len(chat_history)} toplam)")
        else:
            chat_history_summary = ""
            print("   â€¢ Chat History: â© henÃ¼z yok")

        # 5. Rol tespiti (LLM kararÄ±ndan al)
        print("\nğŸ­ 4. Rol belirleniyor...")
        # LLM'in seÃ§tiÄŸi rolÃ¼ al, yoksa fallback olarak friend
        role = decision.get('role', 'friend')
        # GeÃ§erli rol kontrolÃ¼
        valid_roles = ['friend', 'teacher', 'technical_helper', 'acknowledger', 'religious_teacher']
        if role not in valid_roles:
            role = 'friend'
        print(f"   â€¢ Rol: {role} (LLM kararÄ±)")

        # 5.5 ğŸ†• KAPANMIÅ KONU FÄ°LTRELEME (AKILLI SÄ°STEM)
        # Sadece yeni soru kapanmÄ±ÅŸ konuya benziyorsa uyarÄ± ver
        # DeÄŸilse hiÃ§ gÃ¶nderme (token israfÄ± yapma)
        closed_topics_warning = ""
        if is_closed and closed_summary:
            # KullanÄ±cÄ± kapanmÄ±ÅŸ konuya benzer bir ÅŸey sordu
            # Ama AYNI konuyu tekrar mÄ± aÃ§mak istiyor kontrol et
            user_wants_reopen = self._user_wants_to_reopen_topic(user_input)

            if user_wants_reopen:
                # KullanÄ±cÄ± konuyu tekrar aÃ§mak istiyor, izin ver
                print(f"   â€¢ KapanmÄ±ÅŸ Konu: '{closed_summary}' - KullanÄ±cÄ± tekrar aÃ§mak istiyor âœ…")
            else:
                # KullanÄ±cÄ± farklÄ± bir ÅŸey soruyor ama benzer konu
                # Sadece bu durumda uyarÄ± ekle
                closed_topics_warning = closed_summary
                print(f"   â€¢ KapanmÄ±ÅŸ Konu UyarÄ±sÄ±: '{closed_summary}' - AI'a bildirildi")
        else:
            print("   â€¢ KapanmÄ±ÅŸ Konu: Yok veya ilgisiz â©")

        # 6. Prompt hazÄ±rla
        print("\nğŸ“ 6. Prompt hazÄ±rlanÄ±yor...")
        needs_clarification = decision.get('needs_clarification', False)
        llm_reasoning = decision.get('reasoning', '')  # ğŸ§  DecisionLLM'in Ã¶n araÅŸtÄ±rmasÄ±
        is_topic_closed = decision.get('topic_closed', False)  # ğŸ“• Konu kapandÄ± mÄ±?
        is_detail_followup = decision.get('is_detail_followup', False)  # ğŸ†• Takip sorusu mu?

        # ğŸ†• Takip modu log
        if is_detail_followup:
            print(f"   â€¢ ğŸ”„ TAKÄ°P MODU: FAISS arka plan olarak kullanÄ±lacak")
            print(f"   â€¢ ğŸ“Š GÃ¼ven: %{int(decision.get('followup_confidence', 0) * 100)}")

        final_prompt = self._prompt_olustur(
            user_input,
            tool_result,
            semantic_context,
            faiss_context,
            chat_history_summary,
            role,
            closed_topics_warning,  # Sadece gerektiÄŸinde dolu
            combined_silent_context,  # ğŸ§ ğŸ”‡ BirleÅŸik baÄŸlam (ConversationContext + TopicMemory)
            needs_clarification,  # ğŸ†• NetleÅŸtirme gerekli mi?
            llm_reasoning,  # ğŸ§  DecisionLLM'in Ã¶n araÅŸtÄ±rmasÄ± - KOPUKLUK DÃœZELTMESÄ°!
            is_topic_closed,  # ğŸ“• Konu kapandÄ± mÄ±? (kÄ±sa cevap ver)
            is_detail_followup,  # ğŸ†• Takip sorusu mu? (FAISS arka plan olarak)
            tool_name,  # ğŸŒ KullanÄ±lan araÃ§ (wiki_ara iÃ§in Ã¶zel mod)
        )
        print(f"   â€¢ Prompt uzunluÄŸu: {len(final_prompt)} karakter")

        paket = {
            "prompt": final_prompt,
            "role": role,
            "tool_used": tool_name,
            "llm_decision": decision,
            "metadata": {
                "has_tool_result": tool_result is not None,
                "has_semantic": bool(semantic_context),
                "has_faiss": bool(faiss_context),
                "has_history": bool(chat_history_summary),
                "has_context_memory": bool(combined_silent_context),  # ğŸ§ ğŸ”‡ BirleÅŸik baÄŸlam
                "closed_topic_filtered": is_closed,  # KapanmÄ±ÅŸ konu filtresi uygulandÄ± mÄ±
                "needs_clarification": needs_clarification,  # ğŸ†• NetleÅŸtirme gerekli mi?
                "is_detail_followup": is_detail_followup,  # ğŸ†• Takip sorusu mu?
            },
        }

        print("\nâœ… SEKRETER HAZIR - Tek LLM kararÄ±yla paket oluÅŸturuldu!")
        print("=" * 60 + "\n")

        return paket

    # ---------- FAISS INJECT & GERÄ°YE UYUMLULUK ----------

    def set_faiss_kb(self, faiss_kb):
        """FAISS KB'yi inject et"""
        self.faiss_kb.set_faiss_kb(faiss_kb)
        print(f"âœ… FAISS KB inject edildi (aktif: {self.faiss_kb.enabled})")

    @property
    def data(self):
        """Geriye uyumluluk: hafiza.data"""
        return self.hafiza

    @property
    def reranker(self):
        """Geriye uyumluluk: reranker var mÄ±? (ÅŸu an yok)"""
        return None

    def should_search_memory(self, chat_history_length: int) -> bool:
        """
        Geriye uyumluluk: HafÄ±za aramasÄ± yapÄ±lmalÄ± mÄ±?
        Eski PersonalAI bu metodu kullanÄ±yor
        """
        if not self.hafiza or len(self.hafiza) == 0:
            return False
        if len(self.hafiza) < 3:
            return False
        if chat_history_length == 0 and len(self.hafiza) > 0:
            return True
        return True

    def search_with_rerank(
        self, query: str, top_k: Optional[int] = None, initial_k: int = 50
    ) -> str:
        """
        Geriye uyumluluk: Reranker ile arama
        (Åu an normal search'e yÃ¶nlendiriliyor)
        """
        return self.search(query, top_k)

    def ilgili_mesajlari_bul(
        self, yeni_mesaj: str, max_mesaj: Optional[int] = None
    ) -> List[Dict[str, str]]:
        """
        Geriye uyumluluk: Ä°lgili mesajlarÄ± bul (eski API)
        NOT: ArtÄ±k _search_internal() kullanÄ±yor (Ã§ift iÅŸlem kaldÄ±rÄ±ldÄ±)
        Returns: [{"rol": "user", "mesaj": "..."}, ...]
        """
        if not self.hafiza or not yeni_mesaj:
            return []

        k = max_mesaj or self.max_mesaj
        return self._search_internal(yeni_mesaj, k)

    def son_mesajlari_al(self, n: int = 3) -> List[Dict[str, str]]:
        """
        Geriye uyumluluk: Son n mesajÄ± dÃ¶ndÃ¼r
        """
        if len(self.hafiza) < n:
            n = len(self.hafiza)

        son_mesajlar = self.hafiza[-n:]
        return [{"rol": m["rol"], "mesaj": m["mesaj"]} for m in son_mesajlar]


# ============================================================
# TEST KODLARI (opsiyonel)
# ============================================================

async def test_sekreter():
    print("\n" + "=" * 60)
    print("ğŸ§ª HafizaAsistani v3.0 TEST")
    print("=" * 60)

    sekreter = HafizaAsistani(saat_limiti=48, esik=0.50, max_mesaj=20)

    print("\n--- TEST 1: Basit Sohbet ---")
    paket1 = await sekreter.hazirla_ve_prompt_olustur(
        user_input="Merhaba, nasÄ±lsÄ±n?",
        chat_history=[],
    )
    print(f"âœ… Prompt hazÄ±r (uzunluk: {len(paket1['prompt'])})")
    print(f"   Role: {paket1['role']}")
    print(f"   Tool: {paket1['tool_used']}")

    print("\n--- TEST 2: Matematik ---")
    paket2 = await sekreter.hazirla_ve_prompt_olustur(
        user_input="15 Ã§arpÄ± 7 kaÃ§ eder?",
        chat_history=[],
    )
    print("âœ… Prompt hazÄ±r")
    print(f"   Tool: {paket2['tool_used']}")
    print(f"   Tool sonucu: {paket2['metadata']['has_tool_result']}")

    print("\n--- TEST 3: Zaman ---")
    paket3 = await sekreter.hazirla_ve_prompt_olustur(
        user_input="Saat kaÃ§?",
        chat_history=[],
    )
    print("âœ… Prompt hazÄ±r")
    print(f"   Tool: {paket3['tool_used']}")

    print("\n" + "=" * 60)
    print("âœ… TÃ¼m testler tamamlandÄ± (elle de deneyebilirsin).")
    print("=" * 60)


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        asyncio.run(test_sekreter())