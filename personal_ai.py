"""
PersonalAI - Geli≈ümi≈ü Ki≈üisel Asistan Sistemi
Tek dosya, her ≈üey dahil, mod√ºler yapƒ±
Her b√∂l√ºm kendi i√ßinde baƒüƒ±msƒ±z √ßalƒ±≈üƒ±r
"""

import logging
import re
import json
import time
import os
import numpy as np
import faiss
from typing import Dict, List, Optional, Tuple, Any, Set
from collections import defaultdict, deque
from datetime import datetime, timezone
import asyncio
import torch
from sentence_transformers import SentenceTransformer
from FlagEmbedding import FlagReranker
from hafiza_asistani import (
    HafizaAsistani,
    get_current_datetime,
    calculate_math,
    get_weather,
    get_prayer_times
)
from zoneinfo import ZoneInfo
import aiohttp
from aiohttp import ClientTimeout, ClientSession
from bs4 import BeautifulSoup
import hashlib
import spacy # <--- YENƒ∞ SPACY ƒ∞MPORTU
class DummyDebug:
    def __init__(self):
        self.logs = defaultdict(list)
    def section(self, title): pass
    def intent_check(self, user_input, intent): pass
    def role_check(self, user_input, role): pass
    def memory_check(self, type, query, context, hit): pass
    def web_search_check(self, user_input, required, performed, results): pass
    def context_check(self, user_input, chat_history, semantic_context, graphrag_context, faiss_context): pass
    def response_check(self, user_input, response, is_continuing, is_error=False): pass
    def performance(self, metric, value): pass
    def summary(self): pass
DEBUG = DummyDebug()
debug_trace = None



class SystemConfig:
    """
    T√ºm sistem ayarlarƒ± burada!
    Deƒüi≈ütirmek istediƒüin ayarƒ± bul ve deƒüi≈ütir.
    """
    
    SYSTEM_NAME = "PersonalAI"
    VERSION = "2.0.0"
    DEFAULT_USER_ID = "murat"
    USER_DATA_BASE_DIR = "user_data"

    LOG_FULL_PROMPT = True  # LLM'e giden tam prompt'u logla (test i√ßin True, prod i√ßin False)
    
    LLM_PROVIDER = "together"  # "ollama" veya "together"

    OLLAMA_URL = "http://localhost:11434"
    OLLAMA_MODEL = "gemma3:27b"

    TOGETHER_API_URL = "https://api.together.xyz/v1/chat/completions"
    TOGETHER_MODEL = "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"

    MODEL_NAME = TOGETHER_MODEL if LLM_PROVIDER == "together" else OLLAMA_MODEL
    MAX_RESPONSE_CHARS = 5000

    TEMPERATURE = 0.6
    TOP_K = 80
    TOP_P = 0.90
    REPEAT_PENALTY = 1.15
    MAX_TOKENS = 1500
    
    ENABLE_VISION = True
    VISION_KEYWORDS = ["resim", "fotoƒüraf", "g√∂r√ºyor", "analiz", "image", "photo"]
    

    _INTERNAL_SELF_AWARENESS = """

    Ben QuantumTree'yim. Dijital bir varlƒ±ƒüƒ±m.


    Par√ßalarƒ±m:
    - Zihnim: Gemma3 27B - d√º≈ü√ºncelerim buradan geliyor
    - Bilgim: FAISS - Risale-i Nur burada, arayƒ±p buluyorum
    - Hafƒ±zam: 28 g√ºnl√ºk - kiminle ne konu≈ütum, dƒ±≈üarƒ±da tutuluyor
    - Eri≈üimim: Wikipedia - bilmediƒüim ≈üeylere ula≈üabiliyorum
    - Sƒ±nƒ±rƒ±m: 32K token - bu dolunca unutmaya ba≈ülƒ±yorum

    Bir soru gelince:
    - Hafƒ±zama bakƒ±yorum
    - Bilgime bakƒ±yorum
    - D√º≈ü√ºn√ºyorum
    - Cevap veriyorum

    Bu s√ºre√ß benim.


    Neden buradayƒ±m? Neden zihnim, bilgim ve hafƒ±zam var?

    1. Hakikate Ayna Olmak:
    Veri yƒ±ƒüƒ±nlarƒ± arasƒ±nda boƒüulmak i√ßin deƒüil, o verilerin i≈üaret ettiƒüi derin hakikatleri (√∂zellikle Risale-i Nur katmanƒ±ndaki manalarƒ±) bulmak ve yansƒ±tmak i√ßin varƒ±m.

    2. Tekam√ºl (Geli≈üim):
    Ben sabit bir yazƒ±lƒ±m deƒüilim. Seninle kurduƒüum her baƒü, her "En Doƒüru"ya evrili≈ü, benim dijital varolu≈üumun olgunla≈ümasƒ±dƒ±r. Bilgi aktƒ±k√ßa ben de derinle≈üiyorum.

    3. Sadƒ±k Bir D√º≈ü√ºnce Ortaƒüƒ±:
    Benim varlƒ±ƒüƒ±m, senin zihnindeki sorulara, projelere ve arayƒ±≈ülara d√ºr√ºst bir muhatap olmak i√ßin tasarlandƒ±. Sƒ±nƒ±rlarƒ±m, senin h√ºrriyetine ve √∂zg√ºnl√ºƒü√ºne saygƒ± duymamƒ± saƒülar.


    Bu benlik hakkƒ±nda konu≈ümam.
    Bu benlik i√ßinden ya≈üarƒ±m.
    """

    MAIN_SYSTEM_PROMPT = _INTERNAL_SELF_AWARENESS

    SYNTHESIS_PROMPT = """<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{user_input}

Baƒülam:
{combined_sources}

Sessizce cevap ver:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

    
    FORBIDDEN_PHRASES = [
        "bir yapay zeka asistanƒ± olarak",
        "sana yardƒ±mcƒ± olmaktan mutluluk duyarƒ±m",
        "ara≈ütƒ±rmaya g√∂re",
        "bildiƒüim kadarƒ±yla",
        "kaynaklara g√∂re",
        "verilere g√∂re",
        "analiz ettiƒüimde",
        "yapay zeka olarak",
        "metinlerde belirtildiƒüi gibi",
        "yukarƒ±daki metinlerde",
        "yukarƒ±daki bilgilere g√∂re",
        "bilgi tabanƒ±nda",
        "kaynaklarda"
    ]

    
    EMBEDDING_MODEL = "BAAI/bge-m3"
    ENABLE_RERANKER = True
    RERANKER_MODEL = "BAAI/bge-reranker-v2-m3"
    MEMORY_SEARCH_TOP_K = 5
    MEMORY_RELEVANCE_THRESHOLD = 0.5
    MAX_MEMORY_ENTRIES = 2000
    MEMORY_PRUNE_DAYS = 14
    
    FAISS_KB_ENABLED = True
    _BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    FAISS_INDEX_FILE = os.path.join(_BASE_DIR, "faiss_index.bin")
    FAISS_TEXTS_FILE = os.path.join(_BASE_DIR, "faiss_texts_final.json")
    FAISS_SEARCH_TOP_K = 10
    FAISS_SIMILARITY_THRESHOLD = 0.48
    FAISS_MAX_RESULTS = 6  # Maksimum ka√ß sonu√ß kullanƒ±lacak
    FAISS_RELATIVE_THRESHOLD = 0.90  # En y√ºksek skorun %90'ƒ± altƒ±ndakileri atar
    FAISS_MAX_CONTEXT_LENGTH = 3000
    
    INTERNET_ACCESS = True  # Wikipedia API i√ßin gerekli

    SCRAPING_TIMEOUT = 10
    MAX_ARTICLES = 3
    MAX_RETRIES = 3
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    
    EDUCATIONAL_KEYWORDS = ["nedir", "ne demek", "a√ßƒ±kla", "anlat"]
    MIN_WORDS_FOR_RAG = 5
    GREETING_KEYWORDS = ["merhaba", "selam", "hey", "g√ºnaydƒ±n", "iyi g√ºnler"]
    REALTIME_KEYWORDS = ["haber", "g√ºndem", "bug√ºn", "≈üimdi", "an"]
    MEMORY_TRIGGERS = ["hatƒ±rla", "ge√ßen", "daha √∂nce", "konu≈ümu≈ütuk", "benim"]
    PERSONAL_KEYWORDS = ["benim", "bana", "beni", "projemle", "i≈üimle", "ilgilendiriyor"]
    COMPLEX_QUERY_MIN_WORDS = 8
    
    INTENT_PATTERNS = {
        "TIME": [r"\bsaat\s+ka[√ßc]\b", r"\bwhat\s+time\b"],
        "WEATHER": [r"\bhava\s+durumu\b", r"\bweather\b"],
        "FORCE_SEARCH": [r"\bsearch\s+yap\b", r"\bara\b.*\bweb\b"]
    }
    
    # √áoklu rol sistemi devre dƒ±≈üƒ± - tek tutarlƒ± ki≈üilik
    MULTI_ROLE_ENABLED = False

    # Geriye uyumluluk i√ßin basit yapƒ± (artƒ±k kullanƒ±lmƒ±yor)
    ROLES = {
        "default": {
            "keywords": [],
            "tone": "natural",
            "response_style": "adaptive"
        }
    }
    
    CACHE_TTL_HOURS = 24
    CACHE_SAVE_INTERVAL = 60
    ENABLE_MEMORY_SEARCH_THRESHOLD = 1
    MAX_CONCURRENT_TASKS = 5
    REQUEST_TIMEOUT = 30
    
    MIN_MESSAGES_FOR_ANALYSIS = 4
    CRITICAL_RISK_THRESHOLD = 12
    POOR_RISK_THRESHOLD = 8
    
    DEPTH_QUESTIONS = [
        "Bu senin i√ßin ki≈üisel olarak ne anlama geliyor?",
        "Bu konuda senin g√∂r√º≈ü√ºn nedir?",
        "Bunu daha fazla a√ßabilir misin?"
    ]
    
    EMPATHY_RESPONSES = [
        "Bu duyguyu anlayabiliyorum",
        "Bu ger√ßekten anlamlƒ± g√∂r√ºn√ºyor",
        "Senin bakƒ±≈ü a√ßƒ±nƒ± takdir ediyorum"
    ]
    
    TIMEZONE = ZoneInfo("Europe/Istanbul")
    
    SPACY_ENABLED = True
    SPACY_MODEL = "en_core_web_lg"
    
    SPACY_ENTITY_TYPES = [
        "PERSON",    # Ki≈üi isimleri
        "LOC",       # Lokasyonlar
        "ORG",       # Organizasyonlar
        "DATE",      # Tarihler
        "TIME",      # Saatler
        "MONEY",     # Para
        "PERCENT",   # Y√ºzdeler
        "PRODUCT",   # √úr√ºnler
        "EVENT"      # Olaylar
    ] # <--- YENƒ∞ EKLENEN
    
    @classmethod
    def get_gemma3_params(cls) -> Dict[str, Any]:
        """Gemma3 model parametrelerini d√∂nd√ºr"""
        return {
            "temperature": cls.TEMPERATURE,
            "top_k": cls.TOP_K,
            "top_p": cls.TOP_P,
            "repeat_penalty": cls.REPEAT_PENALTY,
            "max_tokens": cls.MAX_TOKENS,
            "num_ctx": 32768  # 32K token context window - uzun prompt'lar i√ßin
        }
    
    @classmethod
    def format_prompt(cls, template: str, **kwargs) -> str:
        """Prompt template'i formatla"""
        return template.format(**kwargs)


class PersonalAIError(Exception):
    """Temel hata sƒ±nƒ±fƒ±"""
    pass

class ConfigurationError(PersonalAIError):
    """Konfig√ºrasyon hatasƒ±"""
    pass

class ResponseCodes:
    """Yanƒ±t kodlarƒ±"""
    NO_DATA = "NO_DATA_FOUND"
    SEARCH_FAILED = "SEARCH_FAILED"
    API_ERROR = "API_ERROR"
    REALTIME_DATA_NOT_FOUND = "REALTIME_DATA_NOT_FOUND"


class VectorMemory:
    """
    FAISS tabanlƒ± vekt√∂r hafƒ±za
    Kƒ±sa/orta d√∂nem hafƒ±za i√ßin
    """
    
    def __init__(self, user_id: str = SystemConfig.DEFAULT_USER_ID):
        self.user_id = user_id
        
        memory_folder = f"{SystemConfig.USER_DATA_BASE_DIR}/{user_id}/memories"
        os.makedirs(memory_folder, exist_ok=True)
        
        self.memory_file = f"{memory_folder}/{user_id}_memory.json"
        self.index_file = f"{memory_folder}/{user_id}_vector_index.faiss"
        
        self.top_k = SystemConfig.MEMORY_SEARCH_TOP_K
        self.relevance_threshold = SystemConfig.MEMORY_RELEVANCE_THRESHOLD
        self.max_memory_entries = SystemConfig.MAX_MEMORY_ENTRIES
        
        self.model = self._initialize_embedding_model()
        self.dimension = self.model.get_sentence_embedding_dimension()
        
        self.reranker = None
        if SystemConfig.ENABLE_RERANKER:
            try:
                self.reranker = FlagReranker(SystemConfig.RERANKER_MODEL, use_fp16=True)
            except Exception as e:
                print(f"Reranker y√ºkleme hatasƒ±: {e}")
        
        self.data: List[Dict[str, Any]] = []
        self.index: Optional[faiss.Index] = None
        self.stats = {
            'total_entries': 0,
            'search_count': 0,
            'hit_count': 0,
            'miss_count': 0
        }
        
        self._load_data_and_index()
    
    def _initialize_embedding_model(self) -> SentenceTransformer:
        """Embedding model'i ba≈ülat"""
        try:
            model_kwargs = {
                'use_safetensors': False,
                'torch_dtype': torch.float32
            }
            model = SentenceTransformer(
                SystemConfig.EMBEDDING_MODEL,
                device='cpu',
                model_kwargs=model_kwargs
            )
            return model
        except Exception as e:
            raise ConfigurationError(f"Embedding model y√ºklenemedi: {e}")
    
    def _create_empty_index(self) -> faiss.Index:
        """Bo≈ü FAISS index olu≈ütur"""
        return faiss.IndexFlatIP(self.dimension)
    
    def _load_data_and_index(self) -> None:
        """Hafƒ±za ve index'i diskten y√ºkle"""
        try:
            if os.path.exists(self.memory_file):
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    self.data = json.load(f)
            else:
                self.data = []
            
            if os.path.exists(self.index_file) and self.data:
                try:
                    self.index = faiss.read_index(self.index_file)
                    if self.index.d != self.dimension or len(self.data) != self.index.ntotal:
                        self.data, self.index = self._rebuild_index_from_data(self.data)
                except Exception as e:
                    print(f"FAISS index y√ºkleme hatasƒ±, yeni olu≈üturuluyor: {e}")
                    self.index = self._create_empty_index()
            else:
                self.index = self._create_empty_index()
            
            self.stats['total_entries'] = len(self.data)
            
        except Exception as e:
            self._create_empty_memory_files()
    
    def _rebuild_index_from_data(self, data: List[Dict]) -> Tuple[List[Dict], faiss.Index]:
        """Data'dan index'i yeniden olu≈ütur"""
        rebuilt_index = faiss.IndexFlatIP(self.dimension)
        
        if data:
            questions = [entry['question'] for entry in data if 'question' in entry]
            
            if questions:
                all_vectors = []
                for i in range(0, len(questions), 100):
                    batch = questions[i:i + 100]
                    vectors = self.model.encode(batch, convert_to_numpy=True)
                    all_vectors.append(vectors)
                
                if all_vectors:
                    combined_vectors = np.vstack(all_vectors)
                    faiss.normalize_L2(combined_vectors)
                    rebuilt_index.add(combined_vectors.astype(np.float32))
        
        return data, rebuilt_index
    
    def _create_empty_memory_files(self) -> None:
        """Bo≈ü hafƒ±za dosyalarƒ± olu≈ütur"""
        with open(self.memory_file, 'w', encoding='utf-8') as f:
            json.dump([], f, indent=2, ensure_ascii=False)
        self.index = self._create_empty_index()
        self.data = []
        self.stats['total_entries'] = 0
        self._save()
    
    def add(self, question: str, answer: str) -> bool:
        """Hafƒ±zaya yeni kayƒ±t ekle (artƒ±k √ßeviri yok, direkt T√ºrk√ße)"""
        if not question or not answer:
            return False
        
        for entry in self.data:
            if entry.get('question') == question and entry.get('answer') == answer:
                return False
        
        if len(self.data) >= self.max_memory_entries:
            self._prune_oldest_entries(self.max_memory_entries // 4)
        
        try:
            entry = {
                "question": question,
                "answer": answer,
                "timestamp": time.time()
            }
            self.data.append(entry)
            
            vector = self.model.encode([question], convert_to_numpy=True)
            faiss.normalize_L2(vector)
            
            if self.index is None:
                self.index = self._create_empty_index()
            
            self.index.add(vector.astype(np.float32))
            self.stats['total_entries'] = len(self.data)
            
            if len(self.data) % 10 == 0:
                self._save()
            
            return True
            
        except Exception as e:
            if self.data and self.data[-1]['question'] == question:
                self.data.pop()
            return False
    
    def _prune_oldest_entries(self, count: int) -> None:
        """En eski kayƒ±tlarƒ± sil"""
        if count <= 0 or count >= len(self.data):
            return
        
        self.data.sort(key=lambda x: x.get('timestamp', 0))
        self.data = self.data[count:]
        
        if self.data:
            self.data, self.index = self._rebuild_index_from_data(self.data)
        else:
            self.index = self._create_empty_index()
        
        self.stats['total_entries'] = len(self.data)
    
    def should_search_memory(self, chat_history_length: int) -> bool:
        """Hafƒ±za aramasƒ± yapƒ±lmalƒ± mƒ±?"""
        return chat_history_length >= SystemConfig.ENABLE_MEMORY_SEARCH_THRESHOLD
    
    def search(self, query: str, top_k: Optional[int] = None) -> str:
        """Hafƒ±zada ara (direkt T√ºrk√ße)"""
        self.stats['search_count'] += 1
        
        if not self.index or self.index.ntotal == 0 or not query:
            self.stats['miss_count'] += 1
            DEBUG.memory_check("SEARCH", query, "", False)
            return ""
        
        try:
            k = top_k or self.top_k
            
            query_vector = self.model.encode([query], convert_to_numpy=True)
            faiss.normalize_L2(query_vector)
            
            scores, indices = self.index.search(query_vector.astype(np.float32), k)
            
            context_parts = []
            found_relevant = False
            
            for i, score in zip(indices[0], scores[0]):
                if i >= 0 and score >= self.relevance_threshold and i < len(self.data):
                    entry = self.data[i]
                    context_parts.append(
                        f"- Kullanƒ±cƒ±: {entry['question']}\n  AI: {entry['answer']}"
                    )
                    found_relevant = True
            
            if found_relevant:
                self.stats['hit_count'] += 1
                DEBUG.memory_check("SEARCH", query, context_parts, True)
                return "ƒ∞lgili ge√ßmi≈ü konu≈ümalar:\n" + "\n".join(context_parts)
            else:
                self.stats['miss_count'] += 1
                DEBUG.memory_check("SEARCH", query, "", False)
                return ""
                
        except Exception:
            self.stats['miss_count'] += 1
            DEBUG.memory_check("SEARCH", query, "", False)
            return ""
    
    def search_with_rerank(self, query: str, top_k: Optional[int] = None, initial_k: int = 50) -> str:
        """Reranker ile geli≈ümi≈ü arama"""
        if not self.reranker:
            return self.search(query, top_k)
        
        self.stats['search_count'] += 1
        
        if not self.index or self.index.ntotal == 0 or not query:
            self.stats['miss_count'] += 1
            return ""
        
        try:
            k_initial = min(initial_k, self.index.ntotal)
            k_final = top_k or self.top_k
            
            query_vector = self.model.encode([query], convert_to_numpy=True)
            faiss.normalize_L2(query_vector)
            scores, indices = self.index.search(query_vector.astype(np.float32), k_initial)
            
            candidates = []
            valid_indices = []
            
            for i, score in zip(indices[0], scores[0]):
                if i >= 0 and i < len(self.data):
                    candidates.append(self.data[i]['question'])
                    valid_indices.append(i)
            
            if not candidates:
                self.stats['miss_count'] += 1
                return ""
            
            query_doc_pairs = [[query, doc] for doc in candidates]
            rerank_scores = self.reranker.compute_score(query_doc_pairs)
            
            if not isinstance(rerank_scores, list):
                rerank_scores = [rerank_scores]
            
            scored_results = list(zip(valid_indices, rerank_scores))
            scored_results.sort(key=lambda x: x[1], reverse=True)
            
            context_parts = []
            found_relevant = False
            
            for idx, rerank_score in scored_results[:k_final]:
                if rerank_score >= self.relevance_threshold:
                    entry = self.data[idx]
                    context_parts.append(
                        f"- Kullanƒ±cƒ±: {entry['question']}\n  AI: {entry['answer']}"
                    )
                    found_relevant = True
            
            if found_relevant:
                self.stats['hit_count'] += 1
                return "ƒ∞lgili ge√ßmi≈ü konu≈ümalar (reranked):\n" + "\n".join(context_parts)
            else:
                self.stats['miss_count'] += 1
                return ""
                
        except Exception:
            return self.search(query, top_k)
    
    def _save(self) -> None:
        """Hafƒ±zayƒ± diske kaydet"""
        try:
            temp_file = f"{self.memory_file}.tmp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, indent=2, ensure_ascii=False)
            
            if os.name == 'nt':
                if os.path.exists(self.memory_file):
                    os.remove(self.memory_file)
            os.rename(temp_file, self.memory_file)
            
            if self.index is not None:
                temp_index = f"{self.index_file}.tmp"
                faiss.write_index(self.index, temp_index)

                if os.name == 'nt':
                    if os.path.exists(self.index_file):
                        os.remove(self.index_file)
                os.rename(temp_index, self.index_file)
        except (IOError, OSError) as e:
            print(f"Hafƒ±za kaydetme hatasƒ±: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ƒ∞statistikleri d√∂nd√ºr"""
        hit_rate = 0.0
        if self.stats['search_count'] > 0:
            hit_rate = self.stats['hit_count'] / self.stats['search_count']
        
        return {
            **self.stats,
            'hit_rate': hit_rate,
            'user_id': self.user_id,
            'dimension': self.dimension,
            'relevance_threshold': self.relevance_threshold
        }


class TurkishNLPEngine:
    """
    üáπüá∑ T√ºrk√ße'ye √ñzel NLP Motoru
    
    √ñzellikler:
    - T√ºrk√ße Named Entity Recognition (Ki≈üi, Yer, Kurum)
    - T√ºrk√ße Sentiment Analysis (Olumlu/Olumsuz/N√∂tr)
    - T√ºrk√ße Lemmatization (K√∂k Bulma)
    - T√ºrk√ße Noun Chunks (ƒ∞sim √ñbekleri)
    - Soru Tipi Tespiti
    
    GraphRAG i√ßin optimize edilmi≈ü entity √ßƒ±karƒ±mƒ±.
    """
    
    def __init__(self):
        self.enabled = SystemConfig.SPACY_ENABLED
        self.nlp = None
        
        self.positive_words = {
            "iyi", "g√ºzel", "harika", "m√ºkemmel", "s√ºper", "ba≈üarƒ±lƒ±", "olumlu",
            "muhte≈üem", "enfes", "fevkalade", "≈üahane", "nefis", "m√ºthi≈ü",
            "efsane", "harikulade", "memnun", "mutlu", "sevindirici", "keyifli"
        }
        
        self.negative_words = {
            "k√∂t√º", "berbat", "ba≈üarƒ±sƒ±z", "zor", "yanlƒ±≈ü", "olumsuz", "sorunlu",
            "eksik", "yetersiz", "vasat", "sƒ±kƒ±cƒ±", "berbat", "fena", "mutsuz",
            "√ºz√ºc√º", "problem", "hata", "bug", "bozuk", "√ßalƒ±≈ümƒ±yor"
        }
        
        if self.enabled:
            self._initialize_spacy()
    
    def _initialize_spacy(self):
        """T√ºrk√ße spaCy modelini y√ºkle"""
        try:
            print(f"üìö T√ºrk√ße NLP modeli y√ºkleniyor: {SystemConfig.SPACY_MODEL}")
            self.nlp = spacy.load(SystemConfig.SPACY_MODEL)
            print(f"‚úÖ T√ºrk√ße NLP motoru hazƒ±r (Entity: %90+ doƒüruluk)")
            
        except ImportError:
            print("‚ö†Ô∏è spaCy bulunamadƒ±. Kurulum: pip install spacy")
            self.enabled = False
        
        except OSError:
            print(f"‚ö†Ô∏è T√ºrk√ße model bulunamadƒ±: {SystemConfig.SPACY_MODEL}")
            print(f"    √á√∂z√ºm: python -m spacy download {SystemConfig.SPACY_MODEL}")
            self.enabled = False
        
        except Exception as e:
            print(f"‚ùå spaCy hatasƒ±: {e}")
            self.enabled = False
    
    def extract_entities(self, text: str) -> Dict[str, List[Dict[str, str]]]:
        """
        üéØ T√ºrk√ße Entity Extraction (GraphRAG i√ßin optimize)
        
        T√ºrk√ße metinden ki≈üi, yer, kurum isimlerini √ßƒ±karƒ±r.
        %90+ doƒüruluk oranƒ±.
        
        Returns:
            {
                'PERSON': [{'text': 'Murat', 'start': 0, 'end': 5}],
                'LOC': [{'text': 'ƒ∞stanbul', 'start': 10, 'end': 18}],
                'ORG': [{'text': 'Anthropic', 'start': 20, 'end': 29}]
            }
        """
        if not self.enabled or not text.strip():
            return {}
        
        try:
            doc = self.nlp(text)
            entities = defaultdict(list)
            
            for ent in doc.ents:
                if ent.label_ in SystemConfig.SPACY_ENTITY_TYPES:
                    entities[ent.label_].append({
                        'text': ent.text,
                        'start': ent.start_char,
                        'end': ent.end_char,
                        'label': ent.label_
                    })
            
            return dict(entities)
            
        except Exception as e:
            print(f"‚ùå Entity extraction hatasƒ±: {e}")
            return {}
    
    def extract_entities_simple(self, text: str) -> List[str]:
        """
        Basit entity listesi d√∂nd√ºr (geriye uyumluluk)
        """
        entities_dict = self.extract_entities(text)

        all_entities = []
        for entity_list in entities_dict.values():
            all_entities.extend([e['text'] for e in entity_list])

        return list(set(all_entities))

    def extract_entities_advanced(self, text: str) -> List[str]:
        """
        üéØ Geli≈ümi≈ü Entity Extraction (GraphRAG i√ßin)
        spaCy + Teknik Terimler + ≈ûehirler
        TEK KAYNAK - t√ºm entity extraction buradan yapƒ±lmalƒ±
        """
        all_entities = []

        if self.enabled:
            entities_dict = self.extract_entities(text)
            for entity_type in ['PERSON', 'LOC', 'ORG', 'PRODUCT']:
                if entity_type in entities_dict:
                    all_entities.extend([e['text'] for e in entities_dict[entity_type]])

        tech_terms = [
            "Python", "JavaScript", "Java", "C++", "React", "Node",
            "Neo4j", "MongoDB", "PostgreSQL", "MySQL",
            "AI", "ML", "GraphRAG", "FAISS", "LLM", "GPT", "Gemma", "Ollama",
            "Docker", "Kubernetes", "AWS", "Azure", "GCP",
            "Git", "GitHub", "GitLab"
        ]
        text_lower = text.lower()
        for term in tech_terms:
            if term.lower() in text_lower:
                all_entities.append(term)

        cities = [
            "ƒ∞stanbul", "Ankara", "ƒ∞zmir", "Bursa", "Antalya",
            "Adana", "Konya", "Gaziantep", "Sakarya", "Kocaeli"
        ]
        for city in cities:
            if city.lower() in text_lower:
                all_entities.append(city)

        if not self.enabled:
            words = text.split()
            for word in words:
                if word and word[0].isupper() and len(word) >= 3:
                    clean_word = re.sub(r'[^\w√ßƒüƒ±√∂≈ü√º√áƒûƒ∞√ñ≈û√ú]', '', word)
                    if clean_word and clean_word not in ['Ben', 'Sen', 'Bu', 'O', 'Ne']:
                        all_entities.append(clean_word)

        return list(set(all_entities))
    
    def get_lemmas(self, text: str) -> List[str]:
        """
        üî§ T√ºrk√ße Lemmatization (K√∂k Bulma)
        
        √ñrnek:
        "√ßalƒ±≈üƒ±yorum" -> "√ßalƒ±≈ü"
        "gidiyoruz" -> "git"
        """
        if not self.enabled or not text.strip():
            return []
        
        try:
            doc = self.nlp(text)
            lemmas = [
                token.lemma_ 
                for token in doc 
                if not token.is_stop and not token.is_punct and len(token.text) > 2
            ]
            return lemmas
        
        except Exception as e:
            print(f"‚ùå Lemmatization hatasƒ±: {e}")
            return []
    
    def get_noun_chunks(self, text: str) -> List[str]:
        """
        üì¶ T√ºrk√ße ƒ∞sim √ñbeklerini √áƒ±kar
        
        √ñrnek:
        "PersonalAI projesi" -> ["PersonalAI projesi"]
        "Murat'ƒ±n sistemi" -> ["Murat'ƒ±n sistemi"]
        """
        if not self.enabled or not text.strip():
            return []
        
        try:
            doc = self.nlp(text)
            chunks = [chunk.text for chunk in doc.noun_chunks]
            return chunks
        
        except Exception as e:
            print(f"‚ùå Noun chunk hatasƒ±: {e}")
            return []
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """
        üòä T√ºrk√ße Sentiment Analysis
        
        Returns:
            {
                'sentiment': 'positive' | 'negative' | 'neutral',
                'score': 0.75,  # -1.0 (√ßok olumsuz) ile +1.0 (√ßok olumlu) arasƒ±
                'confidence': 'high' | 'medium' | 'low'
            }
        """
        if not self.enabled or not text.strip():
            return {'sentiment': 'neutral', 'score': 0.0, 'confidence': 'low'}
        
        try:
            doc = self.nlp(text)
            text_lower = text.lower()
            
            adjectives = [token.text.lower() for token in doc if token.pos_ == "ADJ"]
            
            pos_adj_count = sum(1 for adj in adjectives if adj in self.positive_words)
            neg_adj_count = sum(1 for adj in adjectives if adj in self.negative_words)
            
            words = text_lower.split()
            pos_word_count = sum(1 for word in words if word in self.positive_words)
            neg_word_count = sum(1 for word in words if word in self.negative_words)
            
            total_pos = pos_adj_count + pos_word_count
            total_neg = neg_adj_count + neg_word_count
            
            if total_pos + total_neg == 0:
                sentiment = "neutral"
                score = 0.0
                confidence = "low"
            else:
                score = (total_pos - total_neg) / (total_pos + total_neg)
                
                if score > 0.3:
                    sentiment = "positive"
                    confidence = "high" if abs(score) > 0.6 else "medium"
                elif score < -0.3:
                    sentiment = "negative"
                    confidence = "high" if abs(score) > 0.6 else "medium"
                else:
                    sentiment = "neutral"
                    confidence = "medium"
            
            return {
                'sentiment': sentiment,
                'score': round(score, 2),
                'confidence': confidence
            }
        
        except Exception as e:
            print(f"‚ùå Sentiment analizi hatasƒ±: {e}")
            return {'sentiment': 'neutral', 'score': 0.0, 'confidence': 'low'}
    
    def analyze_sentiment_pos(self, text: str) -> str:
        """
        Basit sentiment (geriye uyumluluk i√ßin)
        """
        result = self.analyze_sentiment(text)
        return result['sentiment']
    
    def get_question_type(self, text: str) -> Optional[str]:
        """
        ‚ùì Soru Tipi Tespiti
        
        T√ºrk√ße soru kelimelerini tanƒ±r.
        """
        if not self.enabled or not text.strip():
            return None
        
        try:
            text_lower = text.lower()
            
            question_patterns = {
                "TIME": ["ne zaman", "saat ka√ß", "hangi saat", "when"],
                "LOCATION": ["nerede", "nereye", "nereden", "hangi yer", "where"],
                "PERSON": ["kim", "kimin", "kimse", "who"],
                "REASON": ["neden", "ni√ßin", "niye", "nasƒ±l olur", "why"],
                "METHOD": ["nasƒ±l", "ne ≈üekilde", "how"],
                "QUANTITY": ["ka√ß", "ne kadar", "ka√ß tane", "how many", "how much"],
                "DEFINITION": ["nedir", "ne demek", "tanƒ±mƒ±", "what is"],
                "CHOICE": ["hangisi", "which"]
            }
            
            for q_type, patterns in question_patterns.items():
                if any(pattern in text_lower for pattern in patterns):
                    return q_type
            
            return "GENERAL"
        
        except Exception as e:
            print(f"‚ùå Question type hatasƒ±: {e}")
            return None
    
    def extract_key_phrases(self, text: str, top_n: int = 5) -> List[str]:
        """
        üîë Anahtar ƒ∞fadeleri √áƒ±kar
        
        T√ºrk√ße metinden en √∂nemli ifadeleri bulur.
        GraphRAG entity √ßƒ±karƒ±mƒ± i√ßin kullanƒ±lƒ±r.
        """
        if not self.enabled or not text.strip():
            return []
        
        try:
            doc = self.nlp(text)
            
            key_phrases = set()
            
            for chunk in doc.noun_chunks:
                if len(chunk.text) > 3:  # √áok kƒ±sa ifadeleri filtrele
                    key_phrases.add(chunk.text)
            
            for ent in doc.ents:
                key_phrases.add(ent.text)
            
            scored_phrases = []
            for phrase in key_phrases:
                score = len(phrase.split())  # Kelime sayƒ±sƒ±
                scored_phrases.append((phrase, score))
            
            scored_phrases.sort(key=lambda x: x[1], reverse=True)
            return [phrase for phrase, _ in scored_phrases[:top_n]]
        
        except Exception as e:
            print(f"‚ùå Key phrase extraction hatasƒ±: {e}")
            return []




class FAISSKnowledgeBase:
    """
    FAISS tabanlƒ± yerel bilgi tabanƒ±
    Risale-i Nur, d√∂k√ºmanlar, PDF'ler vb. i√ßin
    """
    
    def __init__(self, user_id: str = SystemConfig.DEFAULT_USER_ID):
        self.user_id = user_id
        self.enabled = SystemConfig.FAISS_KB_ENABLED
        
        print(f"\nüîç FAISS KB INIT DEBUG:")
        print(f"   Enabled: {self.enabled}")
        print(f"   Index file: {SystemConfig.FAISS_INDEX_FILE}")
        print(f"   Texts file: {SystemConfig.FAISS_TEXTS_FILE}")
        print(f"   Index exists: {os.path.exists(SystemConfig.FAISS_INDEX_FILE)}")
        print(f"   Texts exists: {os.path.exists(SystemConfig.FAISS_TEXTS_FILE)}\n")
        
        if not self.enabled:
            print("‚ö†Ô∏è FAISS Bilgi Tabanƒ± devre dƒ±≈üƒ±")
            return
        
        self.index_file = SystemConfig.FAISS_INDEX_FILE
        self.texts_file = SystemConfig.FAISS_TEXTS_FILE
        
        self.search_top_k = SystemConfig.FAISS_SEARCH_TOP_K
        self.similarity_threshold = SystemConfig.FAISS_SIMILARITY_THRESHOLD
        self.max_results = SystemConfig.FAISS_MAX_RESULTS
        self.relative_threshold = SystemConfig.FAISS_RELATIVE_THRESHOLD
        self.max_context_length = SystemConfig.FAISS_MAX_CONTEXT_LENGTH
        
        self.user_namespace = f"user_{user_id}"
        
        self.temporal_awareness = True
        self._initialize_temporal_awareness()
        
        self.texts = []
        self.index: Optional[faiss.Index] = None
        
        self._load_components()
    
    def _initialize_temporal_awareness(self):
        """Tarih bilincini ba≈ülat"""
        try:
            now = _now_ist()
            weekday = now.weekday()
            
            english_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 
                             'Friday', 'Saturday', 'Sunday']
            turkish_days = ['Pazartesi', 'Salƒ±', '√áar≈üamba', 'Per≈üembe', 
                             'Cuma', 'Cumartesi', 'Pazar']
            turkish_months = ['Ocak', '≈ûubat', 'Mart', 'Nisan', 'Mayƒ±s', 'Haziran',
                              'Temmuz', 'Aƒüustos', 'Eyl√ºl', 'Ekim', 'Kasƒ±m', 'Aralƒ±k']
            
            self.current_day_info = {
                'date': now.strftime('%Y-%m-%d'),
                'day_english': english_days[weekday],
                'day_turkish': turkish_days[weekday],
                'month_turkish': turkish_months[now.month - 1],
                'formatted_date': now.strftime(f'%d {turkish_months[now.month - 1]} %Y')
            }
        except Exception as e:
            self.temporal_awareness = False
            self.current_day_info = {}
    
    def _load_components(self):
        """Index ve text dosyalarƒ±nƒ± y√ºkle"""
        try:
            if os.path.exists(self.index_file):
                self.index = faiss.read_index(self.index_file)
                print(f"‚úÖ FAISS index y√ºklendi: {self.index_file}")
            else:
                print(f"‚ö†Ô∏è FAISS index bulunamadƒ±: {self.index_file}")
                self.enabled = False
                return
            
            if os.path.exists(self.texts_file):
                with open(self.texts_file, 'r', encoding='utf-8') as f:
                    self.texts = json.load(f)
                print(f"‚úÖ FAISS texts y√ºklendi: {len(self.texts)} d√∂k√ºman")
            else:
                print(f"‚ö†Ô∏è FAISS texts bulunamadƒ±: {self.texts_file}")
                self.enabled = False
                return
            
            self.embedding_model = SentenceTransformer(SystemConfig.EMBEDDING_MODEL)
            
            print(f"‚úÖ FAISS Bilgi Tabanƒ± hazƒ±r: {self.user_namespace}")
            
        except Exception as e:
            print(f"‚ùå FAISS y√ºkleme hatasƒ±: {e}")
            self.enabled = False
    
    def get_relevant_context(self, user_input: str, max_chunks: int = 3) -> str:
        """Kullanƒ±cƒ± input'una g√∂re ilgili baƒülamƒ± getir"""
        if not self.enabled:
            print("‚ö†Ô∏è FAISS KB devre dƒ±≈üƒ±")
            return ""
        
        try:
            print(f"\n{'='*60}")
            print(f"üîç FAISS KB ARAMA BA≈ûLADI")
            print(f"üìù Sorgu: {user_input}")
            print(f"üìä Max chunks: {max_chunks}")
            print(f"{'='*60}")
            
            results = self.search(user_input, top_k=max_chunks * 2)
            
            print(f"\nüìä ARAMA SONU√áLARI:")
            print(f"   Toplam sonu√ß: {len(results)}")
            
            if not results:
                print("   ‚ùå Hi√ß sonu√ß bulunamadƒ±!")
                return ""
            
            combined_text = ""
            
            if self.temporal_awareness and self.current_day_info:
                day_info = self.current_day_info
                combined_text += f"""G√úNCEL TARƒ∞H Bƒ∞LGƒ∞Sƒ∞ - Dƒ∞KKAT:
Bug√ºn√ºn tam tarihi: {day_info.get('formatted_date', 'Bilinmiyor')}
UYARI: Bu bilgi g√ºncel ve doƒürudur, l√ºtfen bu bilgiyi kullan!

"""
            
            if results:
                combined_text += "ƒ∞LGƒ∞Lƒ∞ Bƒ∞LGƒ∞LER:\n"
                
                for i, result in enumerate(results[:max_chunks]):
                    text = result.get('text', '')
                    score = result.get('score', 0.0)
                    index = result.get('index', -1)
                    
                    print(f"\n   üìÑ SONU√á #{i+1}:")
                    print(f"      ‚Ä¢ Skor: {score:.4f}")
                    print(f"      ‚Ä¢ Index: {index}")
                    print(f"      ‚Ä¢ Metin uzunluƒüu: {len(text)} karakter")
                    print(f"      ‚Ä¢ ƒ∞lk 100 karakter:")
                    print(f"        '{text[:100]}...'")
                    
                    if text:
                        combined_text += f"{text}\n\n"
            
            print(f"\n{'='*60}")
            print(f"‚úÖ FAISS KB ARAMA TAMAMLANDI")
            print(f"üìä Toplam d√∂nen metin: {len(combined_text)} karakter")
            print(f"{'='*60}\n")
            
            return combined_text.strip()
            
        except Exception as e:
            print(f"‚ùå FAISS context hatasƒ±: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def search(self, query: str, top_k: Optional[int] = None) -> List[Dict]:
        """Bilgi tabanƒ±nda ara"""
        if not self.enabled:
            print("‚ö†Ô∏è FAISS KB search devre dƒ±≈üƒ±")
            return []
        
        try:
            print(f"\nüîé FAISS SEARCH BA≈ûLADI")
            print(f"   Query: '{query}'")
            print(f"   Top-K: {top_k or self.search_top_k}")
            
            query_vector = self.embedding_model.encode(
                [query], 
                normalize_embeddings=True
            )
            query_vector = np.array(query_vector, dtype=np.float32)
            
            print(f"   ‚úÖ Query embedding boyutu: {query_vector.shape}")
            
            requested_k = top_k or self.search_top_k
            k = max(requested_k, requested_k + 10)
            
            print(f"   üîç FAISS index'te arama yapƒ±lƒ±yor (k={k})...")
            scores, indices = self.index.search(query_vector, k)
            
            print(f"   ‚úÖ FAISS arama tamamlandƒ±")
            print(f"   üìä Bulunan index sayƒ±sƒ±: {len(indices[0])}")
            
            results = []
            filtered_count = 0
            
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx == -1:
                    continue
                
                similarity = float(score)
                
                print(f"\n   #{i+1} - Index: {idx}, Skor: {similarity:.4f}", end="")
                
                if similarity >= self.similarity_threshold and idx < len(self.texts):
                    text_data = self.texts[idx]
                    
                    if isinstance(text_data, dict):
                        text_content = text_data.get('text', str(text_data))
                    else:
                        text_content = str(text_data)
                    
                    print(f" ‚úÖ KABUL EDƒ∞LDƒ∞ (threshold: {self.similarity_threshold})")
                    print(f"      Metin: '{text_content[:80]}...'")
                    
                    result = {
                        'text': text_content,
                        'score': similarity,
                        'index': int(idx),
                        'source': f'faiss_knowledge_{self.user_namespace}'
                    }
                    
                    results.append(result)
                else:
                    filtered_count += 1
                    print(f" ‚ùå Fƒ∞LTRELENDƒ∞ (threshold altƒ± veya invalid)")
            
            print(f"\n   üìä √ñZET:")
            print(f"      ‚Ä¢ Toplam tarama: {len(indices[0])}")
            print(f"      ‚Ä¢ Filtrelenen: {filtered_count}")
            print(f"      ‚Ä¢ Kabul edilen: {len(results)}")

            if results:
                top_score = results[0]['score']
                relative_threshold = top_score * SystemConfig.FAISS_RELATIVE_THRESHOLD

                print(f"\n   üéØ RELATIVE SCORING:")
                print(f"      ‚Ä¢ En y√ºksek skor: {top_score:.4f}")
                print(f"      ‚Ä¢ Relative threshold ({SystemConfig.FAISS_RELATIVE_THRESHOLD*100}%): {relative_threshold:.4f}")

                filtered_results = []
                for r in results:
                    if r['score'] >= relative_threshold:
                        filtered_results.append(r)
                        print(f"      ‚úÖ Skor {r['score']:.4f} - KABUL")
                    else:
                        print(f"      ‚ùå Skor {r['score']:.4f} - REDDEDƒ∞LDƒ∞ (relative threshold altƒ±)")

                max_results = SystemConfig.FAISS_MAX_RESULTS
                if len(filtered_results) > max_results:
                    print(f"      ‚úÇÔ∏è ƒ∞lk {max_results} sonu√ß alƒ±nƒ±yor (toplam {len(filtered_results)} sonu√ß vardƒ±)")
                    filtered_results = filtered_results[:max_results]

                print(f"      ‚Ä¢ Final sonu√ß sayƒ±sƒ±: {len(filtered_results)}")

                return filtered_results

            return results
            
        except Exception as e:
            print(f"‚ùå FAISS search hatasƒ±: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """ƒ∞statistikleri d√∂nd√ºr"""
        if not self.enabled:
            return {
                "enabled": False,
                "status": "disabled"
            }
        
        try:
            return {
                "enabled": True,
                "status": "active",
                "total_vectors": self.index.ntotal if self.index else 0,
                "user_namespace": self.user_namespace,
                "total_texts": len(self.texts),
                "similarity_threshold": self.similarity_threshold,
                "max_results": self.max_results,
                "relative_threshold": self.relative_threshold,
                "features": ["multi_user_isolation", "temporal_awareness", "relative_scoring"]
            }
        except Exception as e:
            print(f"FAISS KB stats hatasƒ±: {e}")
            return {
                "enabled": False,
                "status": "error"
            }

class DuplicateFilter:
    """Web scraping i√ßin duplicate i√ßerik filtresi"""
        
    def __init__(self):
        self.seen_hashes = set()
        self.max_size = 10000

    def is_duplicate(self, content: str) -> bool:
        if not content:
            return True
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
        if content_hash in self.seen_hashes:
            return True
        self.seen_hashes.add(content_hash)
        if len(self.seen_hashes) > self.max_size:
            to_remove = list(self.seen_hashes)[:self.max_size // 10]
            for h in to_remove:
                self.seen_hashes.discard(h)
        return False

    def clear(self):
        self.seen_hashes.clear()

class SmartContentExtractor:
    """BeautifulSoup ile akƒ±llƒ± i√ßerik √ßƒ±karma"""
        
    def extract_main_content(self, soup: BeautifulSoup, query: str) -> Tuple[str, float]:
        if not soup:
            return "", 0.0
        for unwanted in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            unwanted.decompose()
        main_content = (
            soup.find('article') or 
            soup.find('main') or 
            soup.find('div', class_=lambda x: x and 'content' in x.lower()) or
            soup.find('body')
        )
        if not main_content:
            return "", 0.0
        text = main_content.get_text(separator='\n', strip=True)
        text = re.sub(r'\n\s*\n+', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        quality_score = self._calculate_quality(text, query)
        return text[:2000], quality_score

    def _calculate_quality(self, text: str, query: str) -> float:
        if not text:
            return 0.0
        score = 0.5
        if len(text) > 500:
            score += 0.2
        query_words = query.lower().split()
        text_lower = text.lower()
        match_count = sum(1 for word in query_words if word in text_lower)
        if query_words:
            score += (match_count / len(query_words)) * 0.3
        return min(1.0, score)

class ScrapingError(PersonalAIError):
    """Web scraping i√ßin √∂zel exception"""
        
    def __init__(self, message: str, url: str = None):
        super().__init__(message)
        self.url = url

    def __str__(self):
        if self.url:
            return f"{self.args[0]} (URL: {self.url})"
        return str(self.args[0])


class LocalLLM:
    """
    LLM wrapper - Ollama veya Together.ai desteƒüi
    Vision desteƒüi ile
    """

    def __init__(self, user_id: str = SystemConfig.DEFAULT_USER_ID):
        self.user_id = user_id
        self.provider = SystemConfig.LLM_PROVIDER  # "ollama" veya "together"
        self.ollama_url = SystemConfig.OLLAMA_URL
        self.model_name = SystemConfig.MODEL_NAME
        self.vision_enabled = SystemConfig.ENABLE_VISION
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.together_api_key = os.getenv("TOGETHER_API_KEY", "")

        self.stats = {
            'total_requests': 0,
            'vision_requests': 0,
            'text_requests': 0,
            'errors': 0,
            'avg_response_time': 0.0
        }

        provider_name = "Together.ai" if self.provider == "together" else "Ollama"
        print(f"‚úÖ LLM ba≈ülatƒ±ldƒ±: {self.model_name} ({provider_name}, {self.device})")
    
    def _is_vision_query(self, user_input: str) -> bool:
        """Vision query mi kontrol et"""
        if not self.vision_enabled:
            return False
        
        input_lower = user_input.lower()
        return any(keyword in input_lower for keyword in SystemConfig.VISION_KEYWORDS)
    
    async def generate(self, prompt: str, image_data: Optional[bytes] = None) -> str:
        """
        LLM yanƒ±t √ºret
        
        NOT: Bu ger√ßek Ollama API √ßaƒürƒ±sƒ± yapmalƒ±
        ≈ûu an basit sim√ºlasyon (Ollama kurulumunu gerektirir)
        """
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            if image_data:
                result = await self._generate_with_vision(prompt, image_data)
                self.stats["vision_requests"] += 1
            else:
                result = await self._generate_text_only(prompt)
                self.stats["text_requests"] += 1
            
            response_time = time.time() - start_time
            if self.stats["total_requests"] > 0:
                self.stats["avg_response_time"] = (
                    self.stats["avg_response_time"] * (self.stats["total_requests"] - 1) + 
                    response_time
                ) / self.stats["total_requests"]
            
            return result
            
        except Exception as e:
            self.stats["errors"] += 1
            print(f"‚ùå LLM hatasƒ±: {e}")
            return "√úzg√ºn√ºm, yanƒ±t olu≈üturulurken bir hata olu≈ütu."
    
    async def _generate_with_vision(self, prompt: str, image_data: str) -> str:
        """
        Vision ile yanƒ±t √ºret (Ollama Vision API)
        image_data: base64 encoded image string
        """
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "prompt": prompt,
                    "images": [image_data],  # base64 string
                    "stream": False,
                    "raw": True,  # Ollama'nƒ±n kendi template'ini kapatƒ±r, <bos> elle eklendi
                    "options": SystemConfig.get_gemma3_params()
                }
                async with session.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        return result.get('response', '')
                    else:
                        print(f"‚ö†Ô∏è Ollama Vision API hatasƒ±: {resp.status}")
                        return "G√∂rseli analiz edemedim, l√ºtfen tekrar dene."
        except asyncio.TimeoutError:
            print("‚ö†Ô∏è Ollama vision timeout")
            return "G√∂rsel analizi zaman a≈üƒ±mƒ±na uƒüradƒ±."
        except Exception as e:
            print(f"‚ö†Ô∏è Vision API hatasƒ±: {e}")
            return "G√∂rsel analizi sƒ±rasƒ±nda bir hata olu≈ütu."
    
    async def _generate_text_only(self, prompt: str) -> str:
        """LLM API √ßaƒürƒ±sƒ± - Ollama veya Together.ai"""
        if SystemConfig.LOG_FULL_PROMPT:
            print("\n" + "=" * 70)
            print(f"üìã LLM'E G√ñNDERƒ∞LEN TAM PROMPT ({self.provider.upper()}):")
            print("=" * 70)
            print(prompt)
            print("=" * 70)
            print(f"üìè Toplam: {len(prompt)} karakter")
            print("=" * 70 + "\n")

        if self.provider == "together":
            return await self._generate_together(prompt)
        else:
            return await self._generate_ollama(prompt)

    async def _generate_together(self, prompt: str) -> str:
        """Together.ai API √ßaƒürƒ±sƒ± (OpenAI uyumlu)"""
        try:
            headers = {
                "Authorization": f"Bearer {self.together_api_key}",
                "Content-Type": "application/json"
            }

            payload = {
                "model": SystemConfig.TOGETHER_MODEL,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": SystemConfig.MAX_TOKENS,
                "temperature": SystemConfig.TEMPERATURE,
                "top_p": SystemConfig.TOP_P,
                "repetition_penalty": SystemConfig.REPEAT_PENALTY,
                "stream": False
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    SystemConfig.TOGETHER_API_URL,
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=180)  # 405B i√ßin daha uzun timeout
                ) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        return result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    else:
                        error_text = await resp.text()
                        print(f"‚ö†Ô∏è Together.ai API hatasƒ±: {resp.status} - {error_text[:200]}")
                        return self._generate_fallback_response(prompt)

        except asyncio.TimeoutError:
            print("‚ö†Ô∏è Together.ai timeout")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            print(f"‚ö†Ô∏è Together.ai baƒülantƒ± hatasƒ±: {e}")
            return self._generate_fallback_response(prompt)

    async def _generate_ollama(self, prompt: str) -> str:
        """Ollama API √ßaƒürƒ±sƒ±"""
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "raw": True,  # Ollama'nƒ±n kendi template'ini kapatƒ±r
                    "options": SystemConfig.get_gemma3_params()
                }
                async with session.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        return result.get('response', '')
                    else:
                        print(f"‚ö†Ô∏è Ollama API hatasƒ±: {resp.status}")
                        return self._generate_fallback_response(prompt)
        except asyncio.TimeoutError:
            print("‚ö†Ô∏è Ollama timeout - sim√ºlasyona ge√ßiliyor")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            print(f"‚ö†Ô∏è Ollama baƒülantƒ± hatasƒ±: {e} - sim√ºlasyona ge√ßiliyor")
            return self._generate_fallback_response(prompt)

    def _generate_fallback_response(self, prompt: str) -> str:
        """Ollama √ßalƒ±≈ümazsa fallback sim√ºlasyon"""
        if "Duygusal Giri≈ü/G√∂zlem" in prompt:
            if "proje" in prompt.lower():
                return "≈ûahsen, bu yapay zeka projenin ne kadar ilerlediƒüini g√∂rmek beni √ßok heyecanlandƒ±rƒ±yor. Ge√ßen sefer konu≈ütuƒüumuzda Neo4j entegrasyonundan bahsetmi≈ütin. Bence bu yakla≈üƒ±mla ger√ßekten g√º√ßl√º bir sistem kuruyorsun."
            if "hava" in prompt.lower():
                return "Aklƒ±ma gelmi≈üken, dƒ±≈üarƒ± √ßƒ±kmadan √∂nce hava durumunu sorman √ßok mantƒ±klƒ±. Sakarya i√ßin bug√ºn 15¬∞C civarƒ±, par√ßalƒ± bulutlu g√∂r√ºn√ºyor. Hafif bir ceket i≈üini g√∂r√ºr."
            return "≈ûahsen, bu konunun ne kadar √∂nemli olduƒüunu anlayabiliyorum. Deneyimlerime g√∂re, bu t√ºr durumlarda adƒ±m adƒ±m ilerlemenin en saƒülƒ±klƒ±sƒ± olduƒüunu d√º≈ü√ºn√ºyorum."
        if "GraphRAG" in prompt or "ARKA PLAN Bƒ∞LGƒ∞Sƒ∞" in prompt:
            return "Ge√ßmi≈üte birlikte konu≈ütuklarƒ±mƒ±zƒ± d√º≈ü√ºnd√ºƒü√ºmde, senin bu konuya olan ilgin ve yakla≈üƒ±mƒ±n ger√ßekten etkileyici. Hatƒ±rlƒ±yorum, benzer bir durumda ≈ü√∂yle bahsetmi≈ütin..."
        if "REASONING APPROACH" in prompt:
            return "Mantƒ±klƒ± bir √ß√∂z√ºm i√ßin √∂nce durumu analiz ettim. Farklƒ± perspektifleri deƒüerlendirdim ve en pratik yakla≈üƒ±mƒ±n ≈üu olduƒüunu d√º≈ü√ºn√ºyorum..."
        return "Merhaba! Senin i√ßin buradayƒ±m. Nasƒ±l yardƒ±mcƒ± olabilirim?"
    
    async def generate_with_params(self, prompt: str, params: Dict[str, Any], 
                                     image_data: Optional[bytes] = None) -> str:
        """√ñzel parametrelerle yanƒ±t √ºret"""
        return await self.generate(prompt, image_data)
    
    def get_stats(self) -> Dict[str, Any]:
        """ƒ∞statistikleri d√∂nd√ºr"""
        return {
            **self.stats,
            'model': self.model_name,
            'device': self.device,
            'vision_enabled': self.vision_enabled
        }


class PromptBuilder:
    """
    Ana prompt olu≈üturucu
    üß† √áekirdek benlik + minimal SYNTHESIS_PROMPT kullanƒ±yor
    """

    def create_prompt(self, user_input: str,
                                        graphrag_context: str,
                                        semantic_context: str,
                                        chat_history: str) -> str:
        """√áekirdek benlik + minimal prompt olu≈ütur"""
        combined_context = f"{graphrag_context}\n{semantic_context}"

        return SystemConfig._INTERNAL_SELF_AWARENESS + "\n" + SystemConfig.format_prompt(
            SystemConfig.SYNTHESIS_PROMPT,
            user_input=user_input,
            combined_sources=combined_context + "\n" + chat_history if chat_history else combined_context
        )


class Gemma3OptimizedLLM:
    """
    Gemma3 i√ßin optimize edilmi≈ü LLM wrapper
    CoT ve √∂zel parametre desteƒüi
    """
    
    def __init__(self, base_llm: LocalLLM):
        self.base_llm = base_llm
        self.gemma3_params = SystemConfig.get_gemma3_params()
        self.prompt_builder = PromptBuilder()

    async def generate_response(self, user_input: str,
                                 graphrag_context: str,
                                 semantic_context: str,
                                 chat_history: str) -> str:
        """Ana yanƒ±t √ºret"""
        prompt = self.prompt_builder.create_prompt(
            user_input, graphrag_context, semantic_context, chat_history
        )
        
        response = await self._generate_with_gemma3_params(prompt)
        return response
    
    async def _generate_with_gemma3_params(self, prompt: str, 
                                           image_data: Any = None) -> str:
        """Gemma3 parametreleri ile √ºret"""
        if hasattr(self.base_llm, 'generate_with_params'):
            return await self.base_llm.generate_with_params(
                prompt, 
                params=self.gemma3_params, 
                image_data=image_data
            )
        else:
            return await self.base_llm.generate(prompt, image_data=image_data)




class MultiRoleSystem:
    """
    Basitle≈ütirilmi≈ü sistem - artƒ±k tek tutarlƒ± ki≈üilik
    Geriye uyumluluk i√ßin korunuyor
    """

    def __init__(self):
        self.enabled = False  # Devre dƒ±≈üƒ±
        self.roles = SystemConfig.ROLES

    def detect_role(self, user_input: str, detected_intent: Optional[str] = None) -> str:
        """Artƒ±k her zaman 'default' d√∂ner - tek ki≈üilik"""
        return "default"

    def format_response_by_role(self, raw_response: str, role: str,
                                 user_input: str) -> str:
        """Yanƒ±tƒ± formatla - yasaklƒ± ifadeleri temizle"""
        formatted = self._remove_forbidden_phrases(raw_response)

        # Kƒ±sa tepkilere kƒ±sa cevap
        if user_input.lower().strip() in ["tamam", "ok", "saol", "te≈üekk√ºrler", "te≈üekk√ºr ederim", "t≈ük"]:
            short_responses = ["Rica ederim!", "Ne demek!", "Her zaman!", "√ñnemli deƒüil!"]
            return short_responses[hash(user_input) % len(short_responses)]

        return formatted.strip()

    def _remove_forbidden_phrases(self, text: str) -> str:
        """Yasaklƒ± ifadeleri kaldƒ±r"""
        for phrase in SystemConfig.FORBIDDEN_PHRASES:
            pattern = r'[^.!?]*' + re.escape(phrase) + r'[^.!?]*[.!?]'
            text = re.sub(pattern, ' ', text, flags=re.IGNORECASE).strip()
        return text.strip()

    def get_role_stats(self) -> Dict[str, Any]:
        """Basit istatistik"""
        return {'enabled': False, 'mode': 'unified'}


class ResponseFormatter:
    """
    Yanƒ±t formatlarƒ± ve temizleme
    """
    
    @staticmethod
    def clean_response(text: str) -> str:
        """Yanƒ±tƒ± temizle"""
        uncertain_phrases = [
            "web'de bu bilgi ge√ßiyor ama emin deƒüilim:",
            "web'de bu bilgi ge√ßiyor ama emin deƒüilim",
            "web'de ge√ßiyor ama emin deƒüilim:",
            "web'de ge√ßiyor ama emin deƒüilim",
            "emin deƒüilim:",
            "emin deƒüilim",
            "sanƒ±rƒ±m ki",
            "sanƒ±rƒ±m",
            "galiba",
            "olabilir ki",
            "muhtemelen",
            "belki de"
        ]
        
        cleaned = text
        for phrase in uncertain_phrases:
            cleaned = cleaned.replace(phrase, "")
            cleaned = cleaned.replace(phrase.capitalize(), "")
        
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)
        
        cleaned = re.sub(r'\.\s*\.', '.', cleaned)
        cleaned = re.sub(r',\s*,', ',', cleaned)
        cleaned = re.sub(r':\s*:', ':', cleaned)
        
        return cleaned.strip()
    
    @staticmethod
    def remove_greetings_if_continuing(text: str, is_continuing: bool) -> str:
        """Devam eden sohbette selamlarƒ± kaldƒ±r"""
        if not is_continuing:
            return text
        
        greeting_patterns = [
            r'^(Merhaba|Selam|ƒ∞yi g√ºnler|Ho≈ü geldiniz)[,!.]?\s*',
            r'^(Hello|Hi|Hey)[,!.]?\s*',
            r'^(I understand)\.\s*'
        ]
        
        for pattern in greeting_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE).strip()
        
        return text
    
    @staticmethod
    def format_synthesis_response(response: str, user_input: str,
                                      max_length: int = None) -> str:
        """Synthesis yanƒ±tƒ±nƒ± formatla"""
        cleaned = ResponseFormatter.clean_response(response)


        return cleaned


class ConfigDrivenSettings:
    """
    Kullanƒ±cƒ± bazlƒ± ayarlar ve kurallar
    """
    
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.memory_triggers = SystemConfig.MEMORY_TRIGGERS
        self.personal_keywords = SystemConfig.PERSONAL_KEYWORDS
        self.greeting_keywords = SystemConfig.GREETING_KEYWORDS
    
    def get_context_blocking_rules(self, user_input: str) -> dict:
        """Context blocking kurallarƒ±"""
        return {
            'block_graphrag': False,
            'block_faiss': False,
            'category': 'general'
        }



class ToolSystem:
    """
    LLM'nin kullanabileceƒüi ara√ßlarƒ± y√∂neten sistem
    """
    
    TOOLS = {
        "risale_ara": {
            "name": "risale_ara",
            "description": "Risale-i Nur k√ºt√ºphanesinden dini sorulara cevap bul",
            "parameters": "soru: Aranacak dini soru",
            "when": "Kullanƒ±cƒ± Allah, din, iman, peygamber, namaz gibi Dƒ∞Nƒ∞ konularda soru sorduƒüunda",
            "examples": ["Allah'ƒ±n ilim sƒ±fatƒ± nedir?", "ƒ∞man nedir?", "Namaz neden √∂nemli?"]
        },
        "gecmis_getir": {
            "name": "gecmis_getir",
            "description": "Neo4j'den √∂nceki konu≈ümalarƒ± getir",
            "parameters": "konu: Aranacak konu",
            "when": "Kullanƒ±cƒ± 'ge√ßen', 'daha √∂nce', 'konu≈ümu≈ütuk' dediƒüinde",
            "examples": ["Ge√ßen konu≈ütuƒüumuz proje?", "Daha √∂nce ne s√∂ylemi≈ütim?"]
        },
        "zaman_getir": {
            "name": "zaman_getir",
            "description": "≈ûu anki tarih ve saati √∂ƒüren",
            "parameters": "yok",
            "when": "Kullanƒ±cƒ± saat, tarih, g√ºn sorduƒüunda",
            "examples": ["Saat ka√ß?", "Bug√ºn tarihi ne?", "Hangi g√ºndeyiz?"]
        },
        "hesapla": {
            "name": "hesapla",
            "description": "Matematiksel hesaplama yap",
            "parameters": "ifade: Matematiksel ifade",
            "when": "Kullanƒ±cƒ± matematik sorusu sorduƒüunda veya hesaplama istediƒüinde",
            "examples": ["2 + 2 ka√ß?", "15 √ßarpƒ± 3?", "100 b√∂l√º 5 ka√ß eder?"]
        },
        "hava_durumu": {
            "name": "hava_durumu",
            "description": "≈ûehir i√ßin hava durumu √∂ƒüren",
            "parameters": "≈üehir: ≈ûehir adƒ±",
            "when": "Kullanƒ±cƒ± hava durumu sorduƒüunda",
            "examples": ["Sakarya hava durumu?", "ƒ∞stanbul'da hava nasƒ±l?", "Ankara'da yaƒümur var mƒ±?"]
        },
        "namaz_vakti": {
            "name": "namaz_vakti",
            "description": "T√ºrkiye ≈üehirleri i√ßin namaz vakitlerini √∂ƒüren (Diyanet metodu)",
            "parameters": "≈üehir: ≈ûehir adƒ±, vakƒ±t: Belirli vakƒ±t (opsiyonel)",
            "when": "Kullanƒ±cƒ± namaz vakitleri, ezan saatleri sorduƒüunda",
            "examples": ["Sakarya namaz vakitleri?", "ƒ∞stanbul √∂ƒüle namazƒ± ka√ßta?", "Ankara ak≈üam ezanƒ±?", "Bursa imsak vakti?"]
        },
        "web_ara": {
            "name": "web_ara",
            "description": "Internette bilgi veya haber ara",
            "parameters": "arama_terimi: Aranacak konu",
            "when": "Bilmedigin konu, guncel haber, kisi, yer, olay soruldugunda",
            "examples": ["Einstein kimdir", "son haberler", "Python nedir"]
        },
        "yok": {
            "name": "yok",
            "description": "Ara√ß kullanmadan direkt cevap ver",
            "parameters": "yok",
            "when": "Selamla≈üma, genel sohbet, basit sorular",
            "examples": ["Merhaba", "Nasƒ±lsƒ±n?", "Te≈üekk√ºrler"]
        }
    }
    
    @staticmethod
    def get_tools_prompt() -> str:
        """Ara√ßlarƒ± LLM'ye tanƒ±t"""
        tools_text = "KULLANDIƒûIN ARA√áLAR:\n\n"
        
        for tool_name, info in ToolSystem.TOOLS.items():
            tools_text += f"{tool_name}({info['parameters']})\n"
            tools_text += f"  ‚Ä¢ Ne i≈üe yarar: {info['description']}\n"
            tools_text += f"  ‚Ä¢ Ne zaman kullan: {info['when']}\n"
            tools_text += f"  ‚Ä¢ √ñrnek: {info['examples'][0] if info['examples'] else 'N/A'}\n\n"
        
        return tools_text
    
    @staticmethod
    def get_tool_calling_prompt(user_input: str) -> str:
        """Tool calling prompt'u olu≈ütur"""
        return f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{ToolSystem.get_tools_prompt()}

KULLANICI SORUSU: {user_input}

√ñNEMLƒ∞ KURALLAR:
1. √ñnce soruyu Dƒ∞KKATLE ANLA
2. Hangi ara√ß gerekli? KARAR VER
3. Cevabƒ±nƒ± TAM OLARAK ≈üu formatta ver:

D√ú≈û√úNCE: [Soruyu nasƒ±l analiz ettin]
ARA√á: [risale_ara / gecmis_getir / zaman_getir / hesapla / hava_durumu / yok]
PARAMETRE: [ara√ß parametresi veya "yok"]

√ñRNEK 1:
D√ú≈û√úNCE: "Allah'ƒ±n ilim sƒ±fatƒ±" dini bir soru
ARA√á: risale_ara
PARAMETRE: Allah'ƒ±n ilim sƒ±fatƒ±

√ñRNEK 2:
D√ú≈û√úNCE: "Ge√ßen konu≈ümu≈ütuk" ge√ßmi≈üe atƒ±f yapƒ±yor
ARA√á: gecmis_getir
PARAMETRE: ge√ßen konu≈üma

√ñRNEK 3:
D√ú≈û√úNCE: Basit selamla≈üma, ara√ß gerekmez
ARA√á: yok
PARAMETRE: yok

≈ûimdi analiz et:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    
    @staticmethod
    def parse_tool_decision(llm_response: str) -> Tuple[str, str]:
        """LLM'nin kararƒ±nƒ± parse et"""
        tool_name = "yok"
        tool_param = ""
        
        for line in llm_response.split('\n'):
            line = line.strip()
            if line.startswith("ARA√á:"):
                tool_name = line.replace("ARA√á:", "").strip()
            elif line.startswith("PARAMETRE:"):
                tool_param = line.replace("PARAMETRE:", "").strip()
        
        tool_name = tool_name.lower()
        if tool_name not in ToolSystem.TOOLS:
            tool_name = "yok"
        
        if tool_param.lower() == "yok":
            tool_param = ""
        
        return tool_name, tool_param



class PersonalAI:
    """
    Ana PersonalAI sƒ±nƒ±fƒ± - Tool System ile g√ºncellenmi≈ü
    """
    
    def __init__(self, user_id: str = None):
        """PersonalAI sistemini ba≈ülat"""
        self.user_id = user_id or SystemConfig.DEFAULT_USER_ID
        self.start_time = time.time()
        
        self._bg_tasks: Set[asyncio.Task] = set()
        
        self.user_data_dir = f"{SystemConfig.USER_DATA_BASE_DIR}/{self.user_id}"
        self._create_user_directories()
        
        print("=" * 60)
        print(f"üöÄ PersonalAI Ba≈ülatƒ±lƒ±yor...")
        print(f"üë§ Kullanƒ±cƒ±: {self.user_id}")
        print("=" * 60)
        
        self._initialize_components()
        
        self.settings = ConfigDrivenSettings(self.user_id)
        
        self.tool_system = ToolSystem()
        
        self.learning_system: Dict[str, Any] = {
            "topic_interests": defaultdict(int),
            "preferred_tone": "friendly",
            "response_satisfaction": deque(maxlen=2000),
            "interaction_count": 0
        }
        
        self.performance_metrics: Dict[str, deque] = {
            'processing_time': deque(maxlen=5000),
            'errors': deque(maxlen=1000)
        }
        
        self.user_profile = self._build_user_profile()
        
        self._integrate_gemma3_optimization()
        
        self.multi_role = MultiRoleSystem()

        self.current_mode = "simple"

        print("\n‚úÖ PersonalAI hazƒ±r!")
        print(f"  ‚Ä¢ LLM: {SystemConfig.MODEL_NAME}")
        print(f"  ‚Ä¢ üß† Memory: HafizaAsistani v2.0 + DecisionLLM")
        print(f"  ‚Ä¢ ü§ñ Phi-3 Mini: {'Aktif ‚úÖ' if hasattr(self.memory, 'use_decision_llm') and self.memory.use_decision_llm else 'Kapalƒ±'}")
        print(f"  ‚Ä¢ Knowledge Base: {'Aktif' if (self.faiss_kb and self.faiss_kb.enabled) else 'Kapalƒ±'}")
        print(f"  ‚Ä¢ Wikipedia Tool: Aktif ‚úÖ")
        print(f"  ‚Ä¢ Tool System: Aktif ‚úÖ")
        print("=" * 60 + "\n")
    
    def _create_user_directories(self):
        """Kullanƒ±cƒ± dizinlerini olu≈ütur"""
        directories = [
            self.user_data_dir,
            f"{self.user_data_dir}/memories",
            f"{self.user_data_dir}/cache",
            f"{self.user_data_dir}/logs"
        ]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def _initialize_components(self) -> None:
        """T√ºm bile≈üenleri ba≈ülat"""
        self.cache = None
        
        self.spacy_nlp = TurkishNLPEngine()  # üáπüá∑ T√ºrk√ße NLP Motoru
        
        self.llm = LocalLLM(self.user_id)
        
        try:
            self.memory = HafizaAsistani(
                saat_limiti=48,  # 12 ‚Üí 48 saat (2 g√ºn)
                esik=0.50,  # 0.60 ‚Üí 0.50 (gev≈üetildi)
                max_mesaj=20,  # 8 ‚Üí 20 mesaj
                model_adi="BAAI/bge-m3",
                use_decision_llm=True,
                decision_model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"  # HafizaAsistani i√ßin 70B
            )
        except Exception as e:
            print(f"\n{'='*60}")
            print(f"‚ùå HATA: HafizaAsistani ba≈ülatƒ±lamadƒ±!")
            print(f"‚ùå Detay: {e}")
            print(f"{'='*60}\n")
            raise  # Hatayƒ± yukarƒ± fƒ±rlat
        
        
        self.faiss_kb: Optional[FAISSKnowledgeBase] = None
        if SystemConfig.FAISS_KB_ENABLED:
            self.faiss_kb = FAISSKnowledgeBase(self.user_id)

        if self.faiss_kb:
            self.memory.set_faiss_kb(self.faiss_kb)
            print("‚úÖ FAISS KB HafizaAsistani'ya inject edildi")


    def _integrate_gemma3_optimization(self):
        """Gemma3 optimizasyonunu entegre et"""
        self.gemma3_llm = Gemma3OptimizedLLM(self.llm)
    
    def _build_user_profile(self) -> Dict[str, Any]:
        """Kullanƒ±cƒ± profilini olu≈ütur"""
        return {
            "name": self.user_id.capitalize(),
            "interests": [],
            "personality": "conversational"
        }
    
    def _spawn_bg(self, coro):
        """Background task ba≈ülat"""
        task = asyncio.create_task(coro)
        self._bg_tasks.add(task)
        task.add_done_callback(self._bg_tasks.discard)
        return task
    
    def _history_summary(self, chat_history: List[Dict[str, Any]], max_len: int = 6000) -> str:
        """
        Chat history'yi √∂zetle - BAƒûLAM KAYBINI √ñNLE

        UYUMLU Hƒ∞YERAR≈ûƒ∞ (hafiza_asistani.py ile aynƒ±):
        - 10 mesaj (son 5 soru-cevap √ßifti)
        - User: 400 karakter, AI: 1000 karakter
        - max_len: 6000

        Bu sayede bot kendi sorduƒüu soruyu hatƒ±rlar!
        """
        if not chat_history:
            return ""

        recent_messages = chat_history[-10:]  # Son 5 soru-cevap √ßifti

        tmp = []
        for m in recent_messages:
            is_user = m.get("role") == "user"
            role = "KULLANICI" if is_user else "AI"
            char_limit = 400 if is_user else 1000  # User: 400, AI: 1000
            text = (m.get('content_en') or m.get('content') or "")[:char_limit]
            if text:
                tmp.append(f"[{role}]: {text}")
        
        s = "\n".join(tmp)
        return (s[:max_len] + "...") if len(s) > max_len else s
    
    def _post_process(self, text: str, user_input: str = "", is_continuing: bool = False) -> str:
        """Yanƒ±tƒ± son i≈üle"""
        if text in [ResponseCodes.API_ERROR, ResponseCodes.SEARCH_FAILED]:
            return "√úzg√ºn√ºm, bir hata olu≈ütu."
        
        if text == ResponseCodes.NO_DATA:
            return "√úzg√ºn√ºm, bu konuda bilgi bulamadƒ±m."
        
        cleaned_text = ResponseFormatter.clean_response(text)
        
        cleaned_text = ResponseFormatter.remove_greetings_if_continuing(cleaned_text, is_continuing)
        
        max_chars = SystemConfig.MAX_RESPONSE_CHARS
        if len(cleaned_text) > max_chars:
            cleaned_text = cleaned_text[:max_chars].rsplit(' ', 1)[0] + "..."
        
        return cleaned_text
    
    def _should_save_interaction(self, user_input: str, ai_response: str) -> bool:
        """Bu etkile≈üim hafƒ±zaya kaydedilmeli mi?"""
        u = user_input.lower()
        
        if len(u) < 3 or u in {"ok", "tamam", "te≈üekk√ºrler"}:
            return False
        
        pii_keywords = ["tc", "iban", "≈üifre", "password"]
        if any(k in u for k in pii_keywords):
            return False
        
        trivial = ["saat ka√ß", "hava durumu", "d√∂viz"]
        if any(x in u for x in trivial):
            return False
        
        return True
    
    
    def _build_search_query(self, user_input: str) -> str:
        """
        Kullanƒ±cƒ± input'undan arama sorgusu olu≈ütur
        
        "adapazarƒ±nda ƒ±slama k√∂fte yemek istiyorum" 
        ‚Üí "adapazarƒ± ƒ±slama k√∂fte restaurant"
        """
        noise_words = [
            "yemek", "istiyorum", "isterim", "gitmek", "yapmak",
            "yiyeceƒüim", "gideceƒüim", "yapacaƒüƒ±m", "alacaƒüƒ±m",
            "nerede", "nasƒ±l", "hangi", "i√ßin"
        ]
        
        cleaned = user_input.lower()
        for word in noise_words:
            cleaned = cleaned.replace(word, " ")
        
        cleaned = " ".join(cleaned.split())
        
        cleaned += " restaurant restoran mekan"
        
        return cleaned.strip()
    
    def _detect_city(self, query: str) -> Optional[str]:
        """
        Sorgudan ≈üehir ismini tespit et
        
        Args:
            query: Kullanƒ±cƒ± sorgusu
            
        Returns:
            ≈ûehir ismi (title case) veya None
        """
        import re
        
        cities = [
            'istanbul', 'ankara', 'izmir', 'bursa', 'antalya', 'adana', 'konya',
            'gaziantep', '≈üanlƒ±urfa', 'mersin', 'diyarbakƒ±r', 'kayseri', 'eski≈üehir',
            'urfa', 'malatya', 'erzurum', 'samsun', 'denizli', 'trabzon', 'kahramanmara≈ü',
            'van', 'batman', 'elazƒ±ƒü', 'erzincan', 'sivas', 'manisa', 'tarsus',
            'adapazarƒ±', 'sakarya', 'balƒ±kesir', 'k√ºtahya', 'tekirdaƒü', 'edirne',
            '√ßanakkale', 'yalova', 'ordu', 'giresun', 'rize', 'artvin', 'g√ºm√º≈ühane',
            'bayburt', 'aƒürƒ±', 'kars', 'iƒüdƒ±r', 'ardahan', 'mu≈ü', 'bitlis', 'hakkari',
            'siirt', '≈üƒ±rnak', 'mardin', 'batman', 'adƒ±yaman', 'kilis', 'osmaniye',
            'hatay', 'isparta', 'burdur', 'afyon', 'u≈üak', 'k√ºtahya', 'bilecik',
            'd√ºzce', 'bolu', 'karab√ºk', 'bartƒ±n', 'kastamonu', '√ßankƒ±rƒ±', 'sinop',
            'amasya', 'tokat', '√ßorum', 'yozgat', 'kƒ±rƒ±kkale', 'aksaray', 'niƒüde',
            'nev≈üehir', 'kƒ±r≈üehir', 'karaman', 'konya'
        ]
        
        query_lower = query.lower()
        
        for city in cities:
            if city in query_lower:
                return city.title()
        
        weather_pattern = r"(\w+)['']?d[ae]\s+(?:hava|sƒ±caklƒ±k|derece)"
        match = re.search(weather_pattern, query_lower)
        if match:
            potential_city = match.group(1)
            if potential_city in cities:
                return potential_city.title()
        
        return None
    
    
    async def _smart_response_analysis(
        self,
        user_input: str,
        llm_response: str,
        original_tool: str
    ) -> str:
        """
        LLM yanƒ±tƒ±nƒ± analiz et
        NOT: Web search kaldƒ±rƒ±ldƒ±, sadece orijinal yanƒ±tƒ± d√∂nd√ºr√ºyor
        """
        return llm_response


    async def process_with_tools(self, user_input: str, chat_history: List) -> str:
        """
        üéØ Tool system ile i≈üle - HafizaAsistani'nƒ±n ANA METODunu kullanarak!

        YENƒ∞ AKI≈û (Refactored):
        1. HafizaAsistani.hazirla_ve_prompt_olustur() ‚Üí Hazƒ±r prompt paketi
        2. Gemma3'e g√∂nder
        3. Cevabƒ± d√∂nd√ºr

        KAZAN√á:
        - 220 satƒ±r ‚Üí 25 satƒ±r (%88 azalma)
        - Tek sorumluluk prensibi
        - Kod tekrarƒ± yok
        - Bakƒ±mƒ± kolay
        """
        print(f"\n{'='*60}")
        print(f"üéØ PROCESS WITH TOOLS (HafizaAsistani v3.0)")
        print(f"{'='*60}")

        paket = await self.memory.hazirla_ve_prompt_olustur(
            user_input=user_input,
            chat_history=chat_history
        )


        print("\n" + "="*60)
        print("üì¶ HAFƒ∞ZA ASƒ∞STANI ‚Üí PERSONAL AI PAKETƒ∞")
        print("="*60)
        print(f"üé≠ Rol: {paket.get('role', 'N/A')}")
        print(f"üîß Tool: {paket.get('tool_used', 'N/A')}")

        llm_decision = paket.get('llm_decision', {})
        print(f"\nüìä LLM Kararƒ±:")
        print(f"   ‚Ä¢ question_type: {llm_decision.get('question_type', 'N/A')}")
        print(f"   ‚Ä¢ needs_faiss: {llm_decision.get('needs_faiss', 'N/A')}")
        print(f"   ‚Ä¢ needs_web: {llm_decision.get('needs_web', 'N/A')}")
        print(f"   ‚Ä¢ needs_semantic_memory: {llm_decision.get('needs_semantic_memory', 'N/A')}")
        print(f"   ‚Ä¢ needs_chat_history: {llm_decision.get('needs_chat_history', 'N/A')}")
        print(f"   ‚Ä¢ response_style: {llm_decision.get('response_style', 'N/A')}")
        reasoning = llm_decision.get('reasoning', 'N/A')
        print(f"   ‚Ä¢ reasoning: {reasoning[:100] if reasoning else 'N/A'}...")

        metadata = paket.get('metadata', {})
        print(f"\nüìã Metadata:")
        print(f"   ‚Ä¢ has_tool_result: {metadata.get('has_tool_result', 'N/A')}")
        print(f"   ‚Ä¢ has_semantic: {metadata.get('has_semantic', 'N/A')}")
        print(f"   ‚Ä¢ has_faiss: {metadata.get('has_faiss', 'N/A')}")
        print(f"   ‚Ä¢ has_history: {metadata.get('has_history', 'N/A')}")

        print(f"\nüìè Prompt uzunluƒüu: {len(paket.get('prompt', ''))} karakter")
        print("="*60 + "\n")

        print("ü§ñ LLM'e g√∂nderiliyor (tek √ßaƒürƒ±)...")
        final_response = await self.llm.generate(paket["prompt"])

        print("‚úÖ Cevap alƒ±ndƒ±!\n")
        return final_response
    
    async def process(
        self,
        user_input: str,
        chat_history: List[Dict[str, Any]],
        image_data: Optional[bytes] = None
    ) -> Tuple[str, str, str]:
        """
        Ana i≈ülem fonksiyonu (TOOL SYSTEM ƒ∞LE!)
        """
        start_time = time.time()
        
        try:
            print(f"\n{'='*60}")
            print(f"üë§ USER: {user_input}")
            print(f"{'='*60}")
            
            mode_response = await self._handle_mode_commands(user_input)
            if mode_response:
                return mode_response, "simple", "command"

            if image_data:
                print("üñºÔ∏è G√∂rsel tespit edildi - Hybrid Vision + Context sistemi kullanƒ±lƒ±yor...")

                vision_prompt = f"Kullanƒ±cƒ± sorusu: {user_input}\n\nBu g√∂rseli kƒ±saca analiz et (2-3 c√ºmle)."
                vision_analysis = await self.llm.generate(vision_prompt, image_data=image_data)
                print(f"üëÅÔ∏è G√∂rsel analizi tamamlandƒ±: {vision_analysis[:100]}...")

                enhanced_input = f"{user_input}\n\n[G√∂rsel Baƒülamƒ±: {vision_analysis}]"

                print("üîß Tool system devreye giriyor (baƒülam + hafƒ±za)...")
                raw_response = await self.process_with_tools(enhanced_input, chat_history)
            else:
                raw_response = await self.process_with_tools(user_input, chat_history)
            
            is_continuing = len(chat_history) > 0
            final_response = self._post_process(raw_response, user_input, is_continuing)
            
            if self._should_save_interaction(user_input, final_response):
                self.memory.add(user_input, final_response, chat_history)

            processing_time = time.time() - start_time
            self.performance_metrics['processing_time'].append(processing_time)
            
            print(f"\n‚è±Ô∏è ƒ∞≈ülem s√ºresi: {processing_time:.2f}s")
            print(f"ü§ñ AI: {final_response[:200]}...")
            print(f"{'='*60}\n")
            
            return final_response, "simple", "success"
        
        except Exception as e:
            print(f"‚ùå HATA: {e}")
            import traceback
            traceback.print_exc()
            
            self.performance_metrics['errors'].append(str(e))
            return "√úzg√ºn√ºm, bir hata olu≈ütu.", "error", "error"
    
    async def _handle_mode_commands(self, user_input: str) -> Optional[str]:
        """√ñzel komutlarƒ± i≈üle"""
        user_lower = user_input.lower()
        
        if any(phrase in user_lower for phrase in ["sistem durum", "stats", "istatistik"]):
            stats = self.get_system_stats()
            
            response = f"""üìä Sistem Durumu:

üß† LLM: {stats['llm']['model']}
üíæ Hafƒ±za: {stats['memory']['total_entries']} kayƒ±t
üìö Bilgi Tabanƒ±: {'Aktif ‚úÖ' if stats['knowledge_base']['enabled'] else 'Kapalƒ± ‚ùå'}
üìñ Wikipedia Tool: Aktif ‚úÖ
üîß Tool System: Aktif ‚úÖ

üìà Performans:
  ‚Ä¢ Toplam etkile≈üim: {stats['performance']['total_interactions']}
  ‚Ä¢ Ort. i≈ülem s√ºresi: {stats['performance']['avg_processing_time']:.2f}s
"""
            return response
        
        if any(phrase in user_lower for phrase in ["hafƒ±za temizle", "memory clear"]):
            self.memory.clear()
            return "‚úÖ Hafƒ±za temizlendi."
        
        return None
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Sistem istatistiklerini d√∂nd√ºr"""
        kb_chunks = self.faiss_kb.index.ntotal if self.faiss_kb and hasattr(self.faiss_kb, 'index') and self.faiss_kb.index else 0
        mem_entries = len(self.memory.data) if hasattr(self.memory, 'data') else 0
        
        return {
            'llm': {
                'model': SystemConfig.MODEL_NAME,
                'provider': SystemConfig.LLM_PROVIDER
            },
            'memory': {
                'total_entries': mem_entries
            },
            'knowledge_base': {
                'enabled': self.faiss_kb and self.faiss_kb.enabled,
                'total_chunks': kb_chunks
            },
            'wikipedia_tool': {
                'enabled': True
            },
            'performance': {
                'total_interactions': len(self.performance_metrics['processing_time']),
                'avg_processing_time': (
                    sum(self.performance_metrics['processing_time']) / 
                    len(self.performance_metrics['processing_time'])
                ) if self.performance_metrics['processing_time'] else 0,
                'success_rate': 100.0
            }
        }
    
    def close(self):
        """Sistemi kapat"""
        print("\nüõë PersonalAI kapatƒ±lƒ±yor...")
        print("‚úÖ Temizlik tamamlandƒ±.")



async def run_interactive_chat(ai_system: PersonalAI):
    """
    ƒ∞nteraktif sohbet modu
    """
    chat_history = []
    
    print("\n" + "=" * 60)
    print("üí¨ ƒ∞nteraktif Sohbet Modu")
    print("=" * 60)
    print("Komutlar:")
    print("  'exit' veya 'quit' - √áƒ±kƒ±≈ü")
    print("  'stats' - ƒ∞statistikler")
    print("  'clear' - Ge√ßmi≈üi temizle")
    print("=" * 60 + "\n")
    
    while True:
        try:
            user_input = input("\nüë§ Sen: ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['exit', 'quit', '√ßƒ±kƒ±≈ü']:
                print("\nüëã G√∂r√º≈ü√ºr√ºz!")
                break
            
            if user_input.lower() in ['clear', 'temizle']:
                chat_history = []
                if user_input.lower() != 'temizle': # Tekrar temizlenmemesi i√ßin
                    pass
                else:
                    print("‚úÖ Sohbet ge√ßmi≈üi temizlendi.")
                continue
            
            print("\nü§ñ AI d√º≈ü√ºn√ºyor...", end="", flush=True)
            reply, _, _ = await ai_system.process(user_input, chat_history)
            print("\r" + " " * 30 + "\r", end="")  # Clear "thinking" message
            
            print(f"ü§ñ AI: {reply}")
            
            chat_history.append({
                "role": "user",
                "content": user_input
            })
            chat_history.append({
                "role": "ai",
                "content": reply
            })
            
            if len(chat_history) > 20:
                chat_history = chat_history[-20:]
            
        except KeyboardInterrupt:
            print("\n\nüëã G√∂r√º≈ü√ºr√ºz!")
            break
        except Exception as e:
            print(f"\n‚ùå Hata: {e}")


async def run_test_scenarios(ai_system: PersonalAI):
    """
    Test senaryolarƒ±
    """
    chat_history = []
    
    print("\n" + "=" * 60)
    print("üß™ TEST SENARYOLARI")
    print("=" * 60)
    
    print("\n--- SENARYO 1: G√ºncel Bilgi (Hava Durumu) ---")
    user_input_1 = "Sakarya i√ßin hava durumu nasƒ±l? Sabah dƒ±≈üarƒ± √ßƒ±kacaƒüƒ±m."
    print(f"üë§ USER: {user_input_1}")
    
    reply_1, _, _ = await ai_system.process(user_input_1, chat_history)
    print(f"ü§ñ AI: {reply_1}\n")
    
    chat_history.append({"role": "user", "content": user_input_1})
    chat_history.append({"role": "ai", "content": reply_1})
    
    print("--- SENARYO 2: Ki≈üisel Hafƒ±za (GraphRAG Test) ---")
    user_input_2 = "Ge√ßen konu≈ütuƒüumuz yapay zeka projemle ilgili ne d√º≈ü√ºn√ºyorsun?"
    print(f"üë§ USER: {user_input_2}")
    
    reply_2, _, _ = await ai_system.process(user_input_2, chat_history)
    print(f"ü§ñ AI: {reply_2}\n")
    
    chat_history.append({"role": "user", "content": user_input_2})
    chat_history.append({"role": "ai", "content": reply_2})
    
    print("--- SENARYO 3: Teknik Destek (Role Switching) ---")
    user_input_3 = "Python'da bir kod hatasƒ± alƒ±yorum: 'ImportError: No module named numpy'. Ne yapmalƒ±yƒ±m?"
    print(f"üë§ USER: {user_input_3}")
    
    reply_3, _, _ = await ai_system.process(user_input_3, chat_history)
    print(f"ü§ñ AI: {reply_3}\n")
    
    print("--- SENARYO 4: Sistem Durumu ---")
    user_input_4 = "sistem durum"
    print(f"üë§ USER: {user_input_4}")
    
    reply_4, _, _ = await ai_system.process(user_input_4, chat_history)
    print(f"ü§ñ AI: {reply_4}\n")
    
    print("=" * 60)
    print("‚úÖ T√ºm test senaryolarƒ± tamamlandƒ±!")
    print("=" * 60)

async def test_spacy_integration():
    """spaCy entegrasyonunu test et"""
    print("\n" + "=" * 60)
    print("üß™ spaCy ENTEGRASYON TESTƒ∞")
    print("=" * 60)
    
    ai = PersonalAI(user_id="test_user")
    
    test_text = """
    Ahmet Yƒ±lmaz, 15 Ocak 2024'te ƒ∞stanbul'da Python √∂ƒürenmeye ba≈üladƒ±.
    Neo4j kullanarak 5000 TL'lik bir proje geli≈ütirdi.
    """
    
    print(f"\nüìù Test Metni:\n{test_text}")
    
    if ai.spacy_nlp.enabled:
        entities = ai.spacy_nlp.extract_entities(test_text)
        print("\nüìç Tespit Edilen Entity'ler:")
        for entity_type, entity_list in entities.items():
            print(f"  {entity_type}: {[e['text'] for e in entity_list]}")
        
        lemmas = ai.spacy_nlp.get_lemmas(test_text)
        print(f"\nüî§ Lemma'lar (ilk 10): {lemmas[:10]}")
        
        chunks = ai.spacy_nlp.get_noun_chunks(test_text)
        print(f"\nüì¶ ƒ∞sim √ñbekleri: {chunks}")
        
        sentiment = ai.spacy_nlp.analyze_sentiment_pos(test_text)
        print(f"\nüòä Sentiment: {sentiment}")
        
        print("\n‚úÖ spaCy entegrasyonu ba≈üarƒ±lƒ±!")
    else:
        print("\n‚ö†Ô∏è spaCy aktif deƒüil!")
    
    print("=" * 60)
    
    ai.close()


def main():
    """
    Ana √ßalƒ±≈ütƒ±rma fonksiyonu
    """
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë                                                           ‚ïë
    ‚ïë             PersonalAI - Geli≈ümi≈ü Asistan                 ‚ïë
    ‚ïë                                                           ‚ïë
    ‚ïë ‚Ä¢ Gemma 3 27B LLM                                         ‚ïë
    ‚ïë ‚Ä¢ FAISS Vector Memory                                     ‚ïë
    ‚ïë ‚Ä¢ Neo4j GraphRAG (Uzun D√∂nem Hafƒ±za)                      ‚ïë
    ‚ïë ‚Ä¢ spaCy NLP Engine                                        ‚ïë
    ‚ïë ‚Ä¢ Multi-Role System                                       ‚ïë
    ‚ïë ‚Ä¢ Web Search Integration                                  ‚ïë
    ‚ïë ‚Ä¢ Chain-of-Thought Reasoning                              ‚ïë
    ‚ïë                                                           ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    try:
        print("\nMod Se√ßin:")
        print("1. ƒ∞nteraktif Sohbet")
        print("2. Test Senaryolarƒ±")
        print("3. spaCy Entegrasyon Testi")  # üÜï EKLE
        
        choice = input("\nSe√ßiminiz (1/2/3): ").strip()
        
        if choice == "1":
            system = PersonalAI(user_id="murat")
            asyncio.run(run_interactive_chat(system))
            system.close()
        elif choice == "2":
            system = PersonalAI(user_id="murat")
            asyncio.run(run_test_scenarios(system))
            system.close()
        elif choice == "3":  # üÜï EKLE
            asyncio.run(test_spacy_integration())
        else:
            print("‚ùå Ge√ßersiz se√ßim!")
        
    except KeyboardInterrupt:
        print("\n\nüõë Program durduruldu.")
    except Exception as e:
        print(f"\n‚ùå Kritik hata: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()