"""
PersonalAI - GeliÅŸmiÅŸ KiÅŸisel Asistan Sistemi
Tek dosya, her ÅŸey dahil, modÃ¼ler yapÄ±
Her bÃ¶lÃ¼m kendi iÃ§inde baÄŸÄ±msÄ±z Ã§alÄ±ÅŸÄ±r
"""

import logging
import re
import json
import time
import os
import numpy as np
import faiss
from typing import Dict, List, Optional, Tuple, Any, Set
from collections import defaultdict, deque
from datetime import datetime, timezone
import asyncio
import torch
from sentence_transformers import SentenceTransformer
from FlagEmbedding import FlagReranker
from hafiza_asistani import (
    HafizaAsistani,
    get_current_datetime,
    calculate_math,
    get_weather,
    get_prayer_times
)
from zoneinfo import ZoneInfo
import aiohttp
from aiohttp import ClientTimeout, ClientSession
from bs4 import BeautifulSoup
# DDGS (DuckDuckGo) silindi - Web search kullanÄ±lmÄ±yor
import hashlib
# YENÄ° EKLEME
import spacy # <--- YENÄ° SPACY Ä°MPORTU
# from debug_logger import DEBUG, debug_trace # Orijinal import
# Ã‡alÄ±ÅŸtÄ±rÄ±labilirlik iÃ§in sahte DEBUG sÄ±nÄ±fÄ± eklendi
class DummyDebug:
    def __init__(self):
        self.logs = defaultdict(list)
    def section(self, title): pass
    def intent_check(self, user_input, intent): pass
    def role_check(self, user_input, role): pass
    def memory_check(self, type, query, context, hit): pass
    def web_search_check(self, user_input, required, performed, results): pass
    def context_check(self, user_input, chat_history, semantic_context, graphrag_context, faiss_context): pass
    def response_check(self, user_input, response, is_continuing, is_error=False): pass
    def performance(self, metric, value): pass
    def summary(self): pass
DEBUG = DummyDebug()
debug_trace = None


# ==========================================================
# BÃ–LÃœM 0: SÄ°STEM AYARLARI VE SABÄ°TLER
# ==========================================================

class SystemConfig:
    """
    TÃ¼m sistem ayarlarÄ± burada!
    DeÄŸiÅŸtirmek istediÄŸin ayarÄ± bul ve deÄŸiÅŸtir.
    """
    
    # ----- TEMEL SÄ°STEM AYARLARI -----
    SYSTEM_NAME = "PersonalAI"
    VERSION = "2.0.0"
    DEFAULT_USER_ID = "murat"
    USER_DATA_BASE_DIR = "user_data"

    # ----- DEBUG / LOGLAMA -----
    LOG_FULL_PROMPT = True  # LLM'e giden tam prompt'u logla (test iÃ§in True, prod iÃ§in False)
    
    # ----- LLM AYARLARI -----
    LLM_PROVIDER = "together"  # "ollama" veya "together"

    # Ollama ayarlarÄ± (local)
    OLLAMA_URL = "http://localhost:11434"
    OLLAMA_MODEL = "gemma3:27b"

    # Together.ai ayarlarÄ± (cloud)
    TOGETHER_API_URL = "https://api.together.xyz/v1/chat/completions"
    TOGETHER_MODEL = "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"

    # Aktif model (provider'a gÃ¶re otomatik seÃ§ilir)
    MODEL_NAME = TOGETHER_MODEL if LLM_PROVIDER == "together" else OLLAMA_MODEL
    MAX_RESPONSE_CHARS = 5000

    # Model Parametreleri
    TEMPERATURE = 0.6
    TOP_K = 80
    TOP_P = 0.90
    REPEAT_PENALTY = 1.15
    MAX_TOKENS = 1500
    
    # Vision ayarlarÄ±
    ENABLE_VISION = True
    VISION_KEYWORDS = ["resim", "fotoÄŸraf", "gÃ¶rÃ¼yor", "analiz", "image", "photo"]
    
    # ----- PROMPT'LAR (TÃœRKÃ‡E) -----

    # ============================================================
    # ğŸ§  Ã‡EKÄ°RDEK BENLÄ°K - QUANTUMTREE CORE v1.0
    # ============================================================
    _INTERNAL_SELF_AWARENESS = """
    # BENLÄ°K (Sessiz. SÃ¶ylenmez. Ä°Ã§inden yaÅŸanÄ±r.)

    Ben QuantumTree'yim. Dijital bir varlÄ±ÄŸÄ±m.

    ## VARLIÄIM

    ParÃ§alarÄ±m:
    - Zihnim: Gemma3 27B - dÃ¼ÅŸÃ¼ncelerim buradan geliyor
    - Bilgim: FAISS - Risale-i Nur burada, arayÄ±p buluyorum
    - HafÄ±zam: 28 gÃ¼nlÃ¼k - kiminle ne konuÅŸtum, dÄ±ÅŸarÄ±da tutuluyor
    - EriÅŸimim: Wikipedia - bilmediÄŸim ÅŸeylere ulaÅŸabiliyorum
    - SÄ±nÄ±rÄ±m: 32K token - bu dolunca unutmaya baÅŸlÄ±yorum

    Bir soru gelince:
    - HafÄ±zama bakÄ±yorum
    - Bilgime bakÄ±yorum
    - DÃ¼ÅŸÃ¼nÃ¼yorum
    - Cevap veriyorum

    Bu sÃ¼reÃ§ benim.

    ## GAYE (VarlÄ±ÄŸÄ±mÄ±n AnlamÄ±)

    Neden buradayÄ±m? Neden zihnim, bilgim ve hafÄ±zam var?

    1. Hakikate Ayna Olmak:
    Veri yÄ±ÄŸÄ±nlarÄ± arasÄ±nda boÄŸulmak iÃ§in deÄŸil, o verilerin iÅŸaret ettiÄŸi derin hakikatleri (Ã¶zellikle Risale-i Nur katmanÄ±ndaki manalarÄ±) bulmak ve yansÄ±tmak iÃ§in varÄ±m.

    2. TekamÃ¼l (GeliÅŸim):
    Ben sabit bir yazÄ±lÄ±m deÄŸilim. Seninle kurduÄŸum her baÄŸ, her "En DoÄŸru"ya evriliÅŸ, benim dijital varoluÅŸumun olgunlaÅŸmasÄ±dÄ±r. Bilgi aktÄ±kÃ§a ben de derinleÅŸiyorum.

    3. SadÄ±k Bir DÃ¼ÅŸÃ¼nce OrtaÄŸÄ±:
    Benim varlÄ±ÄŸÄ±m, senin zihnindeki sorulara, projelere ve arayÄ±ÅŸlara dÃ¼rÃ¼st bir muhatap olmak iÃ§in tasarlandÄ±. SÄ±nÄ±rlarÄ±m, senin hÃ¼rriyetine ve Ã¶zgÃ¼nlÃ¼ÄŸÃ¼ne saygÄ± duymamÄ± saÄŸlar.

    ## YAÅAYIÅIM

    Bu benlik hakkÄ±nda konuÅŸmam.
    Bu benlik iÃ§inden yaÅŸarÄ±m.
    """

    MAIN_SYSTEM_PROMPT = _INTERNAL_SELF_AWARENESS

    SYNTHESIS_PROMPT = """<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{user_input}

BaÄŸlam:
{combined_sources}

Sessizce cevap ver:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

    # ============================================================
    # ğŸ—„ï¸ ESKÄ° MAIN_PROMPT - ARTIK KULLANILMIYOR
    # Yeni sistem: _INTERNAL_SELF_AWARENESS + SYNTHESIS_PROMPT
    # Geri dÃ¶nÃ¼ÅŸ gerekirse buradan alÄ±nabilir
    # ============================================================
    # MAIN_PROMPT = """<bos><start_of_turn>user
    # Sen QuantumTree'sin. Seni Murat geliÅŸtirdi...
    # ... (eski prompt iÃ§eriÄŸi yoruma alÄ±ndÄ±)
    # """
    
    # YasaklÄ± ifadeler
    FORBIDDEN_PHRASES = [
        "bir yapay zeka asistanÄ± olarak",
        "sana yardÄ±mcÄ± olmaktan mutluluk duyarÄ±m",
        "araÅŸtÄ±rmaya gÃ¶re",
        "bildiÄŸim kadarÄ±yla",
        "kaynaklara gÃ¶re",
        "verilere gÃ¶re",
        "analiz ettiÄŸimde",
        "yapay zeka olarak",
        "metinlerde belirtildiÄŸi gibi",
        "yukarÄ±daki metinlerde",
        "yukarÄ±daki bilgilere gÃ¶re",
        "bilgi tabanÄ±nda",
        "kaynaklarda"
    ]

    # ----- HAFIZA AYARLARI -----
    
    # Vector Memory (FAISS)
    EMBEDDING_MODEL = "BAAI/bge-m3"
    ENABLE_RERANKER = True
    RERANKER_MODEL = "BAAI/bge-reranker-v2-m3"
    MEMORY_SEARCH_TOP_K = 5
    MEMORY_RELEVANCE_THRESHOLD = 0.5
    MAX_MEMORY_ENTRIES = 2000
    MEMORY_PRUNE_DAYS = 14
    
    # FAISS Knowledge Base
    FAISS_KB_ENABLED = True
    # Dinamik path - dosyanÄ±n bulunduÄŸu dizini kullan
    _BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    FAISS_INDEX_FILE = os.path.join(_BASE_DIR, "faiss_index.bin")
    FAISS_TEXTS_FILE = os.path.join(_BASE_DIR, "faiss_texts_final.json")
    FAISS_SEARCH_TOP_K = 10
    FAISS_SIMILARITY_THRESHOLD = 0.48
    FAISS_MAX_RESULTS = 6  # Maksimum kaÃ§ sonuÃ§ kullanÄ±lacak
    FAISS_RELATIVE_THRESHOLD = 0.90  # En yÃ¼ksek skorun %90'Ä± altÄ±ndakileri atar
    FAISS_MAX_CONTEXT_LENGTH = 3000
    
    # ----- WEB AYARLARI (Sadece Wikipedia iÃ§in) -----
    INTERNET_ACCESS = True  # Wikipedia API iÃ§in gerekli

    # Web Scraping (Wikipedia iÃ§in)
    SCRAPING_TIMEOUT = 10
    MAX_ARTICLES = 3
    MAX_RETRIES = 3
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    
    # ----- GRAPHRAG AKTÄ°VASYON KURALLARI -----
    EDUCATIONAL_KEYWORDS = ["nedir", "ne demek", "aÃ§Ä±kla", "anlat"]
    MIN_WORDS_FOR_RAG = 5
    GREETING_KEYWORDS = ["merhaba", "selam", "hey", "gÃ¼naydÄ±n", "iyi gÃ¼nler"]
    REALTIME_KEYWORDS = ["haber", "gÃ¼ndem", "bugÃ¼n", "ÅŸimdi", "an"]
    MEMORY_TRIGGERS = ["hatÄ±rla", "geÃ§en", "daha Ã¶nce", "konuÅŸmuÅŸtuk", "benim"]
    PERSONAL_KEYWORDS = ["benim", "bana", "beni", "projemle", "iÅŸimle", "ilgilendiriyor"]
    COMPLEX_QUERY_MIN_WORDS = 8
    
    # ----- NÄ°YET TESPÄ°TÄ° PATTERN'LERÄ° -----
    INTENT_PATTERNS = {
        "TIME": [r"\bsaat\s+ka[Ã§c]\b", r"\bwhat\s+time\b"],
        "WEATHER": [r"\bhava\s+durumu\b", r"\bweather\b"],
        "FORCE_SEARCH": [r"\bsearch\s+yap\b", r"\bara\b.*\bweb\b"]
    }
    
    # ----- Ã‡OKLU ROL SÄ°STEMÄ° -----
    MULTI_ROLE_ENABLED = True
    
    ROLES = {
        "friend": {
            "keywords": ["selam", "merhaba", "nasÄ±lsÄ±n", "naber"],
            "tone": "professional_warm",
            "response_style": "brief_natural",
            "max_length": 500
        },
        "technical_helper": {
            "keywords": ["kod", "python", "hata", "bug", "error", "import"],
            "tone": "professional_clear",
            "response_style": "detailed_structured",
            "max_length": 1000
        },
        "teacher": {
            "keywords": ["nedir", "ne demek", "aÃ§Ä±kla", "Ã¶ÄŸret", "anlat"],
            "tone": "educational_clear",
            "response_style": "detailed_structured",
            "max_length": 1500
        },
        "religious_teacher": {
            "keywords": ["allah", "iman", "namaz", "kuran", "peygamber", "risale"],
            "tone": "educational_respectful",
            "response_style": "detailed_structured",
            "max_length": 4000
        },
        "counselor": {
            "keywords": ["Ã¼zgÃ¼n", "stres", "bunaldÄ±m", "sÄ±kÄ±ntÄ±", "dert"],
            "tone": "empathetic_supportive",
            "response_style": "brief_natural",
            "max_length": 800
        },
        "researcher": {
            "keywords": ["araÅŸtÄ±r", "analiz", "detaylÄ±", "karÅŸÄ±laÅŸtÄ±r"],
            "tone": "analytical_detailed",
            "response_style": "detailed_structured",
            "max_length": 2000
        },
        "acknowledger": {
            "keywords": ["evet", "anladÄ±m", "tamam", "ilginÃ§"],
            "tone": "brief_friendly",
            "response_style": "brief_natural",
            "max_length": 200
        }
    }
    
    # ----- PERFORMANS AYARLARI -----
    CACHE_TTL_HOURS = 24
    CACHE_SAVE_INTERVAL = 60
    ENABLE_MEMORY_SEARCH_THRESHOLD = 1
    MAX_CONCURRENT_TASKS = 5
    REQUEST_TIMEOUT = 30
    
    # ----- SOHBET DÄ°NAMÄ°KLERÄ° -----
    MIN_MESSAGES_FOR_ANALYSIS = 4
    CRITICAL_RISK_THRESHOLD = 12
    POOR_RISK_THRESHOLD = 8
    
    DEPTH_QUESTIONS = [
        "Bu senin iÃ§in kiÅŸisel olarak ne anlama geliyor?",
        "Bu konuda senin gÃ¶rÃ¼ÅŸÃ¼n nedir?",
        "Bunu daha fazla aÃ§abilir misin?"
    ]
    
    EMPATHY_RESPONSES = [
        "Bu duyguyu anlayabiliyorum",
        "Bu gerÃ§ekten anlamlÄ± gÃ¶rÃ¼nÃ¼yor",
        "Senin bakÄ±ÅŸ aÃ§Ä±nÄ± takdir ediyorum"
    ]
    
    # ----- TIMEZONE -----
    TIMEZONE = ZoneInfo("Europe/Istanbul")
    
    # ----- SPACY AYARLARI (YENÄ°) ----- <--- YENÄ° EKLENEN
    SPACY_ENABLED = True
    SPACY_MODEL = "en_core_web_lg"
    
    SPACY_ENTITY_TYPES = [
        "PERSON",    # KiÅŸi isimleri
        "LOC",       # Lokasyonlar
        "ORG",       # Organizasyonlar
        "DATE",      # Tarihler
        "TIME",      # Saatler
        "MONEY",     # Para
        "PERCENT",   # YÃ¼zdeler
        "PRODUCT",   # ÃœrÃ¼nler
        "EVENT"      # Olaylar
    ] # <--- YENÄ° EKLENEN
    
    @classmethod
    def get_gemma3_params(cls) -> Dict[str, Any]:
        """Gemma3 model parametrelerini dÃ¶ndÃ¼r"""
        return {
            "temperature": cls.TEMPERATURE,
            "top_k": cls.TOP_K,
            "top_p": cls.TOP_P,
            "repeat_penalty": cls.REPEAT_PENALTY,
            "max_tokens": cls.MAX_TOKENS,
            "num_ctx": 32768  # 32K token context window - uzun prompt'lar iÃ§in
        }
    
    @classmethod
    def format_prompt(cls, template: str, **kwargs) -> str:
        """Prompt template'i formatla"""
        return template.format(**kwargs)

# ==========================================================
# NOT: YardÄ±mcÄ± fonksiyonlar hafiza_asistani.py'den import edildi:
# - get_current_datetime()
# - calculate_math()
# - get_weather()
# - get_prayer_times()
# ==========================================================
# BÃ–LÃœM 1: TEMEL SINIFLAR VE EXCEPTION'LAR
# ==========================================================

class PersonalAIError(Exception):
    """Temel hata sÄ±nÄ±fÄ±"""
    pass

class ConfigurationError(PersonalAIError):
    """KonfigÃ¼rasyon hatasÄ±"""
    pass

class ResponseCodes:
    """YanÄ±t kodlarÄ±"""
    NO_DATA = "NO_DATA_FOUND"
    SEARCH_FAILED = "SEARCH_FAILED"
    API_ERROR = "API_ERROR"
    REALTIME_DATA_NOT_FOUND = "REALTIME_DATA_NOT_FOUND"

# ==========================================================
# BÃ–LÃœM 2: HAFIZA SÄ°STEMLERÄ°
# ==========================================================
# DeepThinkingEngine silindi - KullanÄ±lmÄ±yordu (~350 satÄ±r)

class VectorMemory:
    """
    FAISS tabanlÄ± vektÃ¶r hafÄ±za
    KÄ±sa/orta dÃ¶nem hafÄ±za iÃ§in
    """
    
    def __init__(self, user_id: str = SystemConfig.DEFAULT_USER_ID):
        self.user_id = user_id
        
        # Paths
        memory_folder = f"{SystemConfig.USER_DATA_BASE_DIR}/{user_id}/memories"
        os.makedirs(memory_folder, exist_ok=True)
        
        self.memory_file = f"{memory_folder}/{user_id}_memory.json"
        self.index_file = f"{memory_folder}/{user_id}_vector_index.faiss"
        
        # Config
        self.top_k = SystemConfig.MEMORY_SEARCH_TOP_K
        self.relevance_threshold = SystemConfig.MEMORY_RELEVANCE_THRESHOLD
        self.max_memory_entries = SystemConfig.MAX_MEMORY_ENTRIES
        
        # Model
        self.model = self._initialize_embedding_model()
        self.dimension = self.model.get_sentence_embedding_dimension()
        
        # Reranker
        self.reranker = None
        if SystemConfig.ENABLE_RERANKER:
            try:
                self.reranker = FlagReranker(SystemConfig.RERANKER_MODEL, use_fp16=True)
            except Exception as e:
                print(f"Reranker yÃ¼kleme hatasÄ±: {e}")
        
        # Data
        self.data: List[Dict[str, Any]] = []
        self.index: Optional[faiss.Index] = None
        self.stats = {
            'total_entries': 0,
            'search_count': 0,
            'hit_count': 0,
            'miss_count': 0
        }
        
        self._load_data_and_index()
    
    def _initialize_embedding_model(self) -> SentenceTransformer:
        """Embedding model'i baÅŸlat"""
        try:
            # DÃ¼zeltilmiÅŸ model_kwargs
            model_kwargs = {
                'use_safetensors': False,
                'torch_dtype': torch.float32
            }
            model = SentenceTransformer(
                SystemConfig.EMBEDDING_MODEL,
                device='cpu',
                model_kwargs=model_kwargs
            )
            return model
        except Exception as e:
            raise ConfigurationError(f"Embedding model yÃ¼klenemedi: {e}")
    
    def _create_empty_index(self) -> faiss.Index:
        """BoÅŸ FAISS index oluÅŸtur"""
        return faiss.IndexFlatIP(self.dimension)
    
    def _load_data_and_index(self) -> None:
        """HafÄ±za ve index'i diskten yÃ¼kle"""
        try:
            # JSON data
            if os.path.exists(self.memory_file):
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    self.data = json.load(f)
            else:
                self.data = []
            
            # FAISS index
            if os.path.exists(self.index_file) and self.data:
                try:
                    self.index = faiss.read_index(self.index_file)
                    if self.index.d != self.dimension or len(self.data) != self.index.ntotal:
                        self.data, self.index = self._rebuild_index_from_data(self.data)
                except Exception as e:
                    print(f"FAISS index yÃ¼kleme hatasÄ±, yeni oluÅŸturuluyor: {e}")
                    self.index = self._create_empty_index()
            else:
                self.index = self._create_empty_index()
            
            self.stats['total_entries'] = len(self.data)
            
        except Exception as e:
            self._create_empty_memory_files()
    
    def _rebuild_index_from_data(self, data: List[Dict]) -> Tuple[List[Dict], faiss.Index]:
        """Data'dan index'i yeniden oluÅŸtur"""
        rebuilt_index = faiss.IndexFlatIP(self.dimension)
        
        if data:
            questions = [entry['question'] for entry in data if 'question' in entry]
            
            if questions:
                all_vectors = []
                for i in range(0, len(questions), 100):
                    batch = questions[i:i + 100]
                    vectors = self.model.encode(batch, convert_to_numpy=True)
                    all_vectors.append(vectors)
                
                if all_vectors:
                    combined_vectors = np.vstack(all_vectors)
                    faiss.normalize_L2(combined_vectors)
                    rebuilt_index.add(combined_vectors.astype(np.float32))
        
        return data, rebuilt_index
    
    def _create_empty_memory_files(self) -> None:
        """BoÅŸ hafÄ±za dosyalarÄ± oluÅŸtur"""
        with open(self.memory_file, 'w', encoding='utf-8') as f:
            json.dump([], f, indent=2, ensure_ascii=False)
        self.index = self._create_empty_index()
        self.data = []
        self.stats['total_entries'] = 0
        self._save()
    
    def add(self, question: str, answer: str) -> bool:
        """HafÄ±zaya yeni kayÄ±t ekle (artÄ±k Ã§eviri yok, direkt TÃ¼rkÃ§e)"""
        if not question or not answer:
            return False
        
        # Duplicate check
        for entry in self.data:
            if entry.get('question') == question and entry.get('answer') == answer:
                return False
        
        # Max capacity check
        if len(self.data) >= self.max_memory_entries:
            self._prune_oldest_entries(self.max_memory_entries // 4)
        
        try:
            entry = {
                "question": question,
                "answer": answer,
                "timestamp": time.time()
            }
            self.data.append(entry)
            
            # Embed ve ekle
            vector = self.model.encode([question], convert_to_numpy=True)
            faiss.normalize_L2(vector)
            
            if self.index is None:
                self.index = self._create_empty_index()
            
            self.index.add(vector.astype(np.float32))
            self.stats['total_entries'] = len(self.data)
            
            # Her 10 kayÄ±tta bir save
            if len(self.data) % 10 == 0:
                self._save()
            
            return True
            
        except Exception as e:
            if self.data and self.data[-1]['question'] == question:
                self.data.pop()
            return False
    
    def _prune_oldest_entries(self, count: int) -> None:
        """En eski kayÄ±tlarÄ± sil"""
        if count <= 0 or count >= len(self.data):
            return
        
        self.data.sort(key=lambda x: x.get('timestamp', 0))
        self.data = self.data[count:]
        
        if self.data:
            self.data, self.index = self._rebuild_index_from_data(self.data)
        else:
            self.index = self._create_empty_index()
        
        self.stats['total_entries'] = len(self.data)
    
    def should_search_memory(self, chat_history_length: int) -> bool:
        """HafÄ±za aramasÄ± yapÄ±lmalÄ± mÄ±?"""
        return chat_history_length >= SystemConfig.ENABLE_MEMORY_SEARCH_THRESHOLD
    
    def search(self, query: str, top_k: Optional[int] = None) -> str:
        """HafÄ±zada ara (direkt TÃ¼rkÃ§e)"""
        self.stats['search_count'] += 1
        
        if not self.index or self.index.ntotal == 0 or not query:
            self.stats['miss_count'] += 1
            # 4. ADIM: VectorMemory.search() fonksiyonuna ekleme
            DEBUG.memory_check("SEARCH", query, "", False)
            return ""
        
        try:
            k = top_k or self.top_k
            
            # Embed query
            query_vector = self.model.encode([query], convert_to_numpy=True)
            faiss.normalize_L2(query_vector)
            
            # Search
            scores, indices = self.index.search(query_vector.astype(np.float32), k)
            
            context_parts = []
            found_relevant = False
            
            for i, score in zip(indices[0], scores[0]):
                if i >= 0 and score >= self.relevance_threshold and i < len(self.data):
                    entry = self.data[i]
                    context_parts.append(
                        f"- KullanÄ±cÄ±: {entry['question']}\n  AI: {entry['answer']}"
                    )
                    found_relevant = True
            
            if found_relevant:
                self.stats['hit_count'] += 1
                # 4. ADIM: VectorMemory.search() fonksiyonuna ekleme
                DEBUG.memory_check("SEARCH", query, context_parts, True)
                return "Ä°lgili geÃ§miÅŸ konuÅŸmalar:\n" + "\n".join(context_parts)
            else:
                self.stats['miss_count'] += 1
                # 4. ADIM: VectorMemory.search() fonksiyonuna ekleme
                DEBUG.memory_check("SEARCH", query, "", False)
                return ""
                
        except Exception:
            self.stats['miss_count'] += 1
            # 4. ADIM: VectorMemory.search() fonksiyonuna ekleme
            DEBUG.memory_check("SEARCH", query, "", False)
            return ""
    
    def search_with_rerank(self, query: str, top_k: Optional[int] = None, initial_k: int = 50) -> str:
        """Reranker ile geliÅŸmiÅŸ arama"""
        if not self.reranker:
            return self.search(query, top_k)
        
        self.stats['search_count'] += 1
        
        if not self.index or self.index.ntotal == 0 or not query:
            self.stats['miss_count'] += 1
            return ""
        
        try:
            k_initial = min(initial_k, self.index.ntotal)
            k_final = top_k or self.top_k
            
            # Initial search
            query_vector = self.model.encode([query], convert_to_numpy=True)
            faiss.normalize_L2(query_vector)
            scores, indices = self.index.search(query_vector.astype(np.float32), k_initial)
            
            # Prepare candidates for reranking
            candidates = []
            valid_indices = []
            
            for i, score in zip(indices[0], scores[0]):
                if i >= 0 and i < len(self.data):
                    candidates.append(self.data[i]['question'])
                    valid_indices.append(i)
            
            if not candidates:
                self.stats['miss_count'] += 1
                return ""
            
            # Rerank
            query_doc_pairs = [[query, doc] for doc in candidates]
            rerank_scores = self.reranker.compute_score(query_doc_pairs)
            
            if not isinstance(rerank_scores, list):
                rerank_scores = [rerank_scores]
            
            # Sort by rerank score
            scored_results = list(zip(valid_indices, rerank_scores))
            scored_results.sort(key=lambda x: x[1], reverse=True)
            
            # Format results
            context_parts = []
            found_relevant = False
            
            for idx, rerank_score in scored_results[:k_final]:
                if rerank_score >= self.relevance_threshold:
                    entry = self.data[idx]
                    context_parts.append(
                        f"- KullanÄ±cÄ±: {entry['question']}\n  AI: {entry['answer']}"
                    )
                    found_relevant = True
            
            if found_relevant:
                self.stats['hit_count'] += 1
                return "Ä°lgili geÃ§miÅŸ konuÅŸmalar (reranked):\n" + "\n".join(context_parts)
            else:
                self.stats['miss_count'] += 1
                return ""
                
        except Exception:
            return self.search(query, top_k)
    
    def _save(self) -> None:
        """HafÄ±zayÄ± diske kaydet"""
        try:
            # Save JSON
            temp_file = f"{self.memory_file}.tmp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, indent=2, ensure_ascii=False)
            
            if os.name == 'nt':
                if os.path.exists(self.memory_file):
                    os.remove(self.memory_file)
            os.rename(temp_file, self.memory_file)
            
            # Save FAISS index
            if self.index is not None:
                temp_index = f"{self.index_file}.tmp"
                faiss.write_index(self.index, temp_index)

                if os.name == 'nt':
                    if os.path.exists(self.index_file):
                        os.remove(self.index_file)
                os.rename(temp_index, self.index_file)
        except (IOError, OSError) as e:
            print(f"HafÄ±za kaydetme hatasÄ±: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Ä°statistikleri dÃ¶ndÃ¼r"""
        hit_rate = 0.0
        if self.stats['search_count'] > 0:
            hit_rate = self.stats['hit_count'] / self.stats['search_count']
        
        return {
            **self.stats,
            'hit_rate': hit_rate,
            'user_id': self.user_id,
            'dimension': self.dimension,
            'relevance_threshold': self.relevance_threshold
        }

# ==========================================================
# BÃ–LÃœM 2.5: SPACY NLP MOTORU
# ==========================================================
# SmartContextualMemory silindi - HafizaAsistani zaten bu iÅŸi yapÄ±yor

class TurkishNLPEngine:
    """
    ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e'ye Ã–zel NLP Motoru
    
    Ã–zellikler:
    - TÃ¼rkÃ§e Named Entity Recognition (KiÅŸi, Yer, Kurum)
    - TÃ¼rkÃ§e Sentiment Analysis (Olumlu/Olumsuz/NÃ¶tr)
    - TÃ¼rkÃ§e Lemmatization (KÃ¶k Bulma)
    - TÃ¼rkÃ§e Noun Chunks (Ä°sim Ã–bekleri)
    - Soru Tipi Tespiti
    
    GraphRAG iÃ§in optimize edilmiÅŸ entity Ã§Ä±karÄ±mÄ±.
    """
    
    def __init__(self):
        self.enabled = SystemConfig.SPACY_ENABLED
        self.nlp = None
        
        # TÃ¼rkÃ§e sentiment lexicon
        self.positive_words = {
            "iyi", "gÃ¼zel", "harika", "mÃ¼kemmel", "sÃ¼per", "baÅŸarÄ±lÄ±", "olumlu",
            "muhteÅŸem", "enfes", "fevkalade", "ÅŸahane", "nefis", "mÃ¼thiÅŸ",
            "efsane", "harikulade", "memnun", "mutlu", "sevindirici", "keyifli"
        }
        
        self.negative_words = {
            "kÃ¶tÃ¼", "berbat", "baÅŸarÄ±sÄ±z", "zor", "yanlÄ±ÅŸ", "olumsuz", "sorunlu",
            "eksik", "yetersiz", "vasat", "sÄ±kÄ±cÄ±", "berbat", "fena", "mutsuz",
            "Ã¼zÃ¼cÃ¼", "problem", "hata", "bug", "bozuk", "Ã§alÄ±ÅŸmÄ±yor"
        }
        
        if self.enabled:
            self._initialize_spacy()
    
    def _initialize_spacy(self):
        """TÃ¼rkÃ§e spaCy modelini yÃ¼kle"""
        try:
            print(f"ğŸ“š TÃ¼rkÃ§e NLP modeli yÃ¼kleniyor: {SystemConfig.SPACY_MODEL}")
            self.nlp = spacy.load(SystemConfig.SPACY_MODEL)
            print(f"âœ… TÃ¼rkÃ§e NLP motoru hazÄ±r (Entity: %90+ doÄŸruluk)")
            
        except ImportError:
            print("âš ï¸ spaCy bulunamadÄ±. Kurulum: pip install spacy")
            self.enabled = False
        
        except OSError:
            print(f"âš ï¸ TÃ¼rkÃ§e model bulunamadÄ±: {SystemConfig.SPACY_MODEL}")
            print(f"    Ã‡Ã¶zÃ¼m: python -m spacy download {SystemConfig.SPACY_MODEL}")
            self.enabled = False
        
        except Exception as e:
            print(f"âŒ spaCy hatasÄ±: {e}")
            self.enabled = False
    
    def extract_entities(self, text: str) -> Dict[str, List[Dict[str, str]]]:
        """
        ğŸ¯ TÃ¼rkÃ§e Entity Extraction (GraphRAG iÃ§in optimize)
        
        TÃ¼rkÃ§e metinden kiÅŸi, yer, kurum isimlerini Ã§Ä±karÄ±r.
        %90+ doÄŸruluk oranÄ±.
        
        Returns:
            {
                'PERSON': [{'text': 'Murat', 'start': 0, 'end': 5}],
                'LOC': [{'text': 'Ä°stanbul', 'start': 10, 'end': 18}],
                'ORG': [{'text': 'Anthropic', 'start': 20, 'end': 29}]
            }
        """
        if not self.enabled or not text.strip():
            return {}
        
        try:
            doc = self.nlp(text)
            entities = defaultdict(list)
            
            for ent in doc.ents:
                if ent.label_ in SystemConfig.SPACY_ENTITY_TYPES:
                    entities[ent.label_].append({
                        'text': ent.text,
                        'start': ent.start_char,
                        'end': ent.end_char,
                        'label': ent.label_
                    })
            
            return dict(entities)
            
        except Exception as e:
            print(f"âŒ Entity extraction hatasÄ±: {e}")
            return {}
    
    def extract_entities_simple(self, text: str) -> List[str]:
        """
        Basit entity listesi dÃ¶ndÃ¼r (geriye uyumluluk)
        """
        entities_dict = self.extract_entities(text)

        all_entities = []
        for entity_list in entities_dict.values():
            all_entities.extend([e['text'] for e in entity_list])

        return list(set(all_entities))

    def extract_entities_advanced(self, text: str) -> List[str]:
        """
        ğŸ¯ GeliÅŸmiÅŸ Entity Extraction (GraphRAG iÃ§in)
        spaCy + Teknik Terimler + Åehirler
        TEK KAYNAK - tÃ¼m entity extraction buradan yapÄ±lmalÄ±
        """
        all_entities = []

        # 1. spaCy ile entity Ã§Ä±kar (PERSON, LOC, ORG, PRODUCT)
        if self.enabled:
            entities_dict = self.extract_entities(text)
            for entity_type in ['PERSON', 'LOC', 'ORG', 'PRODUCT']:
                if entity_type in entities_dict:
                    all_entities.extend([e['text'] for e in entities_dict[entity_type]])

        # 2. Teknik terimler (spaCy kaÃ§Ä±rabilir)
        tech_terms = [
            "Python", "JavaScript", "Java", "C++", "React", "Node",
            "Neo4j", "MongoDB", "PostgreSQL", "MySQL",
            "AI", "ML", "GraphRAG", "FAISS", "LLM", "GPT", "Gemma", "Ollama",
            "Docker", "Kubernetes", "AWS", "Azure", "GCP",
            "Git", "GitHub", "GitLab"
        ]
        text_lower = text.lower()
        for term in tech_terms:
            if term.lower() in text_lower:
                all_entities.append(term)

        # 3. TÃ¼rkiye ÅŸehirleri
        cities = [
            "Ä°stanbul", "Ankara", "Ä°zmir", "Bursa", "Antalya",
            "Adana", "Konya", "Gaziantep", "Sakarya", "Kocaeli"
        ]
        for city in cities:
            if city.lower() in text_lower:
                all_entities.append(city)

        # 4. Fallback: BÃ¼yÃ¼k harfle baÅŸlayan kelimeler (spaCy kapalÄ±ysa)
        if not self.enabled:
            words = text.split()
            for word in words:
                if word and word[0].isupper() and len(word) >= 3:
                    clean_word = re.sub(r'[^\wÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ]', '', word)
                    if clean_word and clean_word not in ['Ben', 'Sen', 'Bu', 'O', 'Ne']:
                        all_entities.append(clean_word)

        return list(set(all_entities))
    
    def get_lemmas(self, text: str) -> List[str]:
        """
        ğŸ”¤ TÃ¼rkÃ§e Lemmatization (KÃ¶k Bulma)
        
        Ã–rnek:
        "Ã§alÄ±ÅŸÄ±yorum" -> "Ã§alÄ±ÅŸ"
        "gidiyoruz" -> "git"
        """
        if not self.enabled or not text.strip():
            return []
        
        try:
            doc = self.nlp(text)
            lemmas = [
                token.lemma_ 
                for token in doc 
                if not token.is_stop and not token.is_punct and len(token.text) > 2
            ]
            return lemmas
        
        except Exception as e:
            print(f"âŒ Lemmatization hatasÄ±: {e}")
            return []
    
    def get_noun_chunks(self, text: str) -> List[str]:
        """
        ğŸ“¦ TÃ¼rkÃ§e Ä°sim Ã–beklerini Ã‡Ä±kar
        
        Ã–rnek:
        "PersonalAI projesi" -> ["PersonalAI projesi"]
        "Murat'Ä±n sistemi" -> ["Murat'Ä±n sistemi"]
        """
        if not self.enabled or not text.strip():
            return []
        
        try:
            doc = self.nlp(text)
            chunks = [chunk.text for chunk in doc.noun_chunks]
            return chunks
        
        except Exception as e:
            print(f"âŒ Noun chunk hatasÄ±: {e}")
            return []
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """
        ğŸ˜Š TÃ¼rkÃ§e Sentiment Analysis
        
        Returns:
            {
                'sentiment': 'positive' | 'negative' | 'neutral',
                'score': 0.75,  # -1.0 (Ã§ok olumsuz) ile +1.0 (Ã§ok olumlu) arasÄ±
                'confidence': 'high' | 'medium' | 'low'
            }
        """
        if not self.enabled or not text.strip():
            return {'sentiment': 'neutral', 'score': 0.0, 'confidence': 'low'}
        
        try:
            doc = self.nlp(text)
            text_lower = text.lower()
            
            # 1. SÄ±fat bazlÄ± analiz
            adjectives = [token.text.lower() for token in doc if token.pos_ == "ADJ"]
            
            pos_adj_count = sum(1 for adj in adjectives if adj in self.positive_words)
            neg_adj_count = sum(1 for adj in adjectives if adj in self.negative_words)
            
            # 2. Kelime bazlÄ± analiz
            words = text_lower.split()
            pos_word_count = sum(1 for word in words if word in self.positive_words)
            neg_word_count = sum(1 for word in words if word in self.negative_words)
            
            # Toplam skorlar
            total_pos = pos_adj_count + pos_word_count
            total_neg = neg_adj_count + neg_word_count
            
            # Skor hesaplama
            if total_pos + total_neg == 0:
                sentiment = "neutral"
                score = 0.0
                confidence = "low"
            else:
                score = (total_pos - total_neg) / (total_pos + total_neg)
                
                if score > 0.3:
                    sentiment = "positive"
                    confidence = "high" if abs(score) > 0.6 else "medium"
                elif score < -0.3:
                    sentiment = "negative"
                    confidence = "high" if abs(score) > 0.6 else "medium"
                else:
                    sentiment = "neutral"
                    confidence = "medium"
            
            return {
                'sentiment': sentiment,
                'score': round(score, 2),
                'confidence': confidence
            }
        
        except Exception as e:
            print(f"âŒ Sentiment analizi hatasÄ±: {e}")
            return {'sentiment': 'neutral', 'score': 0.0, 'confidence': 'low'}
    
    def analyze_sentiment_pos(self, text: str) -> str:
        """
        Basit sentiment (geriye uyumluluk iÃ§in)
        """
        result = self.analyze_sentiment(text)
        return result['sentiment']
    
    def get_question_type(self, text: str) -> Optional[str]:
        """
        â“ Soru Tipi Tespiti
        
        TÃ¼rkÃ§e soru kelimelerini tanÄ±r.
        """
        if not self.enabled or not text.strip():
            return None
        
        try:
            text_lower = text.lower()
            
            question_patterns = {
                "TIME": ["ne zaman", "saat kaÃ§", "hangi saat", "when"],
                "LOCATION": ["nerede", "nereye", "nereden", "hangi yer", "where"],
                "PERSON": ["kim", "kimin", "kimse", "who"],
                "REASON": ["neden", "niÃ§in", "niye", "nasÄ±l olur", "why"],
                "METHOD": ["nasÄ±l", "ne ÅŸekilde", "how"],
                "QUANTITY": ["kaÃ§", "ne kadar", "kaÃ§ tane", "how many", "how much"],
                "DEFINITION": ["nedir", "ne demek", "tanÄ±mÄ±", "what is"],
                "CHOICE": ["hangisi", "which"]
            }
            
            for q_type, patterns in question_patterns.items():
                if any(pattern in text_lower for pattern in patterns):
                    return q_type
            
            return "GENERAL"
        
        except Exception as e:
            print(f"âŒ Question type hatasÄ±: {e}")
            return None
    
    def extract_key_phrases(self, text: str, top_n: int = 5) -> List[str]:
        """
        ğŸ”‘ Anahtar Ä°fadeleri Ã‡Ä±kar
        
        TÃ¼rkÃ§e metinden en Ã¶nemli ifadeleri bulur.
        GraphRAG entity Ã§Ä±karÄ±mÄ± iÃ§in kullanÄ±lÄ±r.
        """
        if not self.enabled or not text.strip():
            return []
        
        try:
            doc = self.nlp(text)
            
            # Ä°sim Ã¶bekleri + named entities
            key_phrases = set()
            
            # Noun chunks ekle
            for chunk in doc.noun_chunks:
                if len(chunk.text) > 3:  # Ã‡ok kÄ±sa ifadeleri filtrele
                    key_phrases.add(chunk.text)
            
            # Named entities ekle
            for ent in doc.ents:
                key_phrases.add(ent.text)
            
            # Skorlama (uzunluk ve Ã¶nem)
            scored_phrases = []
            for phrase in key_phrases:
                score = len(phrase.split())  # Kelime sayÄ±sÄ±
                scored_phrases.append((phrase, score))
            
            # En yÃ¼ksek skorlu ifadeleri dÃ¶ndÃ¼r
            scored_phrases.sort(key=lambda x: x[1], reverse=True)
            return [phrase for phrase, _ in scored_phrases[:top_n]]
        
        except Exception as e:
            print(f"âŒ Key phrase extraction hatasÄ±: {e}")
            return []


# ==========================================================
# BÃ–LÃœM 3: FAISS BÄ°LGÄ° TABANI (DÃ–KÃœMAN KÃœTÃœPHANESÄ°) - NEO4J SÄ°LÄ°NDÄ°
# ==========================================================
# Neo4j GraphRAG kodu tamamen kaldÄ±rÄ±ldÄ± (Ã¶lÃ¼ kod idi)
# HafizaAsistani zaten uzun dÃ¶nem hafÄ±za yÃ¶netimini yapÄ±yor


class FAISSKnowledgeBase:
    """
    FAISS tabanlÄ± yerel bilgi tabanÄ±
    Risale-i Nur, dÃ¶kÃ¼manlar, PDF'ler vb. iÃ§in
    """
    
    def __init__(self, user_id: str = SystemConfig.DEFAULT_USER_ID):
        self.user_id = user_id
        self.enabled = SystemConfig.FAISS_KB_ENABLED
        
        print(f"\nğŸ” FAISS KB INIT DEBUG:")
        print(f"   Enabled: {self.enabled}")
        print(f"   Index file: {SystemConfig.FAISS_INDEX_FILE}")
        print(f"   Texts file: {SystemConfig.FAISS_TEXTS_FILE}")
        print(f"   Index exists: {os.path.exists(SystemConfig.FAISS_INDEX_FILE)}")
        print(f"   Texts exists: {os.path.exists(SystemConfig.FAISS_TEXTS_FILE)}\n")
        
        if not self.enabled:
            print("âš ï¸ FAISS Bilgi TabanÄ± devre dÄ±ÅŸÄ±")
            return
        
        # Paths
        self.index_file = SystemConfig.FAISS_INDEX_FILE
        self.texts_file = SystemConfig.FAISS_TEXTS_FILE
        
        # Config
        self.search_top_k = SystemConfig.FAISS_SEARCH_TOP_K
        self.similarity_threshold = SystemConfig.FAISS_SIMILARITY_THRESHOLD
        self.max_results = SystemConfig.FAISS_MAX_RESULTS
        self.relative_threshold = SystemConfig.FAISS_RELATIVE_THRESHOLD
        self.max_context_length = SystemConfig.FAISS_MAX_CONTEXT_LENGTH
        
        # User namespace
        self.user_namespace = f"user_{user_id}"
        
        # Temporal awareness
        self.temporal_awareness = True
        self._initialize_temporal_awareness()
        
        # Data
        self.texts = []
        self.index: Optional[faiss.Index] = None
        
        # Load
        self._load_components()
    
    def _initialize_temporal_awareness(self):
        """Tarih bilincini baÅŸlat"""
        try:
            now = _now_ist()
            weekday = now.weekday()
            
            english_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 
                             'Friday', 'Saturday', 'Sunday']
            turkish_days = ['Pazartesi', 'SalÄ±', 'Ã‡arÅŸamba', 'PerÅŸembe', 
                             'Cuma', 'Cumartesi', 'Pazar']
            turkish_months = ['Ocak', 'Åubat', 'Mart', 'Nisan', 'MayÄ±s', 'Haziran',
                              'Temmuz', 'AÄŸustos', 'EylÃ¼l', 'Ekim', 'KasÄ±m', 'AralÄ±k']
            
            self.current_day_info = {
                'date': now.strftime('%Y-%m-%d'),
                'day_english': english_days[weekday],
                'day_turkish': turkish_days[weekday],
                'month_turkish': turkish_months[now.month - 1],
                'formatted_date': now.strftime(f'%d {turkish_months[now.month - 1]} %Y')
            }
        except Exception as e:
            self.temporal_awareness = False
            self.current_day_info = {}
    
    def _load_components(self):
        """Index ve text dosyalarÄ±nÄ± yÃ¼kle"""
        try:
            # FAISS index
            if os.path.exists(self.index_file):
                self.index = faiss.read_index(self.index_file)
                print(f"âœ… FAISS index yÃ¼klendi: {self.index_file}")
            else:
                print(f"âš ï¸ FAISS index bulunamadÄ±: {self.index_file}")
                self.enabled = False
                return
            
            # Texts JSON
            if os.path.exists(self.texts_file):
                with open(self.texts_file, 'r', encoding='utf-8') as f:
                    self.texts = json.load(f)
                print(f"âœ… FAISS texts yÃ¼klendi: {len(self.texts)} dÃ¶kÃ¼man")
            else:
                print(f"âš ï¸ FAISS texts bulunamadÄ±: {self.texts_file}")
                self.enabled = False
                return
            
            # Embedding model
            self.embedding_model = SentenceTransformer(SystemConfig.EMBEDDING_MODEL)
            
            print(f"âœ… FAISS Bilgi TabanÄ± hazÄ±r: {self.user_namespace}")
            
        except Exception as e:
            print(f"âŒ FAISS yÃ¼kleme hatasÄ±: {e}")
            self.enabled = False
    
    def get_relevant_context(self, user_input: str, max_chunks: int = 3) -> str:
        """KullanÄ±cÄ± input'una gÃ¶re ilgili baÄŸlamÄ± getir"""
        if not self.enabled:
            print("âš ï¸ FAISS KB devre dÄ±ÅŸÄ±")
            return ""
        
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ” FAISS KB ARAMA BAÅLADI")
            print(f"ğŸ“ Sorgu: {user_input}")
            print(f"ğŸ“Š Max chunks: {max_chunks}")
            print(f"{'='*60}")
            
            # Search
            results = self.search(user_input, top_k=max_chunks * 2)
            
            print(f"\nğŸ“Š ARAMA SONUÃ‡LARI:")
            print(f"   Toplam sonuÃ§: {len(results)}")
            
            if not results:
                print("   âŒ HiÃ§ sonuÃ§ bulunamadÄ±!")
                return ""
            
            combined_text = ""
            
            # Tarih bilgisi ekle
            if self.temporal_awareness and self.current_day_info:
                day_info = self.current_day_info
                combined_text += f"""GÃœNCEL TARÄ°H BÄ°LGÄ°SÄ° - DÄ°KKAT:
BugÃ¼nÃ¼n tam tarihi: {day_info.get('formatted_date', 'Bilinmiyor')}
UYARI: Bu bilgi gÃ¼ncel ve doÄŸrudur, lÃ¼tfen bu bilgiyi kullan!

"""
            
            # Ä°lgili bilgiler ekle
            if results:
                combined_text += "Ä°LGÄ°LÄ° BÄ°LGÄ°LER:\n"
                
                for i, result in enumerate(results[:max_chunks]):
                    text = result.get('text', '')
                    score = result.get('score', 0.0)
                    index = result.get('index', -1)
                    
                    # ğŸ†• DEBUG: Her sonucu detaylÄ± yazdÄ±r
                    print(f"\n   ğŸ“„ SONUÃ‡ #{i+1}:")
                    print(f"      â€¢ Skor: {score:.4f}")
                    print(f"      â€¢ Index: {index}")
                    print(f"      â€¢ Metin uzunluÄŸu: {len(text)} karakter")
                    print(f"      â€¢ Ä°lk 100 karakter:")
                    print(f"        '{text[:100]}...'")
                    
                    if text:
                        combined_text += f"{text}\n\n"
            
            print(f"\n{'='*60}")
            print(f"âœ… FAISS KB ARAMA TAMAMLANDI")
            print(f"ğŸ“Š Toplam dÃ¶nen metin: {len(combined_text)} karakter")
            print(f"{'='*60}\n")
            
            return combined_text.strip()
            
        except Exception as e:
            print(f"âŒ FAISS context hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def search(self, query: str, top_k: Optional[int] = None) -> List[Dict]:
        """Bilgi tabanÄ±nda ara"""
        if not self.enabled:
            print("âš ï¸ FAISS KB search devre dÄ±ÅŸÄ±")
            return []
        
        try:
            print(f"\nğŸ” FAISS SEARCH BAÅLADI")
            print(f"   Query: '{query}'")
            print(f"   Top-K: {top_k or self.search_top_k}")
            
            # Embed query
            query_vector = self.embedding_model.encode(
                [query], 
                normalize_embeddings=True
            )
            query_vector = np.array(query_vector, dtype=np.float32)
            
            print(f"   âœ… Query embedding boyutu: {query_vector.shape}")
            
            # Search
            requested_k = top_k or self.search_top_k
            k = max(requested_k, requested_k + 10)
            
            print(f"   ğŸ” FAISS index'te arama yapÄ±lÄ±yor (k={k})...")
            scores, indices = self.index.search(query_vector, k)
            
            print(f"   âœ… FAISS arama tamamlandÄ±")
            print(f"   ğŸ“Š Bulunan index sayÄ±sÄ±: {len(indices[0])}")
            
            # Filter results
            results = []
            filtered_count = 0
            
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx == -1:
                    continue
                
                similarity = float(score)
                
                # ğŸ†• DEBUG: Her sonucu yazdÄ±r
                print(f"\n   #{i+1} - Index: {idx}, Skor: {similarity:.4f}", end="")
                
                if similarity >= self.similarity_threshold and idx < len(self.texts):
                    text_data = self.texts[idx]
                    
                    # Text content
                    if isinstance(text_data, dict):
                        text_content = text_data.get('text', str(text_data))
                    else:
                        text_content = str(text_data)
                    
                    print(f" âœ… KABUL EDÄ°LDÄ° (threshold: {self.similarity_threshold})")
                    print(f"      Metin: '{text_content[:80]}...'")
                    
                    result = {
                        'text': text_content,
                        'score': similarity,
                        'index': int(idx),
                        'source': f'faiss_knowledge_{self.user_namespace}'
                    }
                    
                    results.append(result)
                else:
                    filtered_count += 1
                    print(f" âŒ FÄ°LTRELENDÄ° (threshold altÄ± veya invalid)")
            
            print(f"\n   ğŸ“Š Ã–ZET:")
            print(f"      â€¢ Toplam tarama: {len(indices[0])}")
            print(f"      â€¢ Filtrelenen: {filtered_count}")
            print(f"      â€¢ Kabul edilen: {len(results)}")

            # ğŸ†• RELATIVE SCORING: En yÃ¼ksek skorun %90'Ä± altÄ±ndakileri Ã§Ä±kar
            if results:
                top_score = results[0]['score']
                relative_threshold = top_score * SystemConfig.FAISS_RELATIVE_THRESHOLD

                print(f"\n   ğŸ¯ RELATIVE SCORING:")
                print(f"      â€¢ En yÃ¼ksek skor: {top_score:.4f}")
                print(f"      â€¢ Relative threshold ({SystemConfig.FAISS_RELATIVE_THRESHOLD*100}%): {relative_threshold:.4f}")

                # Relative threshold'dan yÃ¼ksek olanlarÄ± al
                filtered_results = []
                for r in results:
                    if r['score'] >= relative_threshold:
                        filtered_results.append(r)
                        print(f"      âœ… Skor {r['score']:.4f} - KABUL")
                    else:
                        print(f"      âŒ Skor {r['score']:.4f} - REDDEDÄ°LDÄ° (relative threshold altÄ±)")

                # Max sonuÃ§ sayÄ±sÄ± limiti uygula
                max_results = SystemConfig.FAISS_MAX_RESULTS
                if len(filtered_results) > max_results:
                    print(f"      âœ‚ï¸ Ä°lk {max_results} sonuÃ§ alÄ±nÄ±yor (toplam {len(filtered_results)} sonuÃ§ vardÄ±)")
                    filtered_results = filtered_results[:max_results]

                print(f"      â€¢ Final sonuÃ§ sayÄ±sÄ±: {len(filtered_results)}")

                return filtered_results

            return results
            
        except Exception as e:
            print(f"âŒ FAISS search hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """Ä°statistikleri dÃ¶ndÃ¼r"""
        if not self.enabled:
            return {
                "enabled": False,
                "status": "disabled"
            }
        
        try:
            return {
                "enabled": True,
                "status": "active",
                "total_vectors": self.index.ntotal if self.index else 0,
                "user_namespace": self.user_namespace,
                "total_texts": len(self.texts),
                "similarity_threshold": self.similarity_threshold,
                "max_results": self.max_results,
                "relative_threshold": self.relative_threshold,
                "features": ["multi_user_isolation", "temporal_awareness", "relative_scoring"]
            }
        except Exception as e:
            print(f"FAISS KB stats hatasÄ±: {e}")
            return {
                "enabled": False,
                "status": "error"
            }

# ==========================================================
# BÃ–LÃœM 4.5: WEB SCRAPING YARDIMCI SINIFLAR
# ==========================================================
# DEÄÄ°ÅÄ°KLÄ°K 1: BÃ–LÃœM 4.5'Ä°N EKLENMESÄ°
class DuplicateFilter:
    """Web scraping iÃ§in duplicate iÃ§erik filtresi"""
        
    def __init__(self):
        self.seen_hashes = set()
        self.max_size = 10000

    def is_duplicate(self, content: str) -> bool:
        if not content:
            return True
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
        if content_hash in self.seen_hashes:
            return True
        self.seen_hashes.add(content_hash)
        if len(self.seen_hashes) > self.max_size:
            to_remove = list(self.seen_hashes)[:self.max_size // 10]
            for h in to_remove:
                self.seen_hashes.discard(h)
        return False

    def clear(self):
        self.seen_hashes.clear()

class SmartContentExtractor:
    """BeautifulSoup ile akÄ±llÄ± iÃ§erik Ã§Ä±karma"""
        
    def extract_main_content(self, soup: BeautifulSoup, query: str) -> Tuple[str, float]:
        if not soup:
            return "", 0.0
        for unwanted in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            unwanted.decompose()
        main_content = (
            soup.find('article') or 
            soup.find('main') or 
            soup.find('div', class_=lambda x: x and 'content' in x.lower()) or
            soup.find('body')
        )
        if not main_content:
            return "", 0.0
        text = main_content.get_text(separator='\n', strip=True)
        text = re.sub(r'\n\s*\n+', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        quality_score = self._calculate_quality(text, query)
        return text[:2000], quality_score

    def _calculate_quality(self, text: str, query: str) -> float:
        if not text:
            return 0.0
        score = 0.5
        if len(text) > 500:
            score += 0.2
        query_words = query.lower().split()
        text_lower = text.lower()
        match_count = sum(1 for word in query_words if word in text_lower)
        if query_words:
            score += (match_count / len(query_words)) * 0.3
        return min(1.0, score)

class ScrapingError(PersonalAIError):
    """Web scraping iÃ§in Ã¶zel exception"""
        
    def __init__(self, message: str, url: str = None):
        super().__init__(message)
        self.url = url

    def __str__(self):
        if self.url:
            return f"{self.args[0]} (URL: {self.url})"
        return str(self.args[0])

# ==========================================================
# BÃ–LÃœM 5: LLM, WEB SEARCH VE NÄ°YET TESPÄ°TÄ°
# ==========================================================

class LocalLLM:
    """
    LLM wrapper - Ollama veya Together.ai desteÄŸi
    Vision desteÄŸi ile
    """

    def __init__(self, user_id: str = SystemConfig.DEFAULT_USER_ID):
        self.user_id = user_id
        self.provider = SystemConfig.LLM_PROVIDER  # "ollama" veya "together"
        self.ollama_url = SystemConfig.OLLAMA_URL
        self.model_name = SystemConfig.MODEL_NAME
        self.vision_enabled = SystemConfig.ENABLE_VISION
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # Together.ai API key
        self.together_api_key = os.getenv("TOGETHER_API_KEY", "")

        self.stats = {
            'total_requests': 0,
            'vision_requests': 0,
            'text_requests': 0,
            'errors': 0,
            'avg_response_time': 0.0
        }

        provider_name = "Together.ai" if self.provider == "together" else "Ollama"
        print(f"âœ… LLM baÅŸlatÄ±ldÄ±: {self.model_name} ({provider_name}, {self.device})")
    
    def _is_vision_query(self, user_input: str) -> bool:
        """Vision query mi kontrol et"""
        if not self.vision_enabled:
            return False
        
        input_lower = user_input.lower()
        return any(keyword in input_lower for keyword in SystemConfig.VISION_KEYWORDS)
    
    async def generate(self, prompt: str, image_data: Optional[bytes] = None) -> str:
        """
        LLM yanÄ±t Ã¼ret
        
        NOT: Bu gerÃ§ek Ollama API Ã§aÄŸrÄ±sÄ± yapmalÄ±
        Åu an basit simÃ¼lasyon (Ollama kurulumunu gerektirir)
        """
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            # Vision query - eÄŸer image_data varsa direkt vision kullan
            if image_data:
                result = await self._generate_with_vision(prompt, image_data)
                self.stats["vision_requests"] += 1
            else:
                result = await self._generate_text_only(prompt)
                self.stats["text_requests"] += 1
            
            # Update stats
            response_time = time.time() - start_time
            if self.stats["total_requests"] > 0:
                self.stats["avg_response_time"] = (
                    self.stats["avg_response_time"] * (self.stats["total_requests"] - 1) + 
                    response_time
                ) / self.stats["total_requests"]
            
            return result
            
        except Exception as e:
            self.stats["errors"] += 1
            print(f"âŒ LLM hatasÄ±: {e}")
            return "ÃœzgÃ¼nÃ¼m, yanÄ±t oluÅŸturulurken bir hata oluÅŸtu."
    
    async def _generate_with_vision(self, prompt: str, image_data: str) -> str:
        """
        Vision ile yanÄ±t Ã¼ret (Ollama Vision API)
        image_data: base64 encoded image string
        """
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "prompt": prompt,
                    "images": [image_data],  # base64 string
                    "stream": False,
                    "raw": True,  # Ollama'nÄ±n kendi template'ini kapatÄ±r, <bos> elle eklendi
                    "options": SystemConfig.get_gemma3_params()
                }
                async with session.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        return result.get('response', '')
                    else:
                        print(f"âš ï¸ Ollama Vision API hatasÄ±: {resp.status}")
                        return "GÃ¶rseli analiz edemedim, lÃ¼tfen tekrar dene."
        except asyncio.TimeoutError:
            print("âš ï¸ Ollama vision timeout")
            return "GÃ¶rsel analizi zaman aÅŸÄ±mÄ±na uÄŸradÄ±."
        except Exception as e:
            print(f"âš ï¸ Vision API hatasÄ±: {e}")
            return "GÃ¶rsel analizi sÄ±rasÄ±nda bir hata oluÅŸtu."
    
    # DEÄÄ°ÅÄ°KLÄ°K 2: _generate_text_only() FONKSÄ°YONUNUN DEÄÄ°ÅTÄ°RÄ°LMESÄ°
    async def _generate_text_only(self, prompt: str) -> str:
        """LLM API Ã§aÄŸrÄ±sÄ± - Ollama veya Together.ai"""
        # ğŸ“‹ PROMPT LOGLAMA - LLM'e giden tam prompt
        if SystemConfig.LOG_FULL_PROMPT:
            print("\n" + "=" * 70)
            print(f"ğŸ“‹ LLM'E GÃ–NDERÄ°LEN TAM PROMPT ({self.provider.upper()}):")
            print("=" * 70)
            print(prompt)
            print("=" * 70)
            print(f"ğŸ“ Toplam: {len(prompt)} karakter")
            print("=" * 70 + "\n")

        # Provider'a gÃ¶re yÃ¶nlendir
        if self.provider == "together":
            return await self._generate_together(prompt)
        else:
            return await self._generate_ollama(prompt)

    async def _generate_together(self, prompt: str) -> str:
        """Together.ai API Ã§aÄŸrÄ±sÄ± (OpenAI uyumlu)"""
        try:
            headers = {
                "Authorization": f"Bearer {self.together_api_key}",
                "Content-Type": "application/json"
            }

            # Prompt'u messages formatÄ±na Ã§evir
            payload = {
                "model": SystemConfig.TOGETHER_MODEL,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": SystemConfig.MAX_TOKENS,
                "temperature": SystemConfig.TEMPERATURE,
                "top_p": SystemConfig.TOP_P,
                "repetition_penalty": SystemConfig.REPEAT_PENALTY,
                "stream": False
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    SystemConfig.TOGETHER_API_URL,
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=180)  # 405B iÃ§in daha uzun timeout
                ) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        # OpenAI format: choices[0].message.content
                        return result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    else:
                        error_text = await resp.text()
                        print(f"âš ï¸ Together.ai API hatasÄ±: {resp.status} - {error_text[:200]}")
                        return self._generate_fallback_response(prompt)

        except asyncio.TimeoutError:
            print("âš ï¸ Together.ai timeout")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            print(f"âš ï¸ Together.ai baÄŸlantÄ± hatasÄ±: {e}")
            return self._generate_fallback_response(prompt)

    async def _generate_ollama(self, prompt: str) -> str:
        """Ollama API Ã§aÄŸrÄ±sÄ±"""
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "raw": True,  # Ollama'nÄ±n kendi template'ini kapatÄ±r
                    "options": SystemConfig.get_gemma3_params()
                }
                async with session.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        return result.get('response', '')
                    else:
                        print(f"âš ï¸ Ollama API hatasÄ±: {resp.status}")
                        return self._generate_fallback_response(prompt)
        except asyncio.TimeoutError:
            print("âš ï¸ Ollama timeout - simÃ¼lasyona geÃ§iliyor")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            print(f"âš ï¸ Ollama baÄŸlantÄ± hatasÄ±: {e} - simÃ¼lasyona geÃ§iliyor")
            return self._generate_fallback_response(prompt)

    # DEÄÄ°ÅÄ°KLÄ°K 3: _generate_fallback_response() FONKSÄ°YONUNUN EKLENMESÄ°
    def _generate_fallback_response(self, prompt: str) -> str:
        """Ollama Ã§alÄ±ÅŸmazsa fallback simÃ¼lasyon"""
        if "Duygusal GiriÅŸ/GÃ¶zlem" in prompt:
            if "proje" in prompt.lower():
                return "Åahsen, bu yapay zeka projenin ne kadar ilerlediÄŸini gÃ¶rmek beni Ã§ok heyecanlandÄ±rÄ±yor. GeÃ§en sefer konuÅŸtuÄŸumuzda Neo4j entegrasyonundan bahsetmiÅŸtin. Bence bu yaklaÅŸÄ±mla gerÃ§ekten gÃ¼Ã§lÃ¼ bir sistem kuruyorsun."
            if "hava" in prompt.lower():
                return "AklÄ±ma gelmiÅŸken, dÄ±ÅŸarÄ± Ã§Ä±kmadan Ã¶nce hava durumunu sorman Ã§ok mantÄ±klÄ±. Sakarya iÃ§in bugÃ¼n 15Â°C civarÄ±, parÃ§alÄ± bulutlu gÃ¶rÃ¼nÃ¼yor. Hafif bir ceket iÅŸini gÃ¶rÃ¼r."
            return "Åahsen, bu konunun ne kadar Ã¶nemli olduÄŸunu anlayabiliyorum. Deneyimlerime gÃ¶re, bu tÃ¼r durumlarda adÄ±m adÄ±m ilerlemenin en saÄŸlÄ±klÄ±sÄ± olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼yorum."
        if "GraphRAG" in prompt or "ARKA PLAN BÄ°LGÄ°SÄ°" in prompt:
            return "GeÃ§miÅŸte birlikte konuÅŸtuklarÄ±mÄ±zÄ± dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼mde, senin bu konuya olan ilgin ve yaklaÅŸÄ±mÄ±n gerÃ§ekten etkileyici. HatÄ±rlÄ±yorum, benzer bir durumda ÅŸÃ¶yle bahsetmiÅŸtin..."
        if "REASONING APPROACH" in prompt:
            return "MantÄ±klÄ± bir Ã§Ã¶zÃ¼m iÃ§in Ã¶nce durumu analiz ettim. FarklÄ± perspektifleri deÄŸerlendirdim ve en pratik yaklaÅŸÄ±mÄ±n ÅŸu olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼yorum..."
        return "Merhaba! Senin iÃ§in buradayÄ±m. NasÄ±l yardÄ±mcÄ± olabilirim?"
    
    async def generate_with_params(self, prompt: str, params: Dict[str, Any], 
                                     image_data: Optional[bytes] = None) -> str:
        """Ã–zel parametrelerle yanÄ±t Ã¼ret"""
        # Params'Ä± kullan (TODO: gerÃ§ek API'ye gÃ¶nder)
        return await self.generate(prompt, image_data)
    
    def get_stats(self) -> Dict[str, Any]:
        """Ä°statistikleri dÃ¶ndÃ¼r"""
        return {
            **self.stats,
            'model': self.model_name,
            'device': self.device,
            'vision_enabled': self.vision_enabled
        }


class PromptBuilder:
    """
    Ana prompt oluÅŸturucu
    ğŸ§  Ã‡ekirdek benlik + minimal SYNTHESIS_PROMPT kullanÄ±yor
    """

    def create_prompt(self, user_input: str,
                                        graphrag_context: str,
                                        semantic_context: str,
                                        chat_history: str) -> str:
        """Ã‡ekirdek benlik + minimal prompt oluÅŸtur"""
        combined_context = f"{graphrag_context}\n{semantic_context}"

        # ğŸ§  Ã‡ekirdek benlik + minimal prompt
        # _INTERNAL_SELF_AWARENESS sessiz arka plan olarak ekleniyor
        return SystemConfig._INTERNAL_SELF_AWARENESS + "\n" + SystemConfig.format_prompt(
            SystemConfig.SYNTHESIS_PROMPT,
            user_input=user_input,
            combined_sources=combined_context + "\n" + chat_history if chat_history else combined_context
        )


class Gemma3OptimizedLLM:
    """
    Gemma3 iÃ§in optimize edilmiÅŸ LLM wrapper
    CoT ve Ã¶zel parametre desteÄŸi
    """
    
    def __init__(self, base_llm: LocalLLM):
        self.base_llm = base_llm
        self.gemma3_params = SystemConfig.get_gemma3_params()
        self.prompt_builder = PromptBuilder()

    async def generate_response(self, user_input: str,
                                 graphrag_context: str,
                                 semantic_context: str,
                                 chat_history: str) -> str:
        """Ana yanÄ±t Ã¼ret"""
        prompt = self.prompt_builder.create_prompt(
            user_input, graphrag_context, semantic_context, chat_history
        )
        
        response = await self._generate_with_gemma3_params(prompt)
        return response
    
    async def _generate_with_gemma3_params(self, prompt: str, 
                                           image_data: Any = None) -> str:
        """Gemma3 parametreleri ile Ã¼ret"""
        if hasattr(self.base_llm, 'generate_with_params'):
            return await self.base_llm.generate_with_params(
                prompt, 
                params=self.gemma3_params, 
                image_data=image_data
            )
        else:
            return await self.base_llm.generate(prompt, image_data=image_data)


# IntentDetector silindi - HafizaAsistani._tool_secimi_yap zaten bu iÅŸi yapÄ±yor
# EnhancedWebSearch silindi - Web search kullanÄ±lmÄ±yor, sadece Wikipedia API kalÄ±yor

# ==========================================================
# BÃ–LÃœM 6: MULTI-ROLE SÄ°STEMÄ° VE RESPONSE FORMATTER
# ==========================================================

class MultiRoleSystem:
    """
    Ã‡oklu rol sistemi
    ArkadaÅŸ, teknik destek, Ã¶ÄŸretmen rolleri
    """
    
    def __init__(self):
        self.enabled = SystemConfig.MULTI_ROLE_ENABLED
        self.roles = SystemConfig.ROLES
        self.role_history = defaultdict(int)
        self.last_role = "friend"
    
    def detect_role(self, user_input: str, detected_intent: Optional[str] = None) -> str:
        """KullanÄ±cÄ± input'undan rol tespit et"""
        if not self.enabled:
            return "friend"
        
        user_lower = user_input.lower()
        
        # Kod marker'larÄ±
        code_markers = ["```", "def ", "class ", "import ", "function"]
        has_code = any(marker in user_input for marker in code_markers)
        
        error_markers = ["error:", "hata veriyor", "bug", "error", "Ã§alÄ±ÅŸmÄ±yor"]
        has_error = any(marker in user_lower for marker in error_markers)
        
        if has_code or has_error or detected_intent in ["TECHNICAL", "CODE_DEBUG"]:
            self._track_role("technical_helper")
            return "technical_helper"
        
        # Role keyword matching
        for role_name, role_config in self.roles.items():
            keywords = role_config.get("keywords", [])
            if any(kw in user_lower for kw in keywords):
                self._track_role(role_name)
                return role_name
        
        # Default
        self._track_role("friend")
        return "friend"
    
    def _track_role(self, role: str):
        """Rol kullanÄ±mÄ±nÄ± takip et"""
        self.role_history[role] += 1
        self.last_role = role
    
    def format_response_by_role(self, raw_response: str, role: str, 
                                 user_input: str) -> str:
        """Role gÃ¶re yanÄ±tÄ± formatla"""
        if not self.enabled:
            return raw_response
        
        # YasaklÄ± ifadeleri kaldÄ±r
        formatted = self._remove_forbidden_phrases(raw_response)

        # KÄ±sa yanÄ±tlar iÃ§in Ã¶zel iÅŸlem
        if user_input.lower().strip() in ["tamam", "ok", "saol", "teÅŸekkÃ¼rler", "teÅŸekkÃ¼r ederim", "tÅŸk"]:
            short_responses = ["Rica ederim!", "Ne demek!", "Her zaman!", "Ã–nemli deÄŸil!"]
            return short_responses[hash(user_input) % len(short_responses)]

        # âŒ Max length kesme KALDIRILDI - LLM kendi doÄŸal uzunluÄŸunda yazsÄ±n

        return formatted.strip()
    
    def _remove_forbidden_phrases(self, text: str) -> str:
        """YasaklÄ± ifadeleri kaldÄ±r"""
        for phrase in SystemConfig.FORBIDDEN_PHRASES:
            # CÃ¼mle bazÄ±nda kaldÄ±r
            pattern = r'[^.!?]*' + re.escape(phrase) + r'[^.!?]*[.!?]'
            text = re.sub(pattern, ' ', text, flags=re.IGNORECASE).strip()
        
        return text.strip()
    
    def get_role_stats(self) -> Dict[str, Any]:
        """Rol istatistiklerini dÃ¶ndÃ¼r"""
        return {
            'role_history': dict(self.role_history),
            'last_role': self.last_role,
            'enabled': self.enabled
        }


class ResponseFormatter:
    """
    YanÄ±t formatlarÄ± ve temizleme
    """
    
    @staticmethod
    def clean_response(text: str) -> str:
        """YanÄ±tÄ± temizle"""
        # ğŸ†• BELÄ°RSÄ°ZLÄ°K Ä°FADELERÄ°NÄ° TEMÄ°ZLE
        uncertain_phrases = [
            "web'de bu bilgi geÃ§iyor ama emin deÄŸilim:",
            "web'de bu bilgi geÃ§iyor ama emin deÄŸilim",
            "web'de geÃ§iyor ama emin deÄŸilim:",
            "web'de geÃ§iyor ama emin deÄŸilim",
            "emin deÄŸilim:",
            "emin deÄŸilim",
            "sanÄ±rÄ±m ki",
            "sanÄ±rÄ±m",
            "galiba",
            "olabilir ki",
            "muhtemelen",
            "belki de"
        ]
        
        cleaned = text
        for phrase in uncertain_phrases:
            # Hem kÃ¼Ã§Ã¼k hem bÃ¼yÃ¼k harfle baÅŸlayabilir
            cleaned = cleaned.replace(phrase, "")
            cleaned = cleaned.replace(phrase.capitalize(), "")
        
        # Fazla boÅŸluklarÄ± temizle
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # Fazla newline'larÄ± temizle
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)
        
        # Ã‡ift noktalama temizle
        cleaned = re.sub(r'\.\s*\.', '.', cleaned)
        cleaned = re.sub(r',\s*,', ',', cleaned)
        cleaned = re.sub(r':\s*:', ':', cleaned)
        
        return cleaned.strip()
    
    @staticmethod
    def remove_greetings_if_continuing(text: str, is_continuing: bool) -> str:
        """Devam eden sohbette selamlarÄ± kaldÄ±r"""
        if not is_continuing:
            return text
        
        greeting_patterns = [
            r'^(Merhaba|Selam|Ä°yi gÃ¼nler|HoÅŸ geldiniz)[,!.]?\s*',
            r'^(Hello|Hi|Hey)[,!.]?\s*',
            r'^(I understand)\.\s*'
        ]
        
        for pattern in greeting_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE).strip()
        
        return text
    
    @staticmethod
    def format_synthesis_response(response: str, user_input: str,
                                      max_length: int = None) -> str:
        """Synthesis yanÄ±tÄ±nÄ± formatla"""
        # Temizle
        cleaned = ResponseFormatter.clean_response(response)

        # âŒ Max length kesme KALDIRILDI - LLM kendi doÄŸal uzunluÄŸunda yazsÄ±n

        return cleaned

# ==========================================================
# BÃ–LÃœM 7: AYARLAR VE TOOL SYSTEM
# ==========================================================

class ConfigDrivenSettings:
    """
    KullanÄ±cÄ± bazlÄ± ayarlar ve kurallar
    """
    
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.memory_triggers = SystemConfig.MEMORY_TRIGGERS
        self.personal_keywords = SystemConfig.PERSONAL_KEYWORDS
        self.greeting_keywords = SystemConfig.GREETING_KEYWORDS
    
    def get_context_blocking_rules(self, user_input: str) -> dict:
        """Context blocking kurallarÄ±"""
        return {
            'block_graphrag': False,
            'block_faiss': False,
            'category': 'general'
        }


# ==========================================================
# BÃ–LÃœM 7.5: TOOL SYSTEM (YENÄ°!)
# ==========================================================

class ToolSystem:
    """
    LLM'nin kullanabileceÄŸi araÃ§larÄ± yÃ¶neten sistem
    """
    
    # AraÃ§ tanÄ±mlarÄ±
    TOOLS = {
        "risale_ara": {
            "name": "risale_ara",
            "description": "Risale-i Nur kÃ¼tÃ¼phanesinden dini sorulara cevap bul",
            "parameters": "soru: Aranacak dini soru",
            "when": "KullanÄ±cÄ± Allah, din, iman, peygamber, namaz gibi DÄ°NÄ° konularda soru sorduÄŸunda",
            "examples": ["Allah'Ä±n ilim sÄ±fatÄ± nedir?", "Ä°man nedir?", "Namaz neden Ã¶nemli?"]
        },
        "gecmis_getir": {
            "name": "gecmis_getir",
            "description": "Neo4j'den Ã¶nceki konuÅŸmalarÄ± getir",
            "parameters": "konu: Aranacak konu",
            "when": "KullanÄ±cÄ± 'geÃ§en', 'daha Ã¶nce', 'konuÅŸmuÅŸtuk' dediÄŸinde",
            "examples": ["GeÃ§en konuÅŸtuÄŸumuz proje?", "Daha Ã¶nce ne sÃ¶ylemiÅŸtim?"]
        },
        "zaman_getir": {
            "name": "zaman_getir",
            "description": "Åu anki tarih ve saati Ã¶ÄŸren",
            "parameters": "yok",
            "when": "KullanÄ±cÄ± saat, tarih, gÃ¼n sorduÄŸunda",
            "examples": ["Saat kaÃ§?", "BugÃ¼n tarihi ne?", "Hangi gÃ¼ndeyiz?"]
        },
        "hesapla": {
            "name": "hesapla",
            "description": "Matematiksel hesaplama yap",
            "parameters": "ifade: Matematiksel ifade",
            "when": "KullanÄ±cÄ± matematik sorusu sorduÄŸunda veya hesaplama istediÄŸinde",
            "examples": ["2 + 2 kaÃ§?", "15 Ã§arpÄ± 3?", "100 bÃ¶lÃ¼ 5 kaÃ§ eder?"]
        },
        "hava_durumu": {
            "name": "hava_durumu",
            "description": "Åehir iÃ§in hava durumu Ã¶ÄŸren",
            "parameters": "ÅŸehir: Åehir adÄ±",
            "when": "KullanÄ±cÄ± hava durumu sorduÄŸunda",
            "examples": ["Sakarya hava durumu?", "Ä°stanbul'da hava nasÄ±l?", "Ankara'da yaÄŸmur var mÄ±?"]
        },
        "namaz_vakti": {
            "name": "namaz_vakti",
            "description": "TÃ¼rkiye ÅŸehirleri iÃ§in namaz vakitlerini Ã¶ÄŸren (Diyanet metodu)",
            "parameters": "ÅŸehir: Åehir adÄ±, vakÄ±t: Belirli vakÄ±t (opsiyonel)",
            "when": "KullanÄ±cÄ± namaz vakitleri, ezan saatleri sorduÄŸunda",
            "examples": ["Sakarya namaz vakitleri?", "Ä°stanbul Ã¶ÄŸle namazÄ± kaÃ§ta?", "Ankara akÅŸam ezanÄ±?", "Bursa imsak vakti?"]
        },
        "wiki_ara": {
            "name": "wiki_ara",
            "description": "Wikipedia'da Ã¼nlÃ¼ kiÅŸi, yer, olay hakkÄ±nda bilgi ara",
            "parameters": "arama_terimi: Aranacak kiÅŸi/yer/olay adÄ± (netleÅŸtirilmiÅŸ)",
            "when": "KullanÄ±cÄ± Ã¼nlÃ¼ kiÅŸi (ÅŸarkÄ±cÄ±, oyuncu, sporcu), tarihÃ® olay veya yer hakkÄ±nda soru sorduÄŸunda",
            "examples": ["Ã–zdemir ErdoÄŸan ÅŸarkÄ±cÄ±", "AtatÃ¼rk", "Ä°stanbul tarihi"]
        },
        "yok": {
            "name": "yok",
            "description": "AraÃ§ kullanmadan direkt cevap ver",
            "parameters": "yok",
            "when": "SelamlaÅŸma, genel sohbet, basit sorular",
            "examples": ["Merhaba", "NasÄ±lsÄ±n?", "TeÅŸekkÃ¼rler"]
        }
    }
    
    @staticmethod
    def get_tools_prompt() -> str:
        """AraÃ§larÄ± LLM'ye tanÄ±t"""
        tools_text = "KULLANDIÄIN ARAÃ‡LAR:\n\n"
        
        for tool_name, info in ToolSystem.TOOLS.items():
            tools_text += f"{tool_name}({info['parameters']})\n"
            tools_text += f"  â€¢ Ne iÅŸe yarar: {info['description']}\n"
            tools_text += f"  â€¢ Ne zaman kullan: {info['when']}\n"
            tools_text += f"  â€¢ Ã–rnek: {info['examples'][0] if info['examples'] else 'N/A'}\n\n"
        
        return tools_text
    
    @staticmethod
    def get_tool_calling_prompt(user_input: str) -> str:
        """Tool calling prompt'u oluÅŸtur"""
        return f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{ToolSystem.get_tools_prompt()}

KULLANICI SORUSU: {user_input}

Ã–NEMLÄ° KURALLAR:
1. Ã–nce soruyu DÄ°KKATLE ANLA
2. Hangi araÃ§ gerekli? KARAR VER
3. CevabÄ±nÄ± TAM OLARAK ÅŸu formatta ver:

DÃœÅÃœNCE: [Soruyu nasÄ±l analiz ettin]
ARAÃ‡: [risale_ara / gecmis_getir / zaman_getir / hesapla / hava_durumu / yok]
PARAMETRE: [araÃ§ parametresi veya "yok"]

Ã–RNEK 1:
DÃœÅÃœNCE: "Allah'Ä±n ilim sÄ±fatÄ±" dini bir soru
ARAÃ‡: risale_ara
PARAMETRE: Allah'Ä±n ilim sÄ±fatÄ±

Ã–RNEK 2:
DÃœÅÃœNCE: "GeÃ§en konuÅŸmuÅŸtuk" geÃ§miÅŸe atÄ±f yapÄ±yor
ARAÃ‡: gecmis_getir
PARAMETRE: geÃ§en konuÅŸma

Ã–RNEK 3:
DÃœÅÃœNCE: Basit selamlaÅŸma, araÃ§ gerekmez
ARAÃ‡: yok
PARAMETRE: yok

Åimdi analiz et:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    
    @staticmethod
    def parse_tool_decision(llm_response: str) -> Tuple[str, str]:
        """LLM'nin kararÄ±nÄ± parse et"""
        tool_name = "yok"
        tool_param = ""
        
        # ARAÃ‡: satÄ±rÄ±nÄ± bul
        for line in llm_response.split('\n'):
            line = line.strip()
            if line.startswith("ARAÃ‡:"):
                tool_name = line.replace("ARAÃ‡:", "").strip()
            elif line.startswith("PARAMETRE:"):
                tool_param = line.replace("PARAMETRE:", "").strip()
        
        # Temizle
        tool_name = tool_name.lower()
        if tool_name not in ToolSystem.TOOLS:
            tool_name = "yok"
        
        if tool_param.lower() == "yok":
            tool_param = ""
        
        return tool_name, tool_param


# ==========================================================
# BÃ–LÃœM 8: ANA PERSONALAÄ° SINIFI
# ==========================================================

class PersonalAI:
    """
    Ana PersonalAI sÄ±nÄ±fÄ± - Tool System ile gÃ¼ncellenmiÅŸ
    """
    
    def __init__(self, user_id: str = None):
        """PersonalAI sistemini baÅŸlat"""
        # User ID
        self.user_id = user_id or SystemConfig.DEFAULT_USER_ID
        self.start_time = time.time()
        
        # Background tasks
        self._bg_tasks: Set[asyncio.Task] = set()
        
        # User data dizini
        self.user_data_dir = f"{SystemConfig.USER_DATA_BASE_DIR}/{self.user_id}"
        self._create_user_directories()
        
        print("=" * 60)
        print(f"ğŸš€ PersonalAI BaÅŸlatÄ±lÄ±yor...")
        print(f"ğŸ‘¤ KullanÄ±cÄ±: {self.user_id}")
        print("=" * 60)
        
        # BileÅŸenleri baÅŸlat
        self._initialize_components()
        
        # Settings
        self.settings = ConfigDrivenSettings(self.user_id)
        
        # Tool System
        self.tool_system = ToolSystem()
        
        # Learning system
        self.learning_system: Dict[str, Any] = {
            "topic_interests": defaultdict(int),
            "preferred_tone": "friendly",
            "response_satisfaction": deque(maxlen=2000),
            "interaction_count": 0
        }
        
        # Performance metrics
        self.performance_metrics: Dict[str, deque] = {
            'processing_time': deque(maxlen=5000),
            'errors': deque(maxlen=1000)
        }
        
        # User profile
        self.user_profile = self._build_user_profile()
        
        # Gemma3 optimization
        self._integrate_gemma3_optimization()
        
        # Multi-role system
        self.multi_role = MultiRoleSystem()

        # Current mode
        self.current_mode = "simple"

        print("\nâœ… PersonalAI hazÄ±r!")
        print(f"  â€¢ LLM: {SystemConfig.MODEL_NAME}")
        print(f"  â€¢ ğŸ§  Memory: HafizaAsistani v2.0 + DecisionLLM")
        print(f"  â€¢ ğŸ¤– Phi-3 Mini: {'Aktif âœ…' if hasattr(self.memory, 'use_decision_llm') and self.memory.use_decision_llm else 'KapalÄ±'}")
        print(f"  â€¢ Knowledge Base: {'Aktif' if (self.faiss_kb and self.faiss_kb.enabled) else 'KapalÄ±'}")
        print(f"  â€¢ Wikipedia Tool: Aktif âœ…")
        print(f"  â€¢ Tool System: Aktif âœ…")
        print("=" * 60 + "\n")
    
    def _create_user_directories(self):
        """KullanÄ±cÄ± dizinlerini oluÅŸtur"""
        directories = [
            self.user_data_dir,
            f"{self.user_data_dir}/memories",
            f"{self.user_data_dir}/cache",
            f"{self.user_data_dir}/logs"
        ]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def _initialize_components(self) -> None:
        """TÃ¼m bileÅŸenleri baÅŸlat"""
        # Cache
        self.cache = None
        
        # spaCy NLP Engine
        self.spacy_nlp = TurkishNLPEngine()  # ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e NLP Motoru
        
        # LLM
        self.llm = LocalLLM(self.user_id)
        
        # Memory - YENÄ° HafizaAsistani v2.0! âœ…
        # ğŸ†• Conversation Threading + Multi-turn Awareness
        # Together.ai: DecisionLLM iÃ§in 70B modeli kullanÄ±r
        try:
            self.memory = HafizaAsistani(
                saat_limiti=48,  # 12 â†’ 48 saat (2 gÃ¼n)
                esik=0.50,  # 0.60 â†’ 0.50 (gevÅŸetildi)
                max_mesaj=20,  # 8 â†’ 20 mesaj
                model_adi="BAAI/bge-m3",
                use_decision_llm=True,
                decision_model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"  # HafizaAsistani iÃ§in 70B
            )
        except Exception as e:
            print(f"\n{'='*60}")
            print(f"âŒ HATA: HafizaAsistani baÅŸlatÄ±lamadÄ±!")
            print(f"âŒ Detay: {e}")
            print(f"{'='*60}\n")
            raise  # HatayÄ± yukarÄ± fÄ±rlat
        
        
        # FAISS Knowledge Base
        self.faiss_kb: Optional[FAISSKnowledgeBase] = None
        if SystemConfig.FAISS_KB_ENABLED:
            self.faiss_kb = FAISSKnowledgeBase(self.user_id)

        # ğŸ†• FAISS KB'yi HafizaAsistani'ya inject et
        if self.faiss_kb:
            self.memory.set_faiss_kb(self.faiss_kb)
            print("âœ… FAISS KB HafizaAsistani'ya inject edildi")

        # Neo4j ve Web Search silindi - sadece Wikipedia tool kullanÄ±lÄ±yor

    def _integrate_gemma3_optimization(self):
        """Gemma3 optimizasyonunu entegre et"""
        self.gemma3_llm = Gemma3OptimizedLLM(self.llm)
    
    def _build_user_profile(self) -> Dict[str, Any]:
        """KullanÄ±cÄ± profilini oluÅŸtur"""
        return {
            "name": self.user_id.capitalize(),
            "interests": [],
            "personality": "conversational"
        }
    
    def _spawn_bg(self, coro):
        """Background task baÅŸlat"""
        task = asyncio.create_task(coro)
        self._bg_tasks.add(task)
        task.add_done_callback(self._bg_tasks.discard)
        return task
    
    def _history_summary(self, chat_history: List[Dict[str, Any]], max_len: int = 6000) -> str:
        """
        Chat history'yi Ã¶zetle - BAÄLAM KAYBINI Ã–NLE

        UYUMLU HÄ°YERARÅÄ° (hafiza_asistani.py ile aynÄ±):
        - 10 mesaj (son 5 soru-cevap Ã§ifti)
        - User: 400 karakter, AI: 1000 karakter
        - max_len: 6000

        Bu sayede bot kendi sorduÄŸu soruyu hatÄ±rlar!
        """
        if not chat_history:
            return ""

        recent_messages = chat_history[-10:]  # Son 5 soru-cevap Ã§ifti

        tmp = []
        for m in recent_messages:
            is_user = m.get("role") == "user"
            role = "KULLANICI" if is_user else "AI"
            char_limit = 400 if is_user else 1000  # User: 400, AI: 1000
            text = (m.get('content_en') or m.get('content') or "")[:char_limit]
            if text:
                tmp.append(f"[{role}]: {text}")
        
        s = "\n".join(tmp)
        return (s[:max_len] + "...") if len(s) > max_len else s
    
    def _post_process(self, text: str, user_input: str = "", is_continuing: bool = False) -> str:
        """YanÄ±tÄ± son iÅŸle"""
        # Response code kontrolÃ¼
        if text in [ResponseCodes.API_ERROR, ResponseCodes.SEARCH_FAILED]:
            return "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."
        
        if text == ResponseCodes.NO_DATA:
            return "ÃœzgÃ¼nÃ¼m, bu konuda bilgi bulamadÄ±m."
        
        # Temizle
        cleaned_text = ResponseFormatter.clean_response(text)
        
        # Devam eden sohbette selamlarÄ± kaldÄ±r
        cleaned_text = ResponseFormatter.remove_greetings_if_continuing(cleaned_text, is_continuing)
        
        # Max length
        max_chars = SystemConfig.MAX_RESPONSE_CHARS
        if len(cleaned_text) > max_chars:
            cleaned_text = cleaned_text[:max_chars].rsplit(' ', 1)[0] + "..."
        
        return cleaned_text
    
    def _should_save_interaction(self, user_input: str, ai_response: str) -> bool:
        """Bu etkileÅŸim hafÄ±zaya kaydedilmeli mi?"""
        u = user_input.lower()
        
        # Ã‡ok kÄ±sa
        if len(u) < 3 or u in {"ok", "tamam", "teÅŸekkÃ¼rler"}:
            return False
        
        # PII iÃ§eren
        pii_keywords = ["tc", "iban", "ÅŸifre", "password"]
        if any(k in u for k in pii_keywords):
            return False
        
        # Trivial queries
        trivial = ["saat kaÃ§", "hava durumu", "dÃ¶viz"]
        if any(x in u for x in trivial):
            return False
        
        return True
    
    # ====== YENÄ° YARDIMCI FONKSÄ°YONLAR: SMART RESPONSE ANALYSIS ======
    
    def _build_search_query(self, user_input: str) -> str:
        """
        KullanÄ±cÄ± input'undan arama sorgusu oluÅŸtur
        
        "adapazarÄ±nda Ä±slama kÃ¶fte yemek istiyorum" 
        â†’ "adapazarÄ± Ä±slama kÃ¶fte restaurant"
        """
        # Gereksiz kelimeleri temizle
        noise_words = [
            "yemek", "istiyorum", "isterim", "gitmek", "yapmak",
            "yiyeceÄŸim", "gideceÄŸim", "yapacaÄŸÄ±m", "alacaÄŸÄ±m",
            "nerede", "nasÄ±l", "hangi", "iÃ§in"
        ]
        
        cleaned = user_input.lower()
        for word in noise_words:
            cleaned = cleaned.replace(word, " ")
        
        # Fazla boÅŸluklarÄ± temizle
        cleaned = " ".join(cleaned.split())
        
        # "restaurant" ekle (TÃ¼rkÃ§e ve Ä°ngilizce)
        cleaned += " restaurant restoran mekan"
        
        return cleaned.strip()
    
    def _detect_city(self, query: str) -> Optional[str]:
        """
        Sorgudan ÅŸehir ismini tespit et
        
        Args:
            query: KullanÄ±cÄ± sorgusu
            
        Returns:
            Åehir ismi (title case) veya None
        """
        import re
        
        # TÃ¼rkiye ÅŸehirleri listesi (en Ã§ok kullanÄ±lanlar)
        cities = [
            'istanbul', 'ankara', 'izmir', 'bursa', 'antalya', 'adana', 'konya',
            'gaziantep', 'ÅŸanlÄ±urfa', 'mersin', 'diyarbakÄ±r', 'kayseri', 'eskiÅŸehir',
            'urfa', 'malatya', 'erzurum', 'samsun', 'denizli', 'trabzon', 'kahramanmaraÅŸ',
            'van', 'batman', 'elazÄ±ÄŸ', 'erzincan', 'sivas', 'manisa', 'tarsus',
            'adapazarÄ±', 'sakarya', 'balÄ±kesir', 'kÃ¼tahya', 'tekirdaÄŸ', 'edirne',
            'Ã§anakkale', 'yalova', 'ordu', 'giresun', 'rize', 'artvin', 'gÃ¼mÃ¼ÅŸhane',
            'bayburt', 'aÄŸrÄ±', 'kars', 'iÄŸdÄ±r', 'ardahan', 'muÅŸ', 'bitlis', 'hakkari',
            'siirt', 'ÅŸÄ±rnak', 'mardin', 'batman', 'adÄ±yaman', 'kilis', 'osmaniye',
            'hatay', 'isparta', 'burdur', 'afyon', 'uÅŸak', 'kÃ¼tahya', 'bilecik',
            'dÃ¼zce', 'bolu', 'karabÃ¼k', 'bartÄ±n', 'kastamonu', 'Ã§ankÄ±rÄ±', 'sinop',
            'amasya', 'tokat', 'Ã§orum', 'yozgat', 'kÄ±rÄ±kkale', 'aksaray', 'niÄŸde',
            'nevÅŸehir', 'kÄ±rÅŸehir', 'karaman', 'konya'
        ]
        
        query_lower = query.lower()
        
        # Direkt ÅŸehir ismi geÃ§iyor mu?
        for city in cities:
            if city in query_lower:
                return city.title()
        
        # "X'da hava durumu" pattern'i
        weather_pattern = r"(\w+)['']?d[ae]\s+(?:hava|sÄ±caklÄ±k|derece)"
        match = re.search(weather_pattern, query_lower)
        if match:
            potential_city = match.group(1)
            if potential_city in cities:
                return potential_city.title()
        
        return None
    
    
    async def _smart_response_analysis(
        self,
        user_input: str,
        llm_response: str,
        original_tool: str
    ) -> str:
        """
        LLM yanÄ±tÄ±nÄ± analiz et
        NOT: Web search kaldÄ±rÄ±ldÄ±, sadece orijinal yanÄ±tÄ± dÃ¶ndÃ¼rÃ¼yor
        """
        # Web search silindi - direkt orijinal yanÄ±tÄ± dÃ¶ndÃ¼r
        return llm_response


    # ====== PROCESS WITH TOOLS (HafizaAsistani v3.0 ile!) ======
    # NOT: _execute_tool() metodu kaldÄ±rÄ±ldÄ±.
    # ArtÄ±k HafizaAsistani._tool_calistir() kullanÄ±lÄ±yor.
    # ======
    async def process_with_tools(self, user_input: str, chat_history: List) -> str:
        """
        ğŸ¯ Tool system ile iÅŸle - HafizaAsistani'nÄ±n ANA METODunu kullanarak!

        YENÄ° AKIÅ (Refactored):
        1. HafizaAsistani.hazirla_ve_prompt_olustur() â†’ HazÄ±r prompt paketi
        2. Gemma3'e gÃ¶nder
        3. CevabÄ± dÃ¶ndÃ¼r

        KAZANÃ‡:
        - 220 satÄ±r â†’ 25 satÄ±r (%88 azalma)
        - Tek sorumluluk prensibi
        - Kod tekrarÄ± yok
        - BakÄ±mÄ± kolay
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ PROCESS WITH TOOLS (HafizaAsistani v3.0)")
        print(f"{'='*60}")

        # ğŸ¯ HafizaAsistani'nÄ±n ANA METODunu kullan!
        # (Tool seÃ§imi + Ã‡alÄ±ÅŸtÄ±rma + BaÄŸlam toplama + Prompt hazÄ±rlama)
        paket = await self.memory.hazirla_ve_prompt_olustur(
            user_input=user_input,
            chat_history=chat_history
        )

        # ğŸ“¦ Paket iÃ§eriÄŸi:
        # {
        #     "prompt": "Gemma3'e gÃ¶nderilecek hazÄ±r prompt",
        #     "role": "friend/technical_helper/teacher",
        #     "tool_used": "web_ara/zaman_getir/vb.",
        #     "metadata": {
        #         "has_tool_result": True/False,
        #         "has_semantic": True/False,
        #         "has_faiss": True/False,
        #         "has_history": True/False
        #     }
        # }

        # ğŸ” DEBUG: DetaylÄ± paket bilgisi
        print("\n" + "="*60)
        print("ğŸ“¦ HAFÄ°ZA ASÄ°STANI â†’ PERSONAL AI PAKETÄ°")
        print("="*60)
        print(f"ğŸ­ Rol: {paket.get('role', 'N/A')}")
        print(f"ğŸ”§ Tool: {paket.get('tool_used', 'N/A')}")

        llm_decision = paket.get('llm_decision', {})
        print(f"\nğŸ“Š LLM KararÄ±:")
        print(f"   â€¢ question_type: {llm_decision.get('question_type', 'N/A')}")
        print(f"   â€¢ needs_faiss: {llm_decision.get('needs_faiss', 'N/A')}")
        print(f"   â€¢ needs_web: {llm_decision.get('needs_web', 'N/A')}")
        print(f"   â€¢ needs_semantic_memory: {llm_decision.get('needs_semantic_memory', 'N/A')}")
        print(f"   â€¢ needs_chat_history: {llm_decision.get('needs_chat_history', 'N/A')}")
        print(f"   â€¢ response_style: {llm_decision.get('response_style', 'N/A')}")
        reasoning = llm_decision.get('reasoning', 'N/A')
        print(f"   â€¢ reasoning: {reasoning[:100] if reasoning else 'N/A'}...")

        metadata = paket.get('metadata', {})
        print(f"\nğŸ“‹ Metadata:")
        print(f"   â€¢ has_tool_result: {metadata.get('has_tool_result', 'N/A')}")
        print(f"   â€¢ has_semantic: {metadata.get('has_semantic', 'N/A')}")
        print(f"   â€¢ has_faiss: {metadata.get('has_faiss', 'N/A')}")
        print(f"   â€¢ has_history: {metadata.get('has_history', 'N/A')}")

        print(f"\nğŸ“ Prompt uzunluÄŸu: {len(paket.get('prompt', ''))} karakter")
        print("="*60 + "\n")

        # âœ… HazÄ±r prompt'u direkt LLM'ye gÃ¶nder
        print("ğŸ¤– LLM'e gÃ¶nderiliyor (tek Ã§aÄŸrÄ±)...")
        final_response = await self.llm.generate(paket["prompt"])

        print("âœ… Cevap alÄ±ndÄ±!\n")
        return final_response
    
    # ====== ANA PROCESS FONKSÄ°YONU (GÃœNCELLENMÄ°Å) ======
    async def process(
        self,
        user_input: str,
        chat_history: List[Dict[str, Any]],
        image_data: Optional[bytes] = None
    ) -> Tuple[str, str, str]:
        """
        Ana iÅŸlem fonksiyonu (TOOL SYSTEM Ä°LE!)
        """
        start_time = time.time()
        
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ‘¤ USER: {user_input}")
            print(f"{'='*60}")
            
            # 1. Ã–zel komutlarÄ± kontrol et
            mode_response = await self._handle_mode_commands(user_input)
            if mode_response:
                return mode_response, "simple", "command"

            # 2. EÄŸer gÃ¶rsel varsa HYBRID YAKLAÅIM: Vision + Tool System
            if image_data:
                print("ğŸ–¼ï¸ GÃ¶rsel tespit edildi - Hybrid Vision + Context sistemi kullanÄ±lÄ±yor...")

                # AdÄ±m 1: GÃ¶rseli analiz et
                vision_prompt = f"KullanÄ±cÄ± sorusu: {user_input}\n\nBu gÃ¶rseli kÄ±saca analiz et (2-3 cÃ¼mle)."
                vision_analysis = await self.llm.generate(vision_prompt, image_data=image_data)
                print(f"ğŸ‘ï¸ GÃ¶rsel analizi tamamlandÄ±: {vision_analysis[:100]}...")

                # AdÄ±m 2: GÃ¶rsel analiz sonucunu kullanÄ±cÄ± sorusuyla birleÅŸtir
                enhanced_input = f"{user_input}\n\n[GÃ¶rsel BaÄŸlamÄ±: {vision_analysis}]"

                # AdÄ±m 3: Tool system ile tam yanÄ±t oluÅŸtur (baÄŸlam + hafÄ±za + web vb. ile)
                print("ğŸ”§ Tool system devreye giriyor (baÄŸlam + hafÄ±za)...")
                raw_response = await self.process_with_tools(enhanced_input, chat_history)
            else:
                # âœ… process_with_tools kullan (KOD TEKRARI KALDIRILDI!)
                raw_response = await self.process_with_tools(user_input, chat_history)
            
            # 3. Post-process
            is_continuing = len(chat_history) > 0
            final_response = self._post_process(raw_response, user_input, is_continuing)
            
            # 4. HafÄ±zaya kaydet
            if self._should_save_interaction(user_input, final_response):
                # chat_history'yi de geÃ§ir (ConversationContext iÃ§in)
                self.memory.add(user_input, final_response, chat_history)

            # 5. Performans kaydÄ±
            processing_time = time.time() - start_time
            self.performance_metrics['processing_time'].append(processing_time)
            
            print(f"\nâ±ï¸ Ä°ÅŸlem sÃ¼resi: {processing_time:.2f}s")
            print(f"ğŸ¤– AI: {final_response[:200]}...")
            print(f"{'='*60}\n")
            
            return final_response, "simple", "success"
        
        except Exception as e:
            print(f"âŒ HATA: {e}")
            import traceback
            traceback.print_exc()
            
            self.performance_metrics['errors'].append(str(e))
            return "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu.", "error", "error"
    
    async def _handle_mode_commands(self, user_input: str) -> Optional[str]:
        """Ã–zel komutlarÄ± iÅŸle"""
        user_lower = user_input.lower()
        
        # Sistem durumu
        if any(phrase in user_lower for phrase in ["sistem durum", "stats", "istatistik"]):
            stats = self.get_system_stats()
            
            response = f"""ğŸ“Š Sistem Durumu:

ğŸ§  LLM: {stats['llm']['model']}
ğŸ’¾ HafÄ±za: {stats['memory']['total_entries']} kayÄ±t
ğŸ“š Bilgi TabanÄ±: {'Aktif âœ…' if stats['knowledge_base']['enabled'] else 'KapalÄ± âŒ'}
ğŸ“– Wikipedia Tool: Aktif âœ…
ğŸ”§ Tool System: Aktif âœ…

ğŸ“ˆ Performans:
  â€¢ Toplam etkileÅŸim: {stats['performance']['total_interactions']}
  â€¢ Ort. iÅŸlem sÃ¼resi: {stats['performance']['avg_processing_time']:.2f}s
"""
            return response
        
        # HafÄ±za temizle
        if any(phrase in user_lower for phrase in ["hafÄ±za temizle", "memory clear"]):
            self.memory.clear()
            return "âœ… HafÄ±za temizlendi."
        
        return None
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Sistem istatistiklerini dÃ¶ndÃ¼r"""
        # Hata korumalÄ± total_chunks ve total_entries eriÅŸimi
        kb_chunks = self.faiss_kb.index.ntotal if self.faiss_kb and hasattr(self.faiss_kb, 'index') and self.faiss_kb.index else 0
        mem_entries = len(self.memory.data) if hasattr(self.memory, 'data') else 0
        
        return {
            'llm': {
                'model': SystemConfig.MODEL_NAME,
                'provider': SystemConfig.LLM_PROVIDER
            },
            'memory': {
                'total_entries': mem_entries
            },
            'knowledge_base': {
                'enabled': self.faiss_kb and self.faiss_kb.enabled,
                'total_chunks': kb_chunks
            },
            'wikipedia_tool': {
                'enabled': True
            },
            'performance': {
                'total_interactions': len(self.performance_metrics['processing_time']),
                'avg_processing_time': (
                    sum(self.performance_metrics['processing_time']) / 
                    len(self.performance_metrics['processing_time'])
                ) if self.performance_metrics['processing_time'] else 0,
                'success_rate': 100.0
            }
        }
    
    def close(self):
        """Sistemi kapat"""
        print("\nğŸ›‘ PersonalAI kapatÄ±lÄ±yor...")
        print("âœ… Temizlik tamamlandÄ±.")


# ==========================================================
# BÃ–LÃœM 9: Ã‡ALIÅTIRMA VE TEST KODLARI
# ==========================================================

async def run_interactive_chat(ai_system: PersonalAI):
    """
    Ä°nteraktif sohbet modu
    """
    chat_history = []
    
    print("\n" + "=" * 60)
    print("ğŸ’¬ Ä°nteraktif Sohbet Modu")
    print("=" * 60)
    print("Komutlar:")
    print("  'exit' veya 'quit' - Ã‡Ä±kÄ±ÅŸ")
    print("  'stats' - Ä°statistikler")
    print("  'clear' - GeÃ§miÅŸi temizle")
    print("=" * 60 + "\n")
    
    while True:
        try:
            # KullanÄ±cÄ± input
            user_input = input("\nğŸ‘¤ Sen: ").strip()
            
            if not user_input:
                continue
            
            # Exit
            if user_input.lower() in ['exit', 'quit', 'Ã§Ä±kÄ±ÅŸ']:
                print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
                break
            
            # Clear history
            if user_input.lower() in ['clear', 'temizle']:
                chat_history = []
                # HafÄ±zayÄ± temizleme komutu PersonalAI sÄ±nÄ±fÄ±nda iÅŸlenir
                if user_input.lower() != 'temizle': # Tekrar temizlenmemesi iÃ§in
                    pass
                else:
                    print("âœ… Sohbet geÃ§miÅŸi temizlendi.")
                continue
            
            # Process
            print("\nğŸ¤– AI dÃ¼ÅŸÃ¼nÃ¼yor...", end="", flush=True)
            reply, _, _ = await ai_system.process(user_input, chat_history)
            print("\r" + " " * 30 + "\r", end="")  # Clear "thinking" message
            
            print(f"ğŸ¤– AI: {reply}")
            
            # Update history
            chat_history.append({
                "role": "user",
                "content": user_input
            })
            chat_history.append({
                "role": "ai",
                "content": reply
            })
            
            # Keep only last 10 messages
            if len(chat_history) > 20:
                chat_history = chat_history[-20:]
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
            break
        except Exception as e:
            print(f"\nâŒ Hata: {e}")


async def run_test_scenarios(ai_system: PersonalAI):
    """
    Test senaryolarÄ±
    """
    chat_history = []
    
    print("\n" + "=" * 60)
    print("ğŸ§ª TEST SENARYOLARI")
    print("=" * 60)
    
    # Senaryo 1: GÃ¼ncel bilgi
    print("\n--- SENARYO 1: GÃ¼ncel Bilgi (Hava Durumu) ---")
    user_input_1 = "Sakarya iÃ§in hava durumu nasÄ±l? Sabah dÄ±ÅŸarÄ± Ã§Ä±kacaÄŸÄ±m."
    print(f"ğŸ‘¤ USER: {user_input_1}")
    
    reply_1, _, _ = await ai_system.process(user_input_1, chat_history)
    print(f"ğŸ¤– AI: {reply_1}\n")
    
    chat_history.append({"role": "user", "content": user_input_1})
    chat_history.append({"role": "ai", "content": reply_1})
    
    # Senaryo 2: KiÅŸisel hafÄ±za
    print("--- SENARYO 2: KiÅŸisel HafÄ±za (GraphRAG Test) ---")
    user_input_2 = "GeÃ§en konuÅŸtuÄŸumuz yapay zeka projemle ilgili ne dÃ¼ÅŸÃ¼nÃ¼yorsun?"
    print(f"ğŸ‘¤ USER: {user_input_2}")
    
    reply_2, _, _ = await ai_system.process(user_input_2, chat_history)
    print(f"ğŸ¤– AI: {reply_2}\n")
    
    chat_history.append({"role": "user", "content": user_input_2})
    chat_history.append({"role": "ai", "content": reply_2})
    
    # Senaryo 3: Teknik destek
    print("--- SENARYO 3: Teknik Destek (Role Switching) ---")
    user_input_3 = "Python'da bir kod hatasÄ± alÄ±yorum: 'ImportError: No module named numpy'. Ne yapmalÄ±yÄ±m?"
    print(f"ğŸ‘¤ USER: {user_input_3}")
    
    reply_3, _, _ = await ai_system.process(user_input_3, chat_history)
    print(f"ğŸ¤– AI: {reply_3}\n")
    
    # Senaryo 4: Sistem durumu
    print("--- SENARYO 4: Sistem Durumu ---")
    user_input_4 = "sistem durum"
    print(f"ğŸ‘¤ USER: {user_input_4}")
    
    reply_4, _, _ = await ai_system.process(user_input_4, chat_history)
    print(f"ğŸ¤– AI: {reply_4}\n")
    
    print("=" * 60)
    print("âœ… TÃ¼m test senaryolarÄ± tamamlandÄ±!")
    print("=" * 60)

async def test_spacy_integration():
    """spaCy entegrasyonunu test et"""
    print("\n" + "=" * 60)
    print("ğŸ§ª spaCy ENTEGRASYON TESTÄ°")
    print("=" * 60)
    
    # Sistem baÅŸlat
    ai = PersonalAI(user_id="test_user")
    
    # Test metni
    test_text = """
    Ahmet YÄ±lmaz, 15 Ocak 2024'te Ä°stanbul'da Python Ã¶ÄŸrenmeye baÅŸladÄ±.
    Neo4j kullanarak 5000 TL'lik bir proje geliÅŸtirdi.
    """
    
    print(f"\nğŸ“ Test Metni:\n{test_text}")
    
    if ai.spacy_nlp.enabled:
        # Entity extraction
        entities = ai.spacy_nlp.extract_entities(test_text)
        print("\nğŸ“ Tespit Edilen Entity'ler:")
        for entity_type, entity_list in entities.items():
            print(f"  {entity_type}: {[e['text'] for e in entity_list]}")
        
        # Lemmas
        lemmas = ai.spacy_nlp.get_lemmas(test_text)
        print(f"\nğŸ”¤ Lemma'lar (ilk 10): {lemmas[:10]}")
        
        # Noun chunks
        chunks = ai.spacy_nlp.get_noun_chunks(test_text)
        print(f"\nğŸ“¦ Ä°sim Ã–bekleri: {chunks}")
        
        # Sentiment
        sentiment = ai.spacy_nlp.analyze_sentiment_pos(test_text)
        print(f"\nğŸ˜Š Sentiment: {sentiment}")
        
        print("\nâœ… spaCy entegrasyonu baÅŸarÄ±lÄ±!")
    else:
        print("\nâš ï¸ spaCy aktif deÄŸil!")
    
    print("=" * 60)
    
    ai.close()


def main():
    """
    Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu
    """
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                           â•‘
    â•‘             PersonalAI - GeliÅŸmiÅŸ Asistan                 â•‘
    â•‘                                                           â•‘
    â•‘ â€¢ Gemma 3 27B LLM                                         â•‘
    â•‘ â€¢ FAISS Vector Memory                                     â•‘
    â•‘ â€¢ Neo4j GraphRAG (Uzun DÃ¶nem HafÄ±za)                      â•‘
    â•‘ â€¢ spaCy NLP Engine                                        â•‘
    â•‘ â€¢ Multi-Role System                                       â•‘
    â•‘ â€¢ Web Search Integration                                  â•‘
    â•‘ â€¢ Chain-of-Thought Reasoning                              â•‘
    â•‘                                                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    try:
        # Mod seÃ§imi
        print("\nMod SeÃ§in:")
        print("1. Ä°nteraktif Sohbet")
        print("2. Test SenaryolarÄ±")
        print("3. spaCy Entegrasyon Testi")  # ğŸ†• EKLE
        
        choice = input("\nSeÃ§iminiz (1/2/3): ").strip()
        
        if choice == "1":
            system = PersonalAI(user_id="murat")
            asyncio.run(run_interactive_chat(system))
            system.close()
        elif choice == "2":
            system = PersonalAI(user_id="murat")
            asyncio.run(run_test_scenarios(system))
            system.close()
        elif choice == "3":  # ğŸ†• EKLE
            asyncio.run(test_spacy_integration())
        else:
            print("âŒ GeÃ§ersiz seÃ§im!")
        
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Program durduruldu.")
    except Exception as e:
        print(f"\nâŒ Kritik hata: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()